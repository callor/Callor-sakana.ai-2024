2024-09-10 08:12:28 root@1d58f5ac47cd:/app/AI-Scientist# ls
2024-09-10 08:12:28 
LICENSE    ai_scientist  docs experimental      review_ai_scientist  templates
2024-09-10 08:12:28 README.md  data example_papers  launch_scientist.py  review_iclr_bench
2024-09-10 08:12:31 root@1d58f5ac47cd:/app/AI-Scientist# cd ai_scientist/
2024-09-10 08:12:31 
root@1d58f5ac47cd:/app/AI-Scientist/ai_scientist# ls
2024-09-10 08:12:31 
__init__.py  fewshot_examples  generate_ideas.py  llm.py  perform_experiments.py  perform_review.py  perform_writeup.py
2024-09-10 08:12:36 root@1d58f5ac47cd:/app/AI-Scientist/ai_scientist# ls -al
2024-09-10 08:12:36 
total 96
2024-09-10 08:12:36 drwxr-xr-x 3 root root  4096 Aug 16 23:33 .
2024-09-10 08:12:36 drwxr-xr-x 1 root root  4096 Aug 16 23:33 ..
2024-09-10 08:12:36 -rw-r--r-- 1 root root     0 Aug 16 23:33 __init__.py
2024-09-10 08:12:36 drwxr-xr-x 2 root root  4096 Aug 16 23:33 fewshot_examples
2024-09-10 08:12:36 -rw-r--r-- 1 root root 19346 Aug 16 23:33 generate_ideas.py
2024-09-10 08:12:36 -rw-r--r-- 1 root root  7674 Aug 16 23:33 llm.py
2024-09-10 08:12:36 -rw-r--r-- 1 root root  6151 Aug 16 23:33 perform_experiments.py
2024-09-10 08:12:36 -rw-r--r-- 1 root root 18580 Aug 16 23:33 perform_review.py
2024-09-10 08:12:36 -rw-r--r-- 1 root root 25494 Aug 16 23:33 perform_writeup.py
2024-09-10 08:12:44 root@1d58f5ac47cd:/app/AI-Scientist/ai_scientist# cd ..
2024-09-10 08:12:45 
root@1d58f5ac47cd:/app/AI-Scientist# ls
2024-09-10 08:12:45 
LICENSE    ai_scientist  docs experimental      review_ai_scientist  templates
2024-09-10 08:12:45 README.md  data example_papers  launch_scientist.py  review_iclr_bench
2024-09-10 08:12:47 root@1d58f5ac47cd:/app/AI-Scientist# ls -al
2024-09-10 08:12:47 
total 88
2024-09-10 08:12:47 drwxr-xr-x  1 root root  4096 Aug 16 23:33 .
2024-09-10 08:12:47 drwxr-xr-x  1 root root  4096 Aug 17 00:44 ..
2024-09-10 08:12:47 drwxr-xr-x  8 root root  4096 Aug 16 23:33 .git
2024-09-10 08:12:47 -rwxr-xr-x  1 root root  3229 Aug 16 23:33 .gitignore
2024-09-10 08:12:47 -rwxr-xr-x  1 root root 10238 Aug 16 23:33 LICENSE
2024-09-10 08:12:47 -rw-r--r--  1 root root 12156 Aug 16 23:33 README.md
2024-09-10 08:12:47 drwxr-xr-x  3 root root  4096 Aug 16 23:33 ai_scientist
2024-09-10 08:12:47 drwxr-xr-x  1 root root  4096 Aug 16 23:33 data
2024-09-10 08:12:47 drwxr-xr-x  2 root root  4096 Aug 16 23:33 docs
2024-09-10 08:12:47 drwxr-xr-x 12 root root  4096 Aug 16 23:33 example_papers
2024-09-10 08:12:47 drwxr-xr-x  2 root root  4096 Aug 16 23:33 experimental
2024-09-10 08:12:47 -rw-r--r--  1 root root 13702 Aug 16 23:33 launch_scientist.py
2024-09-10 08:12:47 drwxr-xr-x  7 root root  4096 Aug 16 23:33 review_ai_scientist
2024-09-10 08:12:47 drwxr-xr-x  4 root root  4096 Aug 16 23:33 review_iclr_bench
2024-09-10 08:12:47 drwxr-xr-x  1 root root  4096 Aug 16 23:33 templates
2024-09-10 08:12:50 root@1d58f5ac47cd:/app/AI-Scientist# cd templates/
2024-09-10 08:12:51 
root@1d58f5ac47cd:/app/AI-Scientist/templates# ls
2024-09-10 08:12:51 
2d_diffusion  grokkingnanoGPT  nanoGPT_lite
2024-09-10 08:12:54 root@1d58f5ac47cd:/app/AI-Scientist/templates# cd nanoGPT
2024-09-10 08:12:54 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# ls
2024-09-10 08:12:54 
WRITEUP.md  experiment.py  ideas.json  latex  plot.py  prompt.json  run_0  seed_ideas.json
2024-09-10 08:12:57 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# cd run_0/
2024-09-10 08:12:57 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT/run_0# ls
2024-09-10 08:13:14 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT/run_0# ls
2024-09-10 08:13:15 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT/run_0# cd ..
2024-09-10 08:13:16 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# ls
2024-09-10 08:13:16 
WRITEUP.md  experiment.py  ideas.json  latex  plot.py  prompt.json  run_0  seed_ideas.json
2024-09-10 08:13:17 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# cd ..
2024-09-10 08:13:20 
root@1d58f5ac47cd:/app/AI-Scientist/templates# cd nanoGPT
2024-09-10 08:13:20 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# ls
2024-09-10 08:13:20 
WRITEUP.md  experiment.py  ideas.json  latex  plot.py  prompt.json  run_0  seed_ideas.json
2024-09-10 08:13:26 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python experiment.py 
2024-09-10 08:13:28 
tokens per iteration will be: 16,384
2024-09-10 08:13:28 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:13:28 Initializing a new model from scratch
2024-09-10 08:13:28 number of parameters: 10.65M
2024-09-10 08:13:28 /app/AI-Scientist/templates/nanoGPT/experiment.py:462: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2024-09-10 08:13:28   scaler = torch.cuda.amp.GradScaler(enabled=(dtype == "float16"))
2024-09-10 08:13:28 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:13:28 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:13:29 using fused AdamW: True
2024-09-10 08:13:29 compiling the model... (takes a ~minute)
2024-09-10 08:13:50 step 0: train loss 4.2874, val loss 4.2823
2024-09-10 08:14:06 iter 0: loss 4.2654, time 35844.13ms
2024-09-10 08:14:06 iter 10: loss 3.2457, time 75.31ms
2024-09-10 08:14:07 iter 20: loss 2.7914, time 75.24ms
2024-09-10 08:14:08 iter 30: loss 2.6356, time 75.08ms
2024-09-10 08:14:09 iter 40: loss 2.5777, time 75.42ms
2024-09-10 08:14:09 iter 50: loss 2.5276, time 75.74ms
2024-09-10 08:14:10 iter 60: loss 2.5196, time 76.24ms
2024-09-10 08:14:11 iter 70: loss 2.4957, time 77.53ms
2024-09-10 08:14:12 iter 80: loss 2.4973, time 75.59ms
2024-09-10 08:14:12 iter 90: loss 2.4677, time 75.75ms
2024-09-10 08:14:13 iter 100: loss 2.4617, time 76.75ms
2024-09-10 08:14:14 iter 110: loss 2.4568, time 75.41ms
2024-09-10 08:14:15 iter 120: loss 2.4274, time 75.54ms
2024-09-10 08:14:15 iter 130: loss 2.4142, time 76.69ms
2024-09-10 08:14:16 iter 140: loss 2.4241, time 75.58ms
2024-09-10 08:14:17 iter 150: loss 2.4261, time 75.45ms
2024-09-10 08:14:18 iter 160: loss 2.3768, time 77.34ms
2024-09-10 08:14:19 iter 170: loss 2.3592, time 76.15ms
2024-09-10 08:14:19 iter 180: loss 2.3237, time 75.83ms
2024-09-10 08:14:20 iter 190: loss 2.2548, time 76.27ms
2024-09-10 08:14:21 iter 200: loss 2.2126, time 77.87ms
2024-09-10 08:14:22 iter 210: loss 2.1590, time 77.98ms
2024-09-10 08:14:22 iter 220: loss 2.1511, time 78.72ms
2024-09-10 08:14:23 iter 230: loss 2.0733, time 75.81ms
2024-09-10 08:14:24 iter 240: loss 2.0800, time 77.95ms
2024-09-10 08:14:35 step 250: train loss 1.9743, val loss 2.0713
2024-09-10 08:14:35 iter 250: loss 2.0371, time 10242.75ms
2024-09-10 08:14:35 iter 260: loss 1.9820, time 76.11ms
2024-09-10 08:14:36 iter 270: loss 1.9824, time 76.33ms
2024-09-10 08:14:37 iter 280: loss 1.9833, time 82.72ms
2024-09-10 08:14:38 iter 290: loss 1.9194, time 76.27ms
2024-09-10 08:14:39 iter 300: loss 1.9016, time 76.96ms
2024-09-10 08:14:39 iter 310: loss 1.8739, time 76.83ms
2024-09-10 08:14:40 iter 320: loss 1.8530, time 76.66ms
2024-09-10 08:14:41 iter 330: loss 1.8223, time 76.63ms
2024-09-10 08:14:42 iter 340: loss 1.7951, time 76.84ms
2024-09-10 08:14:42 iter 350: loss 1.8297, time 76.72ms
2024-09-10 08:14:43 iter 360: loss 1.7799, time 77.10ms
2024-09-10 08:14:44 iter 370: loss 1.7425, time 77.00ms
2024-09-10 08:14:45 iter 380: loss 1.7409, time 77.26ms
2024-09-10 08:14:46 iter 390: loss 1.7365, time 82.16ms
2024-09-10 08:14:46 iter 400: loss 1.7683, time 77.13ms
2024-09-10 08:14:47 iter 410: loss 1.7074, time 77.71ms
2024-09-10 08:14:48 iter 420: loss 1.7196, time 76.57ms
2024-09-10 08:14:49 iter 430: loss 1.6918, time 77.99ms
2024-09-10 08:14:49 iter 440: loss 1.6524, time 77.19ms
2024-09-10 08:14:50 iter 450: loss 1.6536, time 77.15ms
2024-09-10 08:14:51 iter 460: loss 1.6024, time 77.02ms
2024-09-10 08:14:52 iter 470: loss 1.6517, time 77.28ms
2024-09-10 08:14:53 iter 480: loss 1.6263, time 77.24ms
2024-09-10 08:14:53 iter 490: loss 1.5980, time 77.62ms
2024-09-10 08:15:04 step 500: train loss 1.5296, val loss 1.7272
2024-09-10 08:15:04 iter 500: loss 1.6017, time 10154.36ms
2024-09-10 08:15:05 iter 510: loss 1.6048, time 78.00ms
2024-09-10 08:15:06 iter 520: loss 1.5931, time 78.01ms
2024-09-10 08:15:06 iter 530: loss 1.5674, time 78.21ms
2024-09-10 08:15:07 iter 540: loss 1.6170, time 78.40ms
2024-09-10 08:15:08 iter 550: loss 1.5656, time 77.98ms
2024-09-10 08:15:09 iter 560: loss 1.5696, time 78.00ms
2024-09-10 08:15:09 iter 570: loss 1.5704, time 78.03ms
2024-09-10 08:15:10 iter 580: loss 1.5334, time 78.80ms
2024-09-10 08:15:11 iter 590: loss 1.4956, time 80.94ms
2024-09-10 08:15:12 iter 600: loss 1.5144, time 80.96ms
2024-09-10 08:15:13 iter 610: loss 1.5475, time 78.31ms
2024-09-10 08:15:14 iter 620: loss 1.5272, time 79.36ms
2024-09-10 08:15:14 iter 630: loss 1.5106, time 84.06ms
2024-09-10 08:15:15 iter 640: loss 1.4694, time 81.05ms
2024-09-10 08:15:16 iter 650: loss 1.5021, time 81.20ms
2024-09-10 08:15:17 iter 660: loss 1.5147, time 78.58ms
2024-09-10 08:15:18 iter 670: loss 1.4407, time 78.43ms
2024-09-10 08:15:18 iter 680: loss 1.5152, time 80.68ms
2024-09-10 08:15:19 iter 690: loss 1.4741, time 84.14ms
2024-09-10 08:15:20 iter 700: loss 1.4892, time 78.34ms
2024-09-10 08:15:21 iter 710: loss 1.4596, time 79.44ms
2024-09-10 08:15:22 iter 720: loss 1.4490, time 79.24ms
2024-09-10 08:15:22 iter 730: loss 1.4228, time 79.24ms
2024-09-10 08:15:23 iter 740: loss 1.4371, time 79.17ms
2024-09-10 08:15:34 step 750: train loss 1.3698, val loss 1.5919
2024-09-10 08:15:34 iter 750: loss 1.4304, time 10322.85ms
2024-09-10 08:15:35 iter 760: loss 1.4504, time 79.28ms
2024-09-10 08:15:36 iter 770: loss 1.4285, time 78.71ms
2024-09-10 08:15:36 iter 780: loss 1.4165, time 79.39ms
2024-09-10 08:15:37 iter 790: loss 1.4188, time 79.54ms
2024-09-10 08:15:38 iter 800: loss 1.4357, time 79.12ms
2024-09-10 08:15:39 iter 810: loss 1.4099, time 81.91ms
2024-09-10 08:15:39 iter 820: loss 1.4107, time 78.74ms
2024-09-10 08:15:40 iter 830: loss 1.3999, time 82.42ms
2024-09-10 08:15:41 iter 840: loss 1.4015, time 79.07ms
2024-09-10 08:15:42 iter 850: loss 1.3868, time 79.68ms
2024-09-10 08:15:43 iter 860: loss 1.4019, time 79.86ms
2024-09-10 08:15:43 iter 870: loss 1.3991, time 79.53ms
2024-09-10 08:15:44 iter 880: loss 1.3748, time 79.73ms
2024-09-10 08:15:45 iter 890: loss 1.3908, time 79.94ms
2024-09-10 08:15:46 iter 900: loss 1.3711, time 79.52ms
2024-09-10 08:15:47 iter 910: loss 1.3178, time 79.62ms
2024-09-10 08:15:47 iter 920: loss 1.3716, time 79.73ms
2024-09-10 08:15:48 iter 930: loss 1.3630, time 79.80ms
2024-09-10 08:15:49 iter 940: loss 1.3523, time 79.47ms
2024-09-10 08:15:50 iter 950: loss 1.3592, time 79.15ms
2024-09-10 08:15:51 iter 960: loss 1.3666, time 79.23ms
2024-09-10 08:15:51 iter 970: loss 1.3689, time 80.15ms
2024-09-10 08:15:52 iter 980: loss 1.3490, time 79.71ms
2024-09-10 08:15:53 iter 990: loss 1.3461, time 82.28ms
2024-09-10 08:16:04 step 1000: train loss 1.2763, val loss 1.5214
2024-09-10 08:16:04 iter 1000: loss 1.3433, time 10584.39ms
2024-09-10 08:16:05 iter 1010: loss 1.3385, time 79.38ms
2024-09-10 08:16:06 iter 1020: loss 1.3159, time 79.73ms
2024-09-10 08:16:07 iter 1030: loss 1.3371, time 80.62ms
2024-09-10 08:16:07 iter 1040: loss 1.3602, time 79.86ms
2024-09-10 08:16:08 iter 1050: loss 1.3037, time 79.63ms
2024-09-10 08:16:09 iter 1060: loss 1.3398, time 80.10ms
2024-09-10 08:16:10 iter 1070: loss 1.3349, time 79.47ms
2024-09-10 08:16:11 iter 1080: loss 1.3453, time 81.54ms
2024-09-10 08:16:11 iter 1090: loss 1.3513, time 80.26ms
2024-09-10 08:16:12 iter 1100: loss 1.3224, time 80.95ms
2024-09-10 08:16:13 iter 1110: loss 1.2996, time 80.08ms
2024-09-10 08:16:14 iter 1120: loss 1.3048, time 79.58ms
2024-09-10 08:16:15 iter 1130: loss 1.2995, time 80.09ms
2024-09-10 08:16:15 iter 1140: loss 1.2982, time 80.04ms
2024-09-10 08:16:16 iter 1150: loss 1.3173, time 79.55ms
2024-09-10 08:16:17 iter 1160: loss 1.3334, time 79.76ms
2024-09-10 08:16:18 iter 1170: loss 1.2956, time 81.70ms
2024-09-10 08:16:19 iter 1180: loss 1.3195, time 80.16ms
2024-09-10 08:16:20 iter 1190: loss 1.2671, time 79.85ms
2024-09-10 08:16:20 iter 1200: loss 1.2916, time 80.65ms
2024-09-10 08:16:21 iter 1210: loss 1.2730, time 83.04ms
2024-09-10 08:16:22 iter 1220: loss 1.3005, time 80.67ms
2024-09-10 08:16:23 iter 1230: loss 1.3020, time 80.58ms
2024-09-10 08:16:24 iter 1240: loss 1.3051, time 79.78ms
2024-09-10 08:16:35 step 1250: train loss 1.2087, val loss 1.4899
2024-09-10 08:16:35 iter 1250: loss 1.2806, time 10549.60ms
2024-09-10 08:16:35 iter 1260: loss 1.2840, time 80.21ms
2024-09-10 08:16:36 iter 1270: loss 1.2664, time 84.16ms
2024-09-10 08:16:37 iter 1280: loss 1.2603, time 82.16ms
2024-09-10 08:16:38 iter 1290: loss 1.2848, time 83.00ms
2024-09-10 08:16:39 iter 1300: loss 1.3049, time 81.21ms
2024-09-10 08:16:40 iter 1310: loss 1.2416, time 82.02ms
2024-09-10 08:16:40 iter 1320: loss 1.3092, time 80.31ms
2024-09-10 08:16:41 iter 1330: loss 1.2702, time 80.31ms
2024-09-10 08:16:42 iter 1340: loss 1.3013, time 80.77ms
2024-09-10 08:16:43 iter 1350: loss 1.2517, time 79.83ms
2024-09-10 08:16:44 iter 1360: loss 1.2798, time 80.37ms
2024-09-10 08:16:44 iter 1370: loss 1.2579, time 80.25ms
2024-09-10 08:16:45 iter 1380: loss 1.2602, time 81.32ms
2024-09-10 08:16:46 iter 1390: loss 1.2542, time 74.96ms
2024-09-10 08:16:47 iter 1400: loss 1.2613, time 80.57ms
2024-09-10 08:16:48 iter 1410: loss 1.2563, time 80.95ms
2024-09-10 08:16:49 iter 1420: loss 1.2736, time 80.76ms
2024-09-10 08:16:49 iter 1430: loss 1.2394, time 80.35ms
2024-09-10 08:16:50 iter 1440: loss 1.2536, time 80.65ms
2024-09-10 08:16:51 iter 1450: loss 1.2311, time 80.03ms
2024-09-10 08:16:52 iter 1460: loss 1.2428, time 80.39ms
2024-09-10 08:16:53 iter 1470: loss 1.2255, time 81.45ms
2024-09-10 08:16:53 iter 1480: loss 1.2190, time 80.38ms
2024-09-10 08:16:54 iter 1490: loss 1.2401, time 80.71ms
2024-09-10 08:17:05 step 1500: train loss 1.1552, val loss 1.4808
2024-09-10 08:17:05 iter 1500: loss 1.1783, time 10430.19ms
2024-09-10 08:17:06 iter 1510: loss 1.2334, time 80.78ms
2024-09-10 08:17:07 iter 1520: loss 1.2327, time 86.84ms
2024-09-10 08:17:08 iter 1530: loss 1.2556, time 80.49ms
2024-09-10 08:17:08 iter 1540: loss 1.1960, time 82.61ms
2024-09-10 08:17:09 iter 1550: loss 1.2412, time 80.33ms
2024-09-10 08:17:10 iter 1560: loss 1.2088, time 80.50ms
2024-09-10 08:17:11 iter 1570: loss 1.2371, time 80.65ms
2024-09-10 08:17:12 iter 1580: loss 1.2031, time 80.17ms
2024-09-10 08:17:12 iter 1590: loss 1.1926, time 82.33ms
2024-09-10 08:17:13 iter 1600: loss 1.1930, time 82.32ms
2024-09-10 08:17:14 iter 1610: loss 1.2389, time 82.77ms
2024-09-10 08:17:15 iter 1620: loss 1.1805, time 82.30ms
2024-09-10 08:17:16 iter 1630: loss 1.2111, time 80.73ms
2024-09-10 08:17:17 iter 1640: loss 1.1985, time 85.02ms
2024-09-10 08:17:17 iter 1650: loss 1.1847, time 81.52ms
2024-09-10 08:17:18 iter 1660: loss 1.2231, time 81.00ms
2024-09-10 08:17:19 iter 1670: loss 1.1960, time 81.11ms
2024-09-10 08:17:20 iter 1680: loss 1.1986, time 82.53ms
2024-09-10 08:17:21 iter 1690: loss 1.2054, time 81.07ms
2024-09-10 08:17:22 iter 1700: loss 1.1806, time 81.57ms
2024-09-10 08:17:22 iter 1710: loss 1.1831, time 80.52ms
2024-09-10 08:17:23 iter 1720: loss 1.1886, time 80.67ms
2024-09-10 08:17:24 iter 1730: loss 1.2013, time 81.01ms
2024-09-10 08:17:25 iter 1740: loss 1.1699, time 81.64ms
2024-09-10 08:17:36 step 1750: train loss 1.1058, val loss 1.4649
2024-09-10 08:17:36 iter 1750: loss 1.1843, time 10400.90ms
2024-09-10 08:17:37 iter 1760: loss 1.1893, time 80.99ms
2024-09-10 08:17:37 iter 1770: loss 1.1939, time 81.03ms
2024-09-10 08:17:38 iter 1780: loss 1.2008, time 80.76ms
2024-09-10 08:17:39 iter 1790: loss 1.1928, time 80.93ms
2024-09-10 08:17:40 iter 1800: loss 1.1786, time 81.45ms
2024-09-10 08:17:41 iter 1810: loss 1.1601, time 81.15ms
2024-09-10 08:17:41 iter 1820: loss 1.1681, time 81.00ms
2024-09-10 08:17:42 iter 1830: loss 1.1712, time 81.67ms
2024-09-10 08:17:43 iter 1840: loss 1.1615, time 82.06ms
2024-09-10 08:17:44 iter 1850: loss 1.1575, time 80.89ms
2024-09-10 08:17:45 iter 1860: loss 1.1760, time 83.65ms
2024-09-10 08:17:46 iter 1870: loss 1.1407, time 80.93ms
2024-09-10 08:17:46 ^CTraceback (most recent call last):
2024-09-10 08:17:46   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 697, in <module>
2024-09-10 08:17:46     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 08:17:46                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 556, in train
2024-09-10 08:17:46     logits, loss = model(X, Y)
2024-09-10 08:17:46                    ^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 08:17:46     return self._call_impl(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 08:17:46     return forward_call(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 08:17:46     return fn(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 08:17:46     return self._call_impl(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 08:17:46     return forward_call(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 207, in forward
2024-09-10 08:17:46     def forward(self, idx, targets=None):
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 600, in _fn
2024-09-10 08:17:46     return fn(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 987, in forward
2024-09-10 08:17:46     return compiled_fn(full_args)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 204, in runtime_wrapper
2024-09-10 08:17:46     all_outs = call_func_at_runtime_with_args(
2024-09-10 08:17:46                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 120, in call_func_at_runtime_with_args
2024-09-10 08:17:46     out = normalize_as_list(f(args))
2024-09-10 08:17:46                             ^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 94, in g
2024-09-10 08:17:46     return f(*args)
2024-09-10 08:17:46            ^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/autograd/function.py", line 574, in apply
2024-09-10 08:17:46     return super().apply(*args, **kwargs)  # type: ignore[misc]
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1451, in forward
2024-09-10 08:17:46     fw_outs = call_func_at_runtime_with_args(
2024-09-10 08:17:46               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 120, in call_func_at_runtime_with_args
2024-09-10 08:17:46     out = normalize_as_list(f(args))
2024-09-10 08:17:46                             ^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 451, in wrapper
2024-09-10 08:17:46     return compiled_fn(runtime_args)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/codecache.py", line 1131, in __call__
2024-09-10 08:17:46     return self.current_callable(inputs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 944, in run
2024-09-10 08:17:46     return model(new_inputs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/tmp/torchinductor_root/ku/ckuiibv4rbrnoitmjiy3ekt63oc34zyzb4fziqfr4erhpwdvtq7a.py", line 1115, in call
2024-09-10 08:17:46     buf133 = aten._scaled_dot_product_flash_attention.default(reinterpret_tensor(buf132, (64, 6, 256, 64), (294912, 64, 1152, 1), 0), reinterpret_tensor(buf132, (64, 6, 256, 64), (294912, 64, 1152, 1), 384), reinterpret_tensor(buf132, (64, 6, 256, 64), (294912, 64, 1152, 1), 768), 0.2, True, scale=0.125)
2024-09-10 08:17:46              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 08:17:46     return self_._op(*args, **kwargs)
2024-09-10 08:17:46            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:17:46 KeyboardInterrupt
2024-09-10 08:17:47 
2024-09-10 08:18:36 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# cd ..
2024-09-10 08:18:37 
root@1d58f5ac47cd:/app/AI-Scientist/templates# cd ..
2024-09-10 08:18:39 
root@1d58f5ac47cd:/app/AI-Scientist# ls
2024-09-10 08:18:39 
LICENSE    ai_scientist  docs experimental      review_ai_scientist  templates
2024-09-10 08:18:39 README.md  data example_papers  launch_scientist.py  review_iclr_benchpython data/enwik8/prepare.py && python data/shakespeare_char/prepare.py && python data/text8/prepare.pyenwik8/prepare.py && python data/shakespeare_char/prepare.py && python data/text8/prepare.py
2024-09-10 08:18:43 
length of dataset in characters: 100,000,000
2024-09-10 08:18:44 all the unique characters: 
2024-09-10 08:18:44  !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~

¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÞàáâãäåæçèéêëìíïð
2024-09-10 08:18:44 vocab size: 205
2024-09-10 08:18:48 train has 90,000,000 tokens
2024-09-10 08:18:48 val has 5,000,000 tokens
2024-09-10 08:18:48 test has 5,000,000 tokens
2024-09-10 08:18:50 length of dataset in characters: 1,115,394
2024-09-10 08:18:50 all the unique characters: 
2024-09-10 08:18:50  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
2024-09-10 08:18:50 vocab size: 65
2024-09-10 08:18:50 train has 1,003,854 tokens
2024-09-10 08:18:50 val has 111,540 tokens
2024-09-10 08:18:50 length of dataset in characters: 100,000,000
2024-09-10 08:18:51 all the unique characters:  abcdefghijklmnopqrstuvwxyz
2024-09-10 08:18:51 vocab size: 27
2024-09-10 08:18:55 train has 90,000,000 tokens
2024-09-10 08:18:55 val has 5,000,000 tokens
2024-09-10 08:18:55 test has 5,000,000 tokens
2024-09-10 08:19:10 root@1d58f5ac47cd:/app/AI-Scientist# cd templates/nanoGPT && python experiment.py --out_dir run_0 && python plot.py
cd templates/nanoGPT && python experiment.py --out_dir run_0 && python plot.py
2024-09-10 08:19:11 
tokens per iteration will be: 16,384
2024-09-10 08:19:11 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:19:11 Initializing a new model from scratch
2024-09-10 08:19:12 number of parameters: 10.65M
2024-09-10 08:19:12 /app/AI-Scientist/templates/nanoGPT/experiment.py:462: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2024-09-10 08:19:12   scaler = torch.cuda.amp.GradScaler(enabled=(dtype == "float16"))
2024-09-10 08:19:12 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:19:12 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:19:12 using fused AdamW: True
2024-09-10 08:19:12 compiling the model... (takes a ~minute)
2024-09-10 08:19:30 step 0: train loss 4.2874, val loss 4.2823
2024-09-10 08:19:40 iter 0: loss 4.2654, time 26913.09ms
2024-09-10 08:19:41 iter 10: loss 3.2457, time 76.25ms
2024-09-10 08:19:41 iter 20: loss 2.7914, time 76.77ms
2024-09-10 08:19:42 iter 30: loss 2.6356, time 76.26ms
2024-09-10 08:19:43 iter 40: loss 2.5777, time 76.96ms
2024-09-10 08:19:44 iter 50: loss 2.5276, time 76.29ms
2024-09-10 08:19:45 iter 60: loss 2.5196, time 76.46ms
2024-09-10 08:19:45 iter 70: loss 2.4958, time 77.35ms
2024-09-10 08:19:46 iter 80: loss 2.4972, time 76.21ms
2024-09-10 08:19:47 iter 90: loss 2.4680, time 77.17ms
2024-09-10 08:19:48 iter 100: loss 2.4611, time 77.12ms
2024-09-10 08:19:48 iter 110: loss 2.4559, time 78.15ms
2024-09-10 08:19:49 iter 120: loss 2.4279, time 76.86ms
2024-09-10 08:19:50 iter 130: loss 2.4174, time 76.79ms
2024-09-10 08:19:51 iter 140: loss 2.4257, time 76.98ms
2024-09-10 08:19:52 iter 150: loss 2.4238, time 77.19ms
2024-09-10 08:19:52 iter 160: loss 2.3771, time 78.49ms
2024-09-10 08:19:53 iter 170: loss 2.3636, time 77.06ms
2024-09-10 08:19:54 iter 180: loss 2.3173, time 77.27ms
2024-09-10 08:19:55 iter 190: loss 2.2509, time 77.08ms
2024-09-10 08:19:55 iter 200: loss 2.2067, time 77.36ms
2024-09-10 08:19:56 iter 210: loss 2.1461, time 77.33ms
2024-09-10 08:19:57 iter 220: loss 2.1445, time 77.11ms
2024-09-10 08:19:58 iter 230: loss 2.0714, time 78.19ms
2024-09-10 08:19:59 iter 240: loss 2.0785, time 79.17ms
2024-09-10 08:20:09 step 250: train loss 1.9729, val loss 2.0725
2024-09-10 08:20:09 iter 250: loss 2.0357, time 10175.56ms
2024-09-10 08:20:10 iter 260: loss 1.9813, time 78.42ms
2024-09-10 08:20:11 iter 270: loss 1.9835, time 78.20ms
2024-09-10 08:20:12 iter 280: loss 1.9772, time 78.07ms
2024-09-10 08:20:12 iter 290: loss 1.9200, time 78.76ms
2024-09-10 08:20:13 iter 300: loss 1.9083, time 79.54ms
2024-09-10 08:20:14 iter 310: loss 1.8739, time 78.72ms
2024-09-10 08:20:15 iter 320: loss 1.8511, time 78.86ms
2024-09-10 08:20:16 iter 330: loss 1.8200, time 78.27ms
2024-09-10 08:20:16 iter 340: loss 1.7934, time 78.93ms
2024-09-10 08:20:17 iter 350: loss 1.8262, time 79.74ms
2024-09-10 08:20:18 iter 360: loss 1.7769, time 78.12ms
2024-09-10 08:20:19 iter 370: loss 1.7452, time 79.60ms
2024-09-10 08:20:20 iter 380: loss 1.7310, time 78.68ms
2024-09-10 08:20:20 iter 390: loss 1.7347, time 79.40ms
2024-09-10 08:20:21 iter 400: loss 1.7641, time 78.15ms
2024-09-10 08:20:22 iter 410: loss 1.7047, time 79.84ms
2024-09-10 08:20:23 iter 420: loss 1.7201, time 81.24ms
2024-09-10 08:20:23 iter 430: loss 1.6856, time 79.45ms
2024-09-10 08:20:24 iter 440: loss 1.6532, time 79.20ms
2024-09-10 08:20:25 iter 450: loss 1.6509, time 79.89ms
2024-09-10 08:20:26 iter 460: loss 1.5999, time 78.90ms
2024-09-10 08:20:27 iter 470: loss 1.6481, time 80.12ms
2024-09-10 08:20:27 iter 480: loss 1.6205, time 79.06ms
2024-09-10 08:20:28 iter 490: loss 1.5995, time 79.31ms
2024-09-10 08:20:39 step 500: train loss 1.5266, val loss 1.7248
2024-09-10 08:20:39 iter 500: loss 1.6026, time 10278.27ms
2024-09-10 08:20:40 iter 510: loss 1.6116, time 79.43ms
2024-09-10 08:20:41 iter 520: loss 1.5955, time 79.82ms
2024-09-10 08:20:41 iter 530: loss 1.5623, time 79.26ms
2024-09-10 08:20:42 iter 540: loss 1.6197, time 80.14ms
2024-09-10 08:20:43 iter 550: loss 1.5567, time 78.76ms
2024-09-10 08:20:44 iter 560: loss 1.5667, time 79.79ms
2024-09-10 08:20:45 iter 570: loss 1.5648, time 81.04ms
2024-09-10 08:20:45 iter 580: loss 1.5322, time 78.88ms
2024-09-10 08:20:46 iter 590: loss 1.5007, time 79.86ms
2024-09-10 08:20:47 iter 600: loss 1.5142, time 79.86ms
2024-09-10 08:20:48 iter 610: loss 1.5464, time 79.76ms
2024-09-10 08:20:49 iter 620: loss 1.5294, time 81.38ms
2024-09-10 08:20:49 iter 630: loss 1.5040, time 79.43ms
2024-09-10 08:20:50 iter 640: loss 1.4687, time 81.51ms
2024-09-10 08:20:51 iter 650: loss 1.5001, time 79.93ms
2024-09-10 08:20:52 iter 660: loss 1.5096, time 80.30ms
2024-09-10 08:20:53 iter 670: loss 1.4472, time 79.86ms
2024-09-10 08:20:53 iter 680: loss 1.5136, time 80.14ms
2024-09-10 08:20:54 iter 690: loss 1.4674, time 79.76ms
2024-09-10 08:20:55 iter 700: loss 1.4875, time 79.66ms
2024-09-10 08:20:56 iter 710: loss 1.4611, time 81.90ms
2024-09-10 08:20:57 iter 720: loss 1.4401, time 80.42ms
2024-09-10 08:20:58 iter 730: loss 1.4206, time 80.03ms
2024-09-10 08:20:58 iter 740: loss 1.4319, time 79.79ms
2024-09-10 08:21:09 step 750: train loss 1.3680, val loss 1.5978
2024-09-10 08:21:09 iter 750: loss 1.4274, time 10373.82ms
2024-09-10 08:21:10 iter 760: loss 1.4470, time 81.94ms
2024-09-10 08:21:11 iter 770: loss 1.4287, time 80.52ms
2024-09-10 08:21:12 iter 780: loss 1.4193, time 80.26ms
2024-09-10 08:21:12 iter 790: loss 1.4201, time 80.86ms
2024-09-10 08:21:13 iter 800: loss 1.4343, time 79.99ms
2024-09-10 08:21:14 iter 810: loss 1.4162, time 79.96ms
2024-09-10 08:21:15 iter 820: loss 1.4077, time 79.96ms
2024-09-10 08:21:16 iter 830: loss 1.4000, time 79.85ms
2024-09-10 08:21:16 iter 840: loss 1.4022, time 80.52ms
2024-09-10 08:21:17 iter 850: loss 1.3913, time 83.00ms
2024-09-10 08:21:18 iter 860: loss 1.3986, time 80.48ms
2024-09-10 08:21:19 iter 870: loss 1.3956, time 80.43ms
2024-09-10 08:21:20 iter 880: loss 1.3752, time 80.50ms
2024-09-10 08:21:21 iter 890: loss 1.3887, time 80.57ms
2024-09-10 08:21:21 iter 900: loss 1.3756, time 80.52ms
2024-09-10 08:21:22 iter 910: loss 1.3274, time 80.59ms
2024-09-10 08:21:23 iter 920: loss 1.3720, time 81.19ms
2024-09-10 08:21:24 iter 930: loss 1.3591, time 80.21ms
2024-09-10 08:21:25 iter 940: loss 1.3478, time 82.68ms
2024-09-10 08:21:25 iter 950: loss 1.3534, time 79.99ms
2024-09-10 08:21:26 iter 960: loss 1.3620, time 80.50ms
2024-09-10 08:21:27 iter 970: loss 1.3645, time 80.38ms
2024-09-10 08:21:28 iter 980: loss 1.3542, time 80.47ms
2024-09-10 08:21:29 iter 990: loss 1.3463, time 80.61ms
2024-09-10 08:21:39 step 1000: train loss 1.2797, val loss 1.5236
2024-09-10 08:21:40 iter 1000: loss 1.3401, time 10430.04ms
2024-09-10 08:21:40 iter 1010: loss 1.3410, time 82.18ms
2024-09-10 08:21:41 iter 1020: loss 1.3105, time 80.70ms
2024-09-10 08:21:42 iter 1030: loss 1.3387, time 80.85ms
2024-09-10 08:21:43 iter 1040: loss 1.3607, time 82.03ms
2024-09-10 08:21:44 iter 1050: loss 1.2935, time 80.86ms
2024-09-10 08:21:44 iter 1060: loss 1.3404, time 80.87ms
2024-09-10 08:21:45 iter 1070: loss 1.3319, time 81.24ms
2024-09-10 08:21:46 iter 1080: loss 1.3359, time 79.26ms
2024-09-10 08:21:47 iter 1090: loss 1.3490, time 81.00ms
2024-09-10 08:21:48 iter 1100: loss 1.3168, time 81.82ms
2024-09-10 08:21:49 iter 1110: loss 1.3049, time 80.56ms
2024-09-10 08:21:49 iter 1120: loss 1.2984, time 82.23ms
2024-09-10 08:21:50 iter 1130: loss 1.2987, time 80.93ms
2024-09-10 08:21:51 iter 1140: loss 1.3074, time 80.96ms
2024-09-10 08:21:52 iter 1150: loss 1.3093, time 80.85ms
2024-09-10 08:21:53 iter 1160: loss 1.3300, time 81.16ms
2024-09-10 08:21:53 iter 1170: loss 1.3040, time 80.74ms
2024-09-10 08:21:54 iter 1180: loss 1.3265, time 80.96ms
2024-09-10 08:21:55 iter 1190: loss 1.2728, time 81.22ms
2024-09-10 08:21:56 iter 1200: loss 1.2882, time 80.91ms
2024-09-10 08:21:57 iter 1210: loss 1.2638, time 82.41ms
2024-09-10 08:21:57 iter 1220: loss 1.3122, time 81.31ms
2024-09-10 08:21:58 iter 1230: loss 1.2958, time 81.04ms
2024-09-10 08:21:59 iter 1240: loss 1.3049, time 80.61ms
2024-09-10 08:22:10 step 1250: train loss 1.2078, val loss 1.4901
2024-09-10 08:22:10 iter 1250: loss 1.2748, time 10447.11ms
2024-09-10 08:22:11 iter 1260: loss 1.2780, time 81.56ms
2024-09-10 08:22:12 iter 1270: loss 1.2683, time 80.90ms
2024-09-10 08:22:13 iter 1280: loss 1.2563, time 81.00ms
2024-09-10 08:22:13 iter 1290: loss 1.2844, time 80.70ms
2024-09-10 08:22:14 iter 1300: loss 1.3065, time 82.37ms
2024-09-10 08:22:15 iter 1310: loss 1.2391, time 80.90ms
2024-09-10 08:22:16 iter 1320: loss 1.3143, time 82.56ms
2024-09-10 08:22:17 iter 1330: loss 1.2704, time 80.80ms
2024-09-10 08:22:17 iter 1340: loss 1.3062, time 80.93ms
2024-09-10 08:22:18 iter 1350: loss 1.2576, time 81.56ms
2024-09-10 08:22:19 iter 1360: loss 1.2806, time 80.78ms
2024-09-10 08:22:20 iter 1370: loss 1.2601, time 81.06ms
2024-09-10 08:22:21 iter 1380: loss 1.2671, time 80.90ms
2024-09-10 08:22:21 iter 1390: loss 1.2539, time 80.55ms
2024-09-10 08:22:22 iter 1400: loss 1.2507, time 81.15ms
2024-09-10 08:22:23 iter 1410: loss 1.2504, time 81.99ms
2024-09-10 08:22:24 iter 1420: loss 1.2723, time 80.92ms
2024-09-10 08:22:25 iter 1430: loss 1.2432, time 81.10ms
2024-09-10 08:22:26 iter 1440: loss 1.2572, time 81.07ms
2024-09-10 08:22:26 iter 1450: loss 1.2278, time 80.87ms
2024-09-10 08:22:27 iter 1460: loss 1.2467, time 81.23ms
2024-09-10 08:22:28 iter 1470: loss 1.2292, time 81.61ms
2024-09-10 08:22:29 iter 1480: loss 1.2184, time 81.66ms
2024-09-10 08:22:30 iter 1490: loss 1.2405, time 80.83ms
2024-09-10 08:22:41 step 1500: train loss 1.1539, val loss 1.4734
2024-09-10 08:22:41 iter 1500: loss 1.1899, time 10467.88ms
2024-09-10 08:22:41 iter 1510: loss 1.2338, time 81.62ms
2024-09-10 08:22:42 iter 1520: loss 1.2314, time 82.71ms
2024-09-10 08:22:43 iter 1530: loss 1.2541, time 81.21ms
2024-09-10 08:22:44 iter 1540: loss 1.1956, time 82.68ms
2024-09-10 08:22:45 iter 1550: loss 1.2394, time 81.14ms
2024-09-10 08:22:46 iter 1560: loss 1.2038, time 80.92ms
2024-09-10 08:22:46 iter 1570: loss 1.2416, time 80.91ms
2024-09-10 08:22:47 iter 1580: loss 1.2018, time 81.17ms
2024-09-10 08:22:48 iter 1590: loss 1.1959, time 81.19ms
2024-09-10 08:22:49 iter 1600: loss 1.1980, time 81.49ms
2024-09-10 08:22:50 iter 1610: loss 1.2407, time 80.90ms
2024-09-10 08:22:50 iter 1620: loss 1.1921, time 81.28ms
2024-09-10 08:22:51 iter 1630: loss 1.2125, time 83.59ms
2024-09-10 08:22:52 iter 1640: loss 1.2025, time 81.63ms
2024-09-10 08:22:53 iter 1650: loss 1.1886, time 83.00ms
2024-09-10 08:22:54 iter 1660: loss 1.2223, time 81.16ms
2024-09-10 08:22:55 iter 1670: loss 1.1983, time 81.28ms
2024-09-10 08:22:55 iter 1680: loss 1.1996, time 81.19ms
2024-09-10 08:22:56 iter 1690: loss 1.2062, time 81.29ms
2024-09-10 08:22:57 iter 1700: loss 1.1826, time 80.87ms
2024-09-10 08:22:58 iter 1710: loss 1.1774, time 80.95ms
2024-09-10 08:22:59 iter 1720: loss 1.1869, time 80.87ms
2024-09-10 08:22:59 iter 1730: loss 1.2056, time 81.12ms
2024-09-10 08:23:00 iter 1740: loss 1.1747, time 83.98ms
2024-09-10 08:23:11 step 1750: train loss 1.1065, val loss 1.4731
2024-09-10 08:23:11 iter 1750: loss 1.1874, time 10497.03ms
2024-09-10 08:23:12 iter 1760: loss 1.1895, time 80.87ms
2024-09-10 08:23:13 iter 1770: loss 1.1979, time 81.58ms
2024-09-10 08:23:14 iter 1780: loss 1.1969, time 82.62ms
2024-09-10 08:23:15 iter 1790: loss 1.1964, time 81.22ms
2024-09-10 08:23:15 iter 1800: loss 1.1788, time 81.16ms
2024-09-10 08:23:16 iter 1810: loss 1.1580, time 80.90ms
2024-09-10 08:23:17 iter 1820: loss 1.1720, time 80.95ms
2024-09-10 08:23:18 iter 1830: loss 1.1742, time 81.11ms
2024-09-10 08:23:19 iter 1840: loss 1.1647, time 81.02ms
2024-09-10 08:23:19 iter 1850: loss 1.1606, time 81.21ms
2024-09-10 08:23:20 iter 1860: loss 1.1821, time 81.64ms
2024-09-10 08:23:21 iter 1870: loss 1.1438, time 82.66ms
2024-09-10 08:23:22 iter 1880: loss 1.1828, time 81.51ms
2024-09-10 08:23:23 iter 1890: loss 1.1821, time 83.07ms
2024-09-10 08:23:24 iter 1900: loss 1.1363, time 81.64ms
2024-09-10 08:23:24 iter 1910: loss 1.1713, time 81.24ms
2024-09-10 08:23:25 iter 1920: loss 1.1731, time 81.30ms
2024-09-10 08:23:26 iter 1930: loss 1.1483, time 81.16ms
2024-09-10 08:23:27 iter 1940: loss 1.1261, time 81.14ms
2024-09-10 08:23:28 iter 1950: loss 1.1369, time 80.86ms
2024-09-10 08:23:28 iter 1960: loss 1.1495, time 80.84ms
2024-09-10 08:23:29 iter 1970: loss 1.1522, time 81.48ms
2024-09-10 08:23:30 iter 1980: loss 1.1551, time 83.34ms
2024-09-10 08:23:31 iter 1990: loss 1.1597, time 81.81ms
2024-09-10 08:23:42 step 2000: train loss 1.0592, val loss 1.4838
2024-09-10 08:23:42 iter 2000: loss 1.1297, time 10463.57ms
2024-09-10 08:23:43 iter 2010: loss 1.1299, time 80.91ms
2024-09-10 08:23:43 iter 2020: loss 1.1193, time 81.36ms
2024-09-10 08:23:44 iter 2030: loss 1.1613, time 81.36ms
2024-09-10 08:23:45 iter 2040: loss 1.1425, time 81.09ms
2024-09-10 08:23:46 iter 2050: loss 1.1115, time 81.63ms
2024-09-10 08:23:47 iter 2060: loss 1.1036, time 81.36ms
2024-09-10 08:23:48 iter 2070: loss 1.1337, time 81.19ms
2024-09-10 08:23:48 iter 2080: loss 1.1190, time 81.02ms
2024-09-10 08:23:49 iter 2090: loss 1.1344, time 81.21ms
2024-09-10 08:23:50 iter 2100: loss 1.1337, time 81.00ms
2024-09-10 08:23:51 iter 2110: loss 1.1364, time 83.55ms
2024-09-10 08:23:52 iter 2120: loss 1.1325, time 81.59ms
2024-09-10 08:23:53 iter 2130: loss 1.1467, time 82.19ms
2024-09-10 08:23:53 iter 2140: loss 1.1401, time 81.51ms
2024-09-10 08:23:54 iter 2150: loss 1.1267, time 81.51ms
2024-09-10 08:23:55 iter 2160: loss 1.1442, time 81.75ms
2024-09-10 08:23:56 iter 2170: loss 1.1359, time 81.38ms
2024-09-10 08:23:57 iter 2180: loss 1.1196, time 81.69ms
2024-09-10 08:23:57 iter 2190: loss 1.1117, time 81.58ms
2024-09-10 08:23:58 iter 2200: loss 1.1310, time 81.93ms
2024-09-10 08:23:59 iter 2210: loss 1.1201, time 81.49ms
2024-09-10 08:24:00 iter 2220: loss 1.1267, time 82.60ms
2024-09-10 08:24:01 iter 2230: loss 1.1253, time 82.19ms
2024-09-10 08:24:02 iter 2240: loss 1.1303, time 82.68ms
2024-09-10 08:24:12 step 2250: train loss 1.0127, val loss 1.4856
2024-09-10 08:24:13 iter 2250: loss 1.1145, time 10483.82ms
2024-09-10 08:24:13 iter 2260: loss 1.1103, time 83.13ms
2024-09-10 08:24:14 iter 2270: loss 1.1309, time 81.40ms
2024-09-10 08:24:15 iter 2280: loss 1.1017, time 82.54ms
2024-09-10 08:24:16 iter 2290: loss 1.1430, time 82.12ms
2024-09-10 08:24:17 iter 2300: loss 1.1260, time 81.50ms
2024-09-10 08:24:17 iter 2310: loss 1.0948, time 81.50ms
2024-09-10 08:24:18 iter 2320: loss 1.0967, time 81.47ms
2024-09-10 08:24:19 iter 2330: loss 1.1039, time 80.96ms
2024-09-10 08:24:20 iter 2340: loss 1.1237, time 81.90ms
2024-09-10 08:24:21 iter 2350: loss 1.1068, time 81.25ms
2024-09-10 08:24:22 iter 2360: loss 1.1103, time 81.47ms
2024-09-10 08:24:22 iter 2370: loss 1.0906, time 82.27ms
2024-09-10 08:24:23 iter 2380: loss 1.0844, time 81.17ms
2024-09-10 08:24:24 iter 2390: loss 1.0746, time 81.04ms
2024-09-10 08:24:25 iter 2400: loss 1.0798, time 81.05ms
2024-09-10 08:24:26 iter 2410: loss 1.0630, time 81.39ms
2024-09-10 08:24:26 iter 2420: loss 1.0808, time 81.36ms
2024-09-10 08:24:27 iter 2430: loss 1.0583, time 81.07ms
2024-09-10 08:24:28 iter 2440: loss 1.0665, time 81.11ms
2024-09-10 08:24:29 iter 2450: loss 1.0719, time 81.40ms
2024-09-10 08:24:30 iter 2460: loss 1.0856, time 80.95ms
2024-09-10 08:24:31 iter 2470: loss 1.0890, time 81.61ms
2024-09-10 08:24:31 iter 2480: loss 1.0930, time 82.34ms
2024-09-10 08:24:32 iter 2490: loss 1.0550, time 81.54ms
2024-09-10 08:24:43 step 2500: train loss 0.9650, val loss 1.4938
2024-09-10 08:24:43 iter 2500: loss 1.0825, time 10511.77ms
2024-09-10 08:24:44 iter 2510: loss 1.0766, time 82.65ms
2024-09-10 08:24:45 iter 2520: loss 1.0430, time 83.42ms
2024-09-10 08:24:46 iter 2530: loss 1.0537, time 81.66ms
2024-09-10 08:24:47 iter 2540: loss 1.0546, time 81.54ms
2024-09-10 08:24:47 iter 2550: loss 1.0783, time 81.16ms
2024-09-10 08:24:48 iter 2560: loss 1.0607, time 81.19ms
2024-09-10 08:24:49 iter 2570: loss 1.0705, time 80.80ms
2024-09-10 08:24:50 iter 2580: loss 1.0799, time 81.27ms
2024-09-10 08:24:51 iter 2590: loss 1.0650, time 81.37ms
2024-09-10 08:24:51 iter 2600: loss 1.0686, time 81.33ms
2024-09-10 08:24:52 iter 2610: loss 1.0580, time 80.96ms
2024-09-10 08:24:53 iter 2620: loss 1.0430, time 83.36ms
2024-09-10 08:24:54 iter 2630: loss 1.0256, time 82.46ms
2024-09-10 08:24:55 iter 2640: loss 1.0495, time 81.64ms
2024-09-10 08:24:56 iter 2650: loss 1.0708, time 83.83ms
2024-09-10 08:24:56 iter 2660: loss 1.0436, time 81.58ms
2024-09-10 08:24:57 iter 2670: loss 1.0204, time 81.49ms
2024-09-10 08:24:58 iter 2680: loss 1.0494, time 81.37ms
2024-09-10 08:24:59 iter 2690: loss 1.0584, time 81.91ms
2024-09-10 08:25:00 iter 2700: loss 1.0219, time 81.53ms
2024-09-10 08:25:00 iter 2710: loss 1.0516, time 80.98ms
2024-09-10 08:25:01 iter 2720: loss 1.0481, time 81.40ms
2024-09-10 08:25:02 iter 2730: loss 1.0554, time 81.30ms
2024-09-10 08:25:03 iter 2740: loss 1.0237, time 80.92ms
2024-09-10 08:25:14 step 2750: train loss 0.9169, val loss 1.5176
2024-09-10 08:25:14 iter 2750: loss 1.0317, time 10520.84ms
2024-09-10 08:25:15 iter 2760: loss 1.0337, time 81.81ms
2024-09-10 08:25:16 iter 2770: loss 1.0170, time 81.55ms
2024-09-10 08:25:16 iter 2780: loss 1.0228, time 80.97ms
2024-09-10 08:25:17 iter 2790: loss 1.0394, time 81.35ms
2024-09-10 08:25:18 iter 2800: loss 1.0102, time 82.53ms
2024-09-10 08:25:19 iter 2810: loss 1.0421, time 80.92ms
2024-09-10 08:25:20 iter 2820: loss 1.0240, time 81.36ms
2024-09-10 08:25:21 iter 2830: loss 1.0356, time 81.63ms
2024-09-10 08:25:21 iter 2840: loss 0.9938, time 81.00ms
2024-09-10 08:25:22 iter 2850: loss 1.0337, time 82.22ms
2024-09-10 08:25:23 iter 2860: loss 1.0218, time 81.90ms
2024-09-10 08:25:24 iter 2870: loss 1.0156, time 81.59ms
2024-09-10 08:25:25 iter 2880: loss 1.0386, time 81.42ms
2024-09-10 08:25:25 iter 2890: loss 1.0104, time 82.08ms
2024-09-10 08:25:26 iter 2900: loss 0.9965, time 81.57ms
2024-09-10 08:25:27 iter 2910: loss 1.0470, time 82.98ms
2024-09-10 08:25:28 iter 2920: loss 1.0093, time 81.80ms
2024-09-10 08:25:29 iter 2930: loss 1.0019, time 82.76ms
2024-09-10 08:25:30 iter 2940: loss 0.9897, time 81.88ms
2024-09-10 08:25:30 iter 2950: loss 1.0209, time 82.52ms
2024-09-10 08:25:31 iter 2960: loss 0.9951, time 81.30ms
2024-09-10 08:25:32 iter 2970: loss 0.9995, time 81.29ms
2024-09-10 08:25:33 iter 2980: loss 0.9978, time 81.42ms
2024-09-10 08:25:34 iter 2990: loss 0.9819, time 81.29ms
2024-09-10 08:25:45 step 3000: train loss 0.8704, val loss 1.5303
2024-09-10 08:25:45 iter 3000: loss 0.9905, time 10498.26ms
2024-09-10 08:25:45 iter 3010: loss 1.0004, time 81.11ms
2024-09-10 08:25:46 iter 3020: loss 1.0055, time 80.89ms
2024-09-10 08:25:47 iter 3030: loss 1.0088, time 81.56ms
2024-09-10 08:25:48 iter 3040: loss 1.0209, time 81.24ms
2024-09-10 08:25:49 iter 3050: loss 0.9823, time 81.96ms
2024-09-10 08:25:50 iter 3060: loss 0.9924, time 82.57ms
2024-09-10 08:25:50 iter 3070: loss 1.0205, time 82.13ms
2024-09-10 08:25:51 iter 3080: loss 0.9991, time 83.68ms
2024-09-10 08:25:52 iter 3090: loss 0.9784, time 82.74ms
2024-09-10 08:25:53 iter 3100: loss 0.9973, time 81.84ms
2024-09-10 08:25:54 iter 3110: loss 0.9768, time 81.85ms
2024-09-10 08:25:55 iter 3120: loss 0.9973, time 81.10ms
2024-09-10 08:25:55 iter 3130: loss 0.9802, time 81.70ms
2024-09-10 08:25:56 iter 3140: loss 0.9764, time 81.68ms
2024-09-10 08:25:57 iter 3150: loss 1.0022, time 81.96ms
2024-09-10 08:25:58 iter 3160: loss 1.0059, time 82.59ms
2024-09-10 08:25:59 iter 3170: loss 0.9565, time 81.85ms
2024-09-10 08:25:59 iter 3180: loss 0.9785, time 80.98ms
2024-09-10 08:26:00 iter 3190: loss 0.9919, time 84.36ms
2024-09-10 08:26:01 iter 3200: loss 0.9690, time 81.71ms
2024-09-10 08:26:02 iter 3210: loss 0.9669, time 83.09ms
2024-09-10 08:26:03 iter 3220: loss 0.9564, time 82.06ms
2024-09-10 08:26:04 iter 3230: loss 0.9536, time 81.69ms
2024-09-10 08:26:04 iter 3240: loss 0.9577, time 81.29ms
2024-09-10 08:26:15 step 3250: train loss 0.8247, val loss 1.5664
2024-09-10 08:26:15 iter 3250: loss 0.9658, time 10499.15ms
2024-09-10 08:26:16 iter 3260: loss 0.9694, time 81.34ms
2024-09-10 08:26:17 iter 3270: loss 0.9657, time 81.30ms
2024-09-10 08:26:18 iter 3280: loss 0.9586, time 81.66ms
2024-09-10 08:26:19 iter 3290: loss 0.9380, time 81.29ms
2024-09-10 08:26:19 iter 3300: loss 0.9455, time 81.24ms
2024-09-10 08:26:20 iter 3310: loss 0.9526, time 81.39ms
2024-09-10 08:26:21 iter 3320: loss 0.9670, time 80.99ms
2024-09-10 08:26:22 iter 3330: loss 0.9617, time 81.23ms
2024-09-10 08:26:23 iter 3340: loss 0.9673, time 82.89ms
2024-09-10 08:26:24 iter 3350: loss 0.9593, time 81.67ms
2024-09-10 08:26:24 iter 3360: loss 0.9282, time 83.18ms
2024-09-10 08:26:25 iter 3370: loss 0.9653, time 81.59ms
2024-09-10 08:26:26 iter 3380: loss 0.9534, time 81.75ms
2024-09-10 08:26:27 iter 3390: loss 0.9541, time 81.72ms
2024-09-10 08:26:28 iter 3400: loss 0.9503, time 82.09ms
2024-09-10 08:26:29 iter 3410: loss 0.9475, time 81.32ms
2024-09-10 08:26:29 iter 3420: loss 0.9501, time 81.33ms
2024-09-10 08:26:30 iter 3430: loss 0.9475, time 81.76ms
2024-09-10 08:26:31 iter 3440: loss 0.9823, time 82.24ms
2024-09-10 08:26:32 iter 3450: loss 0.9563, time 81.81ms
2024-09-10 08:26:33 iter 3460: loss 0.9541, time 81.67ms
2024-09-10 08:26:33 iter 3470: loss 0.9430, time 81.79ms
2024-09-10 08:26:34 iter 3480: loss 0.9472, time 81.88ms
2024-09-10 08:26:35 iter 3490: loss 0.9105, time 83.10ms
2024-09-10 08:26:46 step 3500: train loss 0.7824, val loss 1.5808
2024-09-10 08:26:46 iter 3500: loss 0.9082, time 10523.23ms
2024-09-10 08:26:47 iter 3510: loss 0.9150, time 83.40ms
2024-09-10 08:26:48 iter 3520: loss 0.9266, time 82.31ms
2024-09-10 08:26:49 iter 3530: loss 0.9613, time 82.96ms
2024-09-10 08:26:49 iter 3540: loss 0.9305, time 82.88ms
2024-09-10 08:26:50 iter 3550: loss 0.9232, time 81.09ms
2024-09-10 08:26:51 iter 3560: loss 0.9555, time 81.15ms
2024-09-10 08:26:52 iter 3570: loss 0.9447, time 82.20ms
2024-09-10 08:26:53 iter 3580: loss 0.9267, time 81.97ms
2024-09-10 08:26:54 iter 3590: loss 0.9132, time 81.96ms
2024-09-10 08:26:54 iter 3600: loss 0.9225, time 82.04ms
2024-09-10 08:26:55 iter 3610: loss 0.9107, time 81.77ms
2024-09-10 08:26:56 iter 3620: loss 0.9138, time 81.09ms
2024-09-10 08:26:57 iter 3630: loss 0.9226, time 81.75ms
2024-09-10 08:26:58 iter 3640: loss 0.9202, time 82.41ms
2024-09-10 08:26:58 iter 3650: loss 0.9094, time 81.66ms
2024-09-10 08:26:59 iter 3660: loss 0.9393, time 83.02ms
2024-09-10 08:27:00 iter 3670: loss 0.9379, time 81.44ms
2024-09-10 08:27:01 iter 3680: loss 0.9202, time 81.64ms
2024-09-10 08:27:02 iter 3690: loss 0.9339, time 81.66ms
2024-09-10 08:27:03 iter 3700: loss 0.8770, time 81.86ms
2024-09-10 08:27:03 iter 3710: loss 0.8848, time 81.16ms
2024-09-10 08:27:04 iter 3720: loss 0.9052, time 81.34ms
2024-09-10 08:27:05 iter 3730: loss 0.9013, time 81.70ms
2024-09-10 08:27:06 iter 3740: loss 0.9065, time 81.85ms
2024-09-10 08:27:17 step 3750: train loss 0.7449, val loss 1.6063
2024-09-10 08:27:17 iter 3750: loss 0.9069, time 10517.89ms
2024-09-10 08:27:18 iter 3760: loss 0.9438, time 81.69ms
2024-09-10 08:27:19 iter 3770: loss 0.9307, time 82.90ms
2024-09-10 08:27:19 iter 3780: loss 0.9237, time 81.32ms
2024-09-10 08:27:20 iter 3790: loss 0.9052, time 81.32ms
2024-09-10 08:27:21 iter 3800: loss 0.9166, time 81.59ms
2024-09-10 08:27:22 iter 3810: loss 0.9170, time 82.70ms
2024-09-10 08:27:23 iter 3820: loss 0.8880, time 82.20ms
2024-09-10 08:27:23 iter 3830: loss 0.8986, time 82.96ms
2024-09-10 08:27:24 iter 3840: loss 0.8869, time 81.80ms
2024-09-10 08:27:25 iter 3850: loss 0.8928, time 81.85ms
2024-09-10 08:27:26 iter 3860: loss 0.8710, time 81.03ms
2024-09-10 08:27:27 iter 3870: loss 0.8891, time 81.74ms
2024-09-10 08:27:28 iter 3880: loss 0.8982, time 81.16ms
2024-09-10 08:27:28 iter 3890: loss 0.8898, time 82.20ms
2024-09-10 08:27:29 iter 3900: loss 0.8898, time 82.54ms
2024-09-10 08:27:30 iter 3910: loss 0.8873, time 81.54ms
2024-09-10 08:27:31 iter 3920: loss 0.8728, time 81.24ms
2024-09-10 08:27:32 iter 3930: loss 0.8929, time 80.94ms
2024-09-10 08:27:33 iter 3940: loss 0.8861, time 80.93ms
2024-09-10 08:27:33 iter 3950: loss 0.8839, time 81.56ms
2024-09-10 08:27:34 iter 3960: loss 0.9118, time 83.54ms
2024-09-10 08:27:35 iter 3970: loss 0.8958, time 81.31ms
2024-09-10 08:27:36 iter 3980: loss 0.8952, time 81.36ms
2024-09-10 08:27:37 iter 3990: loss 0.8747, time 81.17ms
2024-09-10 08:27:48 step 4000: train loss 0.7125, val loss 1.6309
2024-09-10 08:27:48 iter 4000: loss 0.8617, time 10525.13ms
2024-09-10 08:27:48 iter 4010: loss 0.8825, time 82.07ms
2024-09-10 08:27:49 iter 4020: loss 0.8879, time 81.91ms
2024-09-10 08:27:50 iter 4030: loss 0.8844, time 81.74ms
2024-09-10 08:27:51 iter 4040: loss 0.8863, time 82.32ms
2024-09-10 08:27:52 iter 4050: loss 0.8718, time 81.18ms
2024-09-10 08:27:53 iter 4060: loss 0.8730, time 82.12ms
2024-09-10 08:27:53 iter 4070: loss 0.8662, time 82.13ms
2024-09-10 08:27:54 iter 4080: loss 0.8882, time 81.82ms
2024-09-10 08:27:55 iter 4090: loss 0.8567, time 81.33ms
2024-09-10 08:27:56 iter 4100: loss 0.9046, time 81.39ms
2024-09-10 08:27:57 iter 4110: loss 0.8768, time 82.74ms
2024-09-10 08:27:58 iter 4120: loss 0.8791, time 81.47ms
2024-09-10 08:27:58 iter 4130: loss 0.8720, time 83.35ms
2024-09-10 08:27:59 iter 4140: loss 0.8814, time 81.61ms
2024-09-10 08:28:00 iter 4150: loss 0.8710, time 81.89ms
2024-09-10 08:28:01 iter 4160: loss 0.8576, time 81.36ms
2024-09-10 08:28:02 iter 4170: loss 0.8711, time 81.37ms
2024-09-10 08:28:02 iter 4180: loss 0.8687, time 81.27ms
2024-09-10 08:28:03 iter 4190: loss 0.8794, time 81.39ms
2024-09-10 08:28:04 iter 4200: loss 0.8646, time 81.40ms
2024-09-10 08:28:05 iter 4210: loss 0.8738, time 81.78ms
2024-09-10 08:28:06 iter 4220: loss 0.8609, time 82.68ms
2024-09-10 08:28:07 iter 4230: loss 0.8787, time 81.27ms
2024-09-10 08:28:07 iter 4240: loss 0.8720, time 83.46ms
2024-09-10 08:28:18 step 4250: train loss 0.6806, val loss 1.6512
2024-09-10 08:28:18 iter 4250: loss 0.8690, time 10514.51ms
2024-09-10 08:28:19 iter 4260: loss 0.8597, time 82.98ms
2024-09-10 08:28:20 iter 4270: loss 0.8613, time 81.86ms
2024-09-10 08:28:21 iter 4280: loss 0.8528, time 83.44ms
2024-09-10 08:28:22 iter 4290: loss 0.8430, time 81.51ms
2024-09-10 08:28:22 iter 4300: loss 0.8231, time 81.41ms
2024-09-10 08:28:23 iter 4310: loss 0.8567, time 80.87ms
2024-09-10 08:28:24 iter 4320: loss 0.8407, time 82.16ms
2024-09-10 08:28:25 iter 4330: loss 0.8579, time 81.40ms
2024-09-10 08:28:26 iter 4340: loss 0.8417, time 81.84ms
2024-09-10 08:28:27 iter 4350: loss 0.8441, time 82.06ms
2024-09-10 08:28:27 iter 4360: loss 0.8514, time 82.00ms
2024-09-10 08:28:28 iter 4370: loss 0.8517, time 81.11ms
2024-09-10 08:28:29 iter 4380: loss 0.8383, time 81.61ms
2024-09-10 08:28:30 iter 4390: loss 0.8666, time 82.88ms
2024-09-10 08:28:31 iter 4400: loss 0.8415, time 81.36ms
2024-09-10 08:28:32 iter 4410: loss 0.8604, time 83.40ms
2024-09-10 08:28:32 iter 4420: loss 0.8639, time 82.55ms
2024-09-10 08:28:33 iter 4430: loss 0.8424, time 82.17ms
2024-09-10 08:28:34 iter 4440: loss 0.8517, time 82.55ms
2024-09-10 08:28:35 iter 4450: loss 0.8444, time 81.46ms
2024-09-10 08:28:36 iter 4460: loss 0.8401, time 81.52ms
2024-09-10 08:28:36 iter 4470: loss 0.8512, time 81.69ms
2024-09-10 08:28:37 iter 4480: loss 0.8283, time 81.95ms
2024-09-10 08:28:38 iter 4490: loss 0.8424, time 82.22ms
2024-09-10 08:28:49 step 4500: train loss 0.6548, val loss 1.6743
2024-09-10 08:28:49 iter 4500: loss 0.8600, time 10519.85ms
2024-09-10 08:28:50 iter 4510: loss 0.8555, time 82.14ms
2024-09-10 08:28:51 iter 4520: loss 0.8325, time 81.76ms
2024-09-10 08:28:52 iter 4530: loss 0.8574, time 82.18ms
2024-09-10 08:28:52 iter 4540: loss 0.8500, time 81.58ms
2024-09-10 08:28:53 iter 4550: loss 0.8686, time 81.80ms
2024-09-10 08:28:54 iter 4560: loss 0.8538, time 83.09ms
2024-09-10 08:28:55 iter 4570: loss 0.8406, time 82.13ms
2024-09-10 08:28:56 iter 4580: loss 0.8561, time 83.80ms
2024-09-10 08:28:57 iter 4590: loss 0.8532, time 82.17ms
2024-09-10 08:28:57 iter 4600: loss 0.8275, time 82.20ms
2024-09-10 08:28:58 iter 4610: loss 0.8655, time 81.39ms
2024-09-10 08:28:59 iter 4620: loss 0.8382, time 81.33ms
2024-09-10 08:29:00 iter 4630: loss 0.8267, time 82.07ms
2024-09-10 08:29:01 iter 4640: loss 0.8359, time 81.67ms
2024-09-10 08:29:01 iter 4650: loss 0.8630, time 81.92ms
2024-09-10 08:29:02 iter 4660: loss 0.8494, time 81.71ms
2024-09-10 08:29:03 iter 4670: loss 0.8357, time 82.11ms
2024-09-10 08:29:04 iter 4680: loss 0.8582, time 81.53ms
2024-09-10 08:29:05 iter 4690: loss 0.8370, time 82.57ms
2024-09-10 08:29:06 iter 4700: loss 0.8203, time 82.16ms
2024-09-10 08:29:06 iter 4710: loss 0.7903, time 83.04ms
2024-09-10 08:29:07 iter 4720: loss 0.8296, time 81.92ms
2024-09-10 08:29:08 iter 4730: loss 0.8190, time 82.72ms
2024-09-10 08:29:09 iter 4740: loss 0.8314, time 81.74ms
2024-09-10 08:29:20 step 4750: train loss 0.6383, val loss 1.6859
2024-09-10 08:29:20 iter 4750: loss 0.8078, time 10506.01ms
2024-09-10 08:29:21 iter 4760: loss 0.8186, time 80.95ms
2024-09-10 08:29:22 iter 4770: loss 0.8022, time 81.15ms
2024-09-10 08:29:22 iter 4780: loss 0.8133, time 81.27ms
2024-09-10 08:29:23 iter 4790: loss 0.8367, time 81.40ms
2024-09-10 08:29:24 iter 4800: loss 0.8249, time 81.68ms
2024-09-10 08:29:25 iter 4810: loss 0.8431, time 81.32ms
2024-09-10 08:29:26 iter 4820: loss 0.8230, time 81.47ms
2024-09-10 08:29:26 iter 4830: loss 0.8288, time 81.64ms
2024-09-10 08:29:27 iter 4840: loss 0.8214, time 82.38ms
2024-09-10 08:29:28 iter 4850: loss 0.8253, time 81.89ms
2024-09-10 08:29:29 iter 4860: loss 0.8234, time 82.86ms
2024-09-10 08:29:30 iter 4870: loss 0.8037, time 81.98ms
2024-09-10 08:29:31 iter 4880: loss 0.8342, time 81.38ms
2024-09-10 08:29:31 iter 4890: loss 0.8084, time 81.34ms
2024-09-10 08:29:32 iter 4900: loss 0.8058, time 82.37ms
2024-09-10 08:29:33 iter 4910: loss 0.8230, time 81.81ms
2024-09-10 08:29:34 iter 4920: loss 0.8201, time 81.87ms
2024-09-10 08:29:35 iter 4930: loss 0.8087, time 82.10ms
2024-09-10 08:29:36 iter 4940: loss 0.7986, time 81.25ms
2024-09-10 08:29:36 iter 4950: loss 0.8279, time 81.03ms
2024-09-10 08:29:37 iter 4960: loss 0.8294, time 81.68ms
2024-09-10 08:29:38 iter 4970: loss 0.8015, time 83.12ms
2024-09-10 08:29:39 iter 4980: loss 0.8006, time 82.14ms
2024-09-10 08:29:40 iter 4990: loss 0.8179, time 83.24ms
2024-09-10 08:29:51 step 5000: train loss 0.6232, val loss 1.7081
2024-09-10 08:29:51 iter 5000: loss 0.8201, time 10519.67ms
2024-09-10 08:29:51 training done
2024-09-10 08:29:51 Best validation loss: 1.473085880279541
2024-09-10 08:29:51 Total train time: 10.63 mins
2024-09-10 08:29:51 Loading meta from ../../data/shakespeare_char/meta.pkl...
2024-09-10 08:29:53 Sample 1:
2024-09-10 08:29:53  beasts,
2024-09-10 08:29:53 And slain not one of these woes things sweat.
2024-09-10 08:29:53 
2024-09-10 08:29:53 NORTHUMBERLAND:
2024-09-10 08:29:53 As much in me; and therefore for I stumbly.
2024-09-10 08:29:53 
2024-09-10 08:29:53 LORD ROSS:
2024-09-10 08:29:53 So shall I see, that thou sheer'st the fairest nor king.
2024-09-10 08:29:53 
2024-09-10 08:29:53 NORTHUMBERLAND:
2024-09-10 08:29:53 A heaven stand on and great shrubs:
2024-09-10 08:29:53 And yet victory subjects mortal world to their weeping!
2024-09-10 08:29:53 They say there best the like trembling draws
2024-09-10 08:29:53 Shall be saved as they see their course, and not their wives
2024-09-10 08:29:53 With some three terrors than would speak with use
2024-09-10 08:29:53 Their contrary last of themselves. It is not
2024-09-10 08:29:53 Theirs
2024-09-10 08:29:53 Inference time: 2.57 seconds
2024-09-10 08:29:53 Tokens per second: 194.50
2024-09-10 08:29:53 ---------------
2024-09-10 08:29:55 Sample 2:
2024-09-10 08:29:55  such evil scratches, makes her heavenly heart,
2024-09-10 08:29:55 That I may hear them and threaten her with her babes
2024-09-10 08:29:55 Of what is here to make her brother life.
2024-09-10 08:29:55 
2024-09-10 08:29:55 KING RICHARD III:
2024-09-10 08:29:55 Thou art a tidingling prince, there thou stirst,
2024-09-10 08:29:55 Wilt thou ask me what to Saint George father;
2024-09-10 08:29:55 And, for thy ancient deep is thine.
2024-09-10 08:29:55 
2024-09-10 08:29:55 CATESBY:
2024-09-10 08:29:55 But I did kill thee.
2024-09-10 08:29:55 
2024-09-10 08:29:55 HASTINGS:
2024-09-10 08:29:55 Post you to him, man; I hope he is too hot.
2024-09-10 08:29:55 
2024-09-10 08:29:55 BUCKINGHAM:
2024-09-10 08:29:55 I pray you, my lord, will not away the latter.
2024-09-10 08:29:55 
2024-09-10 08:29:55 GLOUCESTER:
2024-09-10 08:29:55 How! dame not, my lord?
2024-09-10 08:29:55 
2024-09-10 08:29:55 BUCKINGHAM:
2024-09-10 08:29:55 Nor none, by
2024-09-10 08:29:55 Inference time: 2.07 seconds
2024-09-10 08:29:55 Tokens per second: 241.99
2024-09-10 08:29:55 ---------------
2024-09-10 08:29:57 Sample 3:
2024-09-10 08:29:57  bastard be there?
2024-09-10 08:29:57 
2024-09-10 08:29:57 CLAUDIO:
2024-09-10 08:29:57 I will not do't there.
2024-09-10 08:29:57 
2024-09-10 08:29:57 ISABELLA:
2024-09-10 08:29:57 Yes, the thing the untainted friends together,
2024-09-10 08:29:57 But that this hath absent, that thou hast beard,
2024-09-10 08:29:57 Unless thou mean to see me; and my good
2024-09-10 08:29:57 To hope that I had writ. My father, if my resign
2024-09-10 08:29:57 We should have some influence together, and my ends
2024-09-10 08:29:57 Unto the seat of it. O, here comes my friend,
2024-09-10 08:29:57 Or else her writ downright the morning of my foot,
2024-09-10 08:29:57 A monument that I did suffer her means
2024-09-10 08:29:57 So long at the sun of my soul's son: here comes my brother;
2024-09-10 08:29:57 And I
2024-09-10 08:29:57 Inference time: 2.12 seconds
2024-09-10 08:29:57 Tokens per second: 235.62
2024-09-10 08:29:57 ---------------
2024-09-10 08:30:00 Sample 4:
2024-09-10 08:30:00  let us pass.
2024-09-10 08:30:00 
2024-09-10 08:30:00 KING EDWARD IV:
2024-09-10 08:30:00 I pray thee, madam: so I will not stay.
2024-09-10 08:30:00 
2024-09-10 08:30:00 QUEEN MARGARET:
2024-09-10 08:30:00 I have heard it say my regiment.
2024-09-10 08:30:00 
2024-09-10 08:30:00 KING LEWIS XI:
2024-09-10 08:30:00 Why, so that he be in France that he hath found
2024-09-10 08:30:00 Shall not in silence and so heaven in peace
2024-09-10 08:30:00 That thus be made to make him speak with pause,
2024-09-10 08:30:00 Where he hath some that made me his father slew my blood,
2024-09-10 08:30:00 As he gave himself, and his father with his liking:
2024-09-10 08:30:00 He says he would that runs into his worth.
2024-09-10 08:30:00 Farewell, my lord.
2024-09-10 08:30:00 
2024-09-10 08:30:00 KING RICHARD III:
2024-09-10 08:30:00 Now thou shalt not stay to him, f
2024-09-10 08:30:00 Inference time: 2.16 seconds
2024-09-10 08:30:00 Tokens per second: 231.61
2024-09-10 08:30:00 ---------------
2024-09-10 08:30:02 Sample 5:
2024-09-10 08:30:02  will I make thee most of thee
2024-09-10 08:30:02 Duke to death, what to make inferior him
2024-09-10 08:30:02 Were for life a tumbling now to give,
2024-09-10 08:30:02 Or as thou shalt live on the slander of my blood
2024-09-10 08:30:02 And hid so dissolved his curse of blood;
2024-09-10 08:30:02 Which were stronger shadows himself came from me.
2024-09-10 08:30:02 
2024-09-10 08:30:02 DUKE OF YORK:
2024-09-10 08:30:02 Give me thy hand; I must ring in:
2024-09-10 08:30:02 So shall not stand to the king come to the prince.
2024-09-10 08:30:02 
2024-09-10 08:30:02 DUKE OF YORK:
2024-09-10 08:30:02 I pray thee, good my lord; what disperse?
2024-09-10 08:30:02 
2024-09-10 08:30:02 DUCHESS OF YORK:
2024-09-10 08:30:02 A good time of days cannot do thee the deed?
2024-09-10 08:30:02 
2024-09-10 08:30:02 DUCHESS OF YORK:
2024-09-10 08:30:02 No, good lord
2024-09-10 08:30:02 Inference time: 2.38 seconds
2024-09-10 08:30:02 Tokens per second: 210.38
2024-09-10 08:30:02 ---------------
2024-09-10 08:30:04 Sample 6:
2024-09-10 08:30:04  my father's son,
2024-09-10 08:30:04 She would live live to see that will I should wish thee
2024-09-10 08:30:04 Only point in blood.
2024-09-10 08:30:04 
2024-09-10 08:30:04 CAMILLO:
2024-09-10 08:30:04 I will not fear
2024-09-10 08:30:04 What I do procure to her father maid
2024-09-10 08:30:04 Hath no less felt behind the king
2024-09-10 08:30:04 Should kill her and marriage.
2024-09-10 08:30:04 
2024-09-10 08:30:04 POLIXENES:
2024-09-10 08:30:04 I think it is:
2024-09-10 08:30:04 But I had rather know thou didst know
2024-09-10 08:30:04 Than in thy business.
2024-09-10 08:30:04 
2024-09-10 08:30:04 CAMILLO:
2024-09-10 08:30:04 Well, my lord.
2024-09-10 08:30:04 
2024-09-10 08:30:04 LEONTES:
2024-09-10 08:30:04 We can be admitted to come; there is no better;
2024-09-10 08:30:04 For 'tis that you may be satisfied before:
2024-09-10 08:30:04 I have a last as learn, and great love prologrees
2024-09-10 08:30:04 To be sacred wit
2024-09-10 08:30:04 Inference time: 1.98 seconds
2024-09-10 08:30:04 Tokens per second: 252.25
2024-09-10 08:30:04 ---------------
2024-09-10 08:30:06 Sample 7:
2024-09-10 08:30:06  will I.
2024-09-10 08:30:06 
2024-09-10 08:30:06 LADY ANNE:
2024-09-10 08:30:06 Where is the matter, when the duke?
2024-09-10 08:30:06 
2024-09-10 08:30:06 GLOUCESTER:
2024-09-10 08:30:06 
2024-09-10 08:30:06 LADY ANNE:
2024-09-10 08:30:06 
2024-09-10 08:30:06 GLOUCESTER:
2024-09-10 08:30:06 
2024-09-10 08:30:06 KING EDWARD IV:
2024-09-10 08:30:06 
2024-09-10 08:30:06 GLOUCESTER:
2024-09-10 08:30:06 Then be the Duke of Buckingham and I
2024-09-10 08:30:06 Took the Lady Grey that Henry but ones,
2024-09-10 08:30:06 Hast thou for the prince your soul is shut.
2024-09-10 08:30:06 
2024-09-10 08:30:06 KING EDWARD IV:
2024-09-10 08:30:06 Why, then 'tis meet to me enough; but it must confess.
2024-09-10 08:30:06 
2024-09-10 08:30:06 LADY GREY:
2024-09-10 08:30:06 The Lord prove is the mighty queen, for she is but she
2024-09-10 08:30:06 That I may find her all death:
2024-09-10 08:30:06 Yet she will not stay, I'll make her sweet like sorrow.
2024-09-10 08:30:06 
2024-09-10 08:30:06 QUEEN MARGARET:
2024-09-10 08:30:06 Thanks, gent
2024-09-10 08:30:06 Inference time: 2.07 seconds
2024-09-10 08:30:06 Tokens per second: 242.03
2024-09-10 08:30:06 ---------------
2024-09-10 08:30:08 Sample 8:
2024-09-10 08:30:08  do it know
2024-09-10 08:30:08 That you have possess'd to endure the people,
2024-09-10 08:30:08 You shall not call them again.
2024-09-10 08:30:08 
2024-09-10 08:30:08 Gentleman:
2024-09-10 08:30:08 Consider you, sir, that you might well enough,
2024-09-10 08:30:08 Because they are become an eye.
2024-09-10 08:30:08 
2024-09-10 08:30:08 LUCIO:
2024-09-10 08:30:08 Well, well, well; I must to have it so.
2024-09-10 08:30:08 Grumio, there's no further will not be in the
2024-09-10 08:30:08 duke: if he were not dislike the change of himself a
2024-09-10 08:30:08 man, sir, being nothing of him that I have said to you,
2024-09-10 08:30:08 fourth the absent times, he stumbling not.
2024-09-10 08:30:08 
2024-09-10 08:30:08 AUTOLYCUS:
2024-09-10 08:30:08 I think not well, sir: have you not been brief thence?
2024-09-10 08:30:08 
2024-09-10 08:30:08 Clown:
2024-09-10 08:30:08 
2024-09-10 08:30:08 Inference time: 2.05 seconds
2024-09-10 08:30:08 Tokens per second: 243.75
2024-09-10 08:30:08 ---------------
2024-09-10 08:30:10 Sample 9:
2024-09-10 08:30:10  slanderous lips at the hand;
2024-09-10 08:30:10 Cry like a clouds in tears, and stain all good,
2024-09-10 08:30:10 So soon for wive I should not be satisfied.
2024-09-10 08:30:10 
2024-09-10 08:30:10 KING HENRY VI:
2024-09-10 08:30:10 Think'st thou I mean that Edward slew my father?
2024-09-10 08:30:10 
2024-09-10 08:30:10 YORK:
2024-09-10 08:30:10 I fear, my liege.
2024-09-10 08:30:10 
2024-09-10 08:30:10 KING HENRY VI:
2024-09-10 08:30:10 Fair cousin Margaret, stay with thee:
2024-09-10 08:30:10 Therefore lies the close my father bled.
2024-09-10 08:30:10 
2024-09-10 08:30:10 QUEEN MARGARET:
2024-09-10 08:30:10 The counsel and not my true lord chamberlain:
2024-09-10 08:30:10 I will not stay the service that thou liest.
2024-09-10 08:30:10 
2024-09-10 08:30:10 KING HENRY VI:
2024-09-10 08:30:10 Where is thy office the crown? is he no son?
2024-09-10 08:30:10 There is the rest, whom w
2024-09-10 08:30:10 Inference time: 2.20 seconds
2024-09-10 08:30:10 Tokens per second: 227.55
2024-09-10 08:30:10 ---------------
2024-09-10 08:30:13 Sample 10:
2024-09-10 08:30:13  makes me disposition.
2024-09-10 08:30:13 
2024-09-10 08:30:13 First Gentleman:
2024-09-10 08:30:13 Not a word, but not a word, if indeed which he
2024-09-10 08:30:13 did need it so?
2024-09-10 08:30:13 
2024-09-10 08:30:13 LUCIO:
2024-09-10 08:30:13 I am as an unwise, sir, it is lawful.
2024-09-10 08:30:13 
2024-09-10 08:30:13 LUCIO:
2024-09-10 08:30:13 O, that is a scolding lady or two; for if I were good to the
2024-09-10 08:30:13 duke's eyes a letter.
2024-09-10 08:30:13 
2024-09-10 08:30:13 DUKE VINCENTIO:
2024-09-10 08:30:13 So that is a jest woman the sense that there were spent.
2024-09-10 08:30:13 
2024-09-10 08:30:13 LUCIO:
2024-09-10 08:30:13 As dissolved as like an example. I would to do it so,
2024-09-10 08:30:13 it will but should have more importance for it.
2024-09-10 08:30:13 
2024-09-10 08:30:13 DUKE VINCENTIO:
2024-09-10 08:30:13 But is the remedy than a thing I lie.
2024-09-10 08:30:13 Pardon him, be she ma
2024-09-10 08:30:13 Inference time: 2.34 seconds
2024-09-10 08:30:13 Tokens per second: 214.00
2024-09-10 08:30:13 ---------------
2024-09-10 08:30:13 Average tokens per second: 229.37
2024-09-10 08:30:13 tokens per iteration will be: 16,384
2024-09-10 08:30:13 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:30:13 Initializing a new model from scratch
2024-09-10 08:30:13 number of parameters: 10.65M
2024-09-10 08:30:13 /app/AI-Scientist/templates/nanoGPT/experiment.py:462: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2024-09-10 08:30:13   scaler = torch.cuda.amp.GradScaler(enabled=(dtype == "float16"))
2024-09-10 08:30:13 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:30:13 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:30:13 using fused AdamW: True
2024-09-10 08:30:13 compiling the model... (takes a ~minute)
2024-09-10 08:30:27 step 0: train loss 4.2372, val loss 4.2295
2024-09-10 08:30:36 iter 0: loss 4.2337, time 23225.34ms
2024-09-10 08:30:37 iter 10: loss 3.2253, time 78.62ms
2024-09-10 08:30:38 iter 20: loss 2.7687, time 78.51ms
2024-09-10 08:30:39 iter 30: loss 2.6201, time 78.92ms
2024-09-10 08:30:39 iter 40: loss 2.5384, time 79.53ms
2024-09-10 08:30:40 iter 50: loss 2.5372, time 79.52ms
2024-09-10 08:30:41 iter 60: loss 2.4993, time 78.92ms
2024-09-10 08:30:42 iter 70: loss 2.4937, time 78.85ms
2024-09-10 08:30:43 iter 80: loss 2.4955, time 78.89ms
2024-09-10 08:30:43 iter 90: loss 2.4794, time 78.99ms
2024-09-10 08:30:44 iter 100: loss 2.4682, time 78.92ms
2024-09-10 08:30:45 iter 110: loss 2.4339, time 78.93ms
2024-09-10 08:30:46 iter 120: loss 2.4512, time 79.32ms
2024-09-10 08:30:47 iter 130: loss 2.4348, time 79.31ms
2024-09-10 08:30:47 iter 140: loss 2.4279, time 79.27ms
2024-09-10 08:30:48 iter 150: loss 2.3598, time 80.65ms
2024-09-10 08:30:49 iter 160: loss 2.3876, time 79.39ms
2024-09-10 08:30:50 iter 170: loss 2.3358, time 79.60ms
2024-09-10 08:30:51 iter 180: loss 2.3230, time 79.29ms
2024-09-10 08:30:51 iter 190: loss 2.2837, time 79.64ms
2024-09-10 08:30:52 iter 200: loss 2.2494, time 80.64ms
2024-09-10 08:30:53 iter 210: loss 2.1718, time 79.93ms
2024-09-10 08:30:54 iter 220: loss 2.1231, time 79.86ms
2024-09-10 08:30:55 iter 230: loss 2.0876, time 80.36ms
2024-09-10 08:30:55 iter 240: loss 2.0609, time 79.99ms
2024-09-10 08:31:06 step 250: train loss 1.9821, val loss 2.0865
2024-09-10 08:31:06 iter 250: loss 2.0375, time 10349.05ms
2024-09-10 08:31:07 iter 260: loss 2.0181, time 80.17ms
2024-09-10 08:31:08 iter 270: loss 1.9491, time 81.36ms
2024-09-10 08:31:09 iter 280: loss 1.9428, time 80.08ms
2024-09-10 08:31:09 iter 290: loss 1.9149, time 80.07ms
2024-09-10 08:31:10 iter 300: loss 1.9050, time 80.16ms
2024-09-10 08:31:11 iter 310: loss 1.9152, time 80.41ms
2024-09-10 08:31:12 iter 320: loss 1.8532, time 81.33ms
2024-09-10 08:31:13 iter 330: loss 1.8607, time 80.62ms
2024-09-10 08:31:14 iter 340: loss 1.7952, time 81.39ms
2024-09-10 08:31:14 iter 350: loss 1.7798, time 80.46ms
2024-09-10 08:31:15 iter 360: loss 1.7928, time 80.00ms
2024-09-10 08:31:16 iter 370: loss 1.7779, time 80.28ms
2024-09-10 08:31:17 iter 380: loss 1.7479, time 80.11ms
2024-09-10 08:31:18 iter 390: loss 1.7398, time 80.29ms
2024-09-10 08:31:18 iter 400: loss 1.7093, time 80.43ms
2024-09-10 08:31:19 iter 410: loss 1.7048, time 82.61ms
2024-09-10 08:31:20 iter 420: loss 1.7140, time 80.53ms
2024-09-10 08:31:21 iter 430: loss 1.6888, time 80.40ms
2024-09-10 08:31:22 iter 440: loss 1.6656, time 80.96ms
2024-09-10 08:31:22 iter 450: loss 1.6399, time 80.99ms
2024-09-10 08:31:23 iter 460: loss 1.6841, time 80.91ms
2024-09-10 08:31:24 iter 470: loss 1.6511, time 80.67ms
2024-09-10 08:31:25 iter 480: loss 1.6524, time 80.67ms
2024-09-10 08:31:26 iter 490: loss 1.6070, time 80.73ms
2024-09-10 08:31:37 step 500: train loss 1.5441, val loss 1.7396
2024-09-10 08:31:37 iter 500: loss 1.6205, time 10433.05ms
2024-09-10 08:31:37 iter 510: loss 1.6031, time 81.27ms
2024-09-10 08:31:38 iter 520: loss 1.5652, time 81.26ms
2024-09-10 08:31:39 iter 530: loss 1.6023, time 82.10ms
2024-09-10 08:31:40 iter 540: loss 1.5585, time 82.24ms
2024-09-10 08:31:41 iter 550: loss 1.5573, time 81.16ms
2024-09-10 08:31:42 iter 560: loss 1.5565, time 81.67ms
2024-09-10 08:31:42 iter 570: loss 1.5723, time 81.06ms
2024-09-10 08:31:43 iter 580: loss 1.5169, time 81.06ms
2024-09-10 08:31:44 iter 590: loss 1.5360, time 80.82ms
2024-09-10 08:31:45 iter 600: loss 1.5043, time 81.12ms
2024-09-10 08:31:46 iter 610: loss 1.5369, time 82.48ms
2024-09-10 08:31:46 iter 620: loss 1.4926, time 81.51ms
2024-09-10 08:31:47 iter 630: loss 1.5474, time 81.73ms
2024-09-10 08:31:48 iter 640: loss 1.4690, time 81.03ms
2024-09-10 08:31:49 iter 650: loss 1.5034, time 80.84ms
2024-09-10 08:31:50 iter 660: loss 1.4893, time 81.28ms
2024-09-10 08:31:51 iter 670: loss 1.4889, time 81.07ms
2024-09-10 08:31:51 iter 680: loss 1.4514, time 80.96ms
2024-09-10 08:31:52 iter 690: loss 1.4636, time 80.62ms
2024-09-10 08:31:53 iter 700: loss 1.4618, time 80.87ms
2024-09-10 08:31:54 iter 710: loss 1.4860, time 81.53ms
2024-09-10 08:31:55 iter 720: loss 1.4422, time 82.97ms
2024-09-10 08:31:55 iter 730: loss 1.4564, time 81.28ms
2024-09-10 08:31:56 iter 740: loss 1.4398, time 81.12ms
2024-09-10 08:32:07 step 750: train loss 1.3674, val loss 1.5988
2024-09-10 08:32:07 iter 750: loss 1.4414, time 10490.01ms
2024-09-10 08:32:08 iter 760: loss 1.4074, time 82.40ms
2024-09-10 08:32:09 iter 770: loss 1.4162, time 81.27ms
2024-09-10 08:32:10 iter 780: loss 1.4239, time 81.09ms
2024-09-10 08:32:11 iter 790: loss 1.3890, time 81.01ms
2024-09-10 08:32:11 iter 800: loss 1.4204, time 81.46ms
2024-09-10 08:32:12 iter 810: loss 1.4335, time 81.25ms
2024-09-10 08:32:13 iter 820: loss 1.4056, time 81.29ms
2024-09-10 08:32:14 iter 830: loss 1.4055, time 81.10ms
2024-09-10 08:32:15 iter 840: loss 1.3949, time 81.73ms
2024-09-10 08:32:15 iter 850: loss 1.4250, time 82.98ms
2024-09-10 08:32:16 iter 860: loss 1.3782, time 81.02ms
2024-09-10 08:32:17 iter 870: loss 1.4022, time 81.60ms
2024-09-10 08:32:18 iter 880: loss 1.3603, time 81.58ms
2024-09-10 08:32:19 iter 890: loss 1.3694, time 81.32ms
2024-09-10 08:32:20 iter 900: loss 1.3598, time 81.31ms
2024-09-10 08:32:20 iter 910: loss 1.3968, time 81.20ms
2024-09-10 08:32:21 iter 920: loss 1.3728, time 81.44ms
2024-09-10 08:32:22 iter 930: loss 1.3622, time 82.20ms
2024-09-10 08:32:23 iter 940: loss 1.3718, time 81.01ms
2024-09-10 08:32:24 iter 950: loss 1.3565, time 81.24ms
2024-09-10 08:32:24 iter 960: loss 1.3913, time 81.28ms
2024-09-10 08:32:25 iter 970: loss 1.3779, time 81.78ms
2024-09-10 08:32:26 iter 980: loss 1.3673, time 82.70ms
2024-09-10 08:32:27 iter 990: loss 1.3131, time 81.56ms
2024-09-10 08:32:38 step 1000: train loss 1.2716, val loss 1.5201
2024-09-10 08:32:38 iter 1000: loss 1.3416, time 10521.94ms
2024-09-10 08:32:39 iter 1010: loss 1.3637, time 81.87ms
2024-09-10 08:32:40 iter 1020: loss 1.3496, time 83.23ms
2024-09-10 08:32:40 iter 1030: loss 1.3286, time 81.48ms
2024-09-10 08:32:41 iter 1040: loss 1.2987, time 81.52ms
2024-09-10 08:32:42 iter 1050: loss 1.3030, time 81.22ms
2024-09-10 08:32:43 iter 1060: loss 1.3132, time 81.77ms
2024-09-10 08:32:44 iter 1070: loss 1.3287, time 80.99ms
2024-09-10 08:32:45 iter 1080: loss 1.3192, time 81.90ms
2024-09-10 08:32:45 iter 1090: loss 1.3643, time 81.61ms
2024-09-10 08:32:46 iter 1100: loss 1.2872, time 81.38ms
2024-09-10 08:32:47 iter 1110: loss 1.3124, time 81.53ms
2024-09-10 08:32:48 iter 1120: loss 1.2988, time 81.41ms
2024-09-10 08:32:49 iter 1130: loss 1.3018, time 82.53ms
2024-09-10 08:32:49 iter 1140: loss 1.2996, time 82.25ms
2024-09-10 08:32:50 iter 1150: loss 1.2925, time 83.95ms
2024-09-10 08:32:51 iter 1160: loss 1.2839, time 81.47ms
2024-09-10 08:32:52 iter 1170: loss 1.2780, time 81.53ms
2024-09-10 08:32:53 iter 1180: loss 1.2751, time 81.28ms
2024-09-10 08:32:54 iter 1190: loss 1.3272, time 82.33ms
2024-09-10 08:32:54 iter 1200: loss 1.2961, time 81.66ms
2024-09-10 08:32:55 iter 1210: loss 1.2923, time 82.01ms
2024-09-10 08:32:56 iter 1220: loss 1.2949, time 81.99ms
2024-09-10 08:32:57 iter 1230: loss 1.2573, time 81.43ms
2024-09-10 08:32:58 iter 1240: loss 1.2639, time 82.19ms
2024-09-10 08:33:09 step 1250: train loss 1.2101, val loss 1.4982
2024-09-10 08:33:09 iter 1250: loss 1.3023, time 10542.22ms
2024-09-10 08:33:10 iter 1260: loss 1.2890, time 81.05ms
2024-09-10 08:33:10 iter 1270: loss 1.3005, time 82.08ms
2024-09-10 08:33:11 iter 1280: loss 1.2652, time 81.78ms
2024-09-10 08:33:12 iter 1290: loss 1.2975, time 82.09ms
2024-09-10 08:33:13 iter 1300: loss 1.2947, time 83.01ms
2024-09-10 08:33:14 iter 1310: loss 1.2506, time 81.50ms
2024-09-10 08:33:14 iter 1320: loss 1.2887, time 83.10ms
2024-09-10 08:33:15 iter 1330: loss 1.2657, time 81.68ms
2024-09-10 08:33:16 iter 1340: loss 1.2472, time 80.28ms
2024-09-10 08:33:17 iter 1350: loss 1.2478, time 81.83ms
2024-09-10 08:33:18 iter 1360: loss 1.2944, time 81.46ms
2024-09-10 08:33:19 iter 1370: loss 1.2619, time 81.07ms
2024-09-10 08:33:19 iter 1380: loss 1.2565, time 81.49ms
2024-09-10 08:33:20 iter 1390: loss 1.2642, time 81.31ms
2024-09-10 08:33:21 iter 1400: loss 1.2585, time 81.68ms
2024-09-10 08:33:22 iter 1410: loss 1.2594, time 81.76ms
2024-09-10 08:33:23 iter 1420: loss 1.2426, time 82.12ms
2024-09-10 08:33:24 iter 1430: loss 1.2284, time 83.29ms
2024-09-10 08:33:24 iter 1440: loss 1.2245, time 81.91ms
2024-09-10 08:33:25 iter 1450: loss 1.2730, time 83.99ms
2024-09-10 08:33:26 iter 1460: loss 1.2375, time 82.49ms
2024-09-10 08:33:27 iter 1470: loss 1.2255, time 83.44ms
2024-09-10 08:33:28 iter 1480: loss 1.2167, time 81.86ms
2024-09-10 08:33:28 iter 1490: loss 1.2451, time 81.94ms
2024-09-10 08:33:39 step 1500: train loss 1.1513, val loss 1.4762
2024-09-10 08:33:40 iter 1500: loss 1.2436, time 10536.27ms
2024-09-10 08:33:40 iter 1510: loss 1.2270, time 83.66ms
2024-09-10 08:33:41 iter 1520: loss 1.2396, time 81.52ms
2024-09-10 08:33:42 iter 1530: loss 1.2154, time 82.26ms
2024-09-10 08:33:43 iter 1540: loss 1.2453, time 81.96ms
2024-09-10 08:33:44 iter 1550: loss 1.2192, time 81.95ms
2024-09-10 08:33:44 iter 1560: loss 1.2566, time 81.73ms
2024-09-10 08:33:45 iter 1570: loss 1.2117, time 82.24ms
2024-09-10 08:33:46 iter 1580: loss 1.1999, time 80.10ms
2024-09-10 08:33:47 iter 1590: loss 1.1979, time 81.94ms
2024-09-10 08:33:48 iter 1600: loss 1.2161, time 82.39ms
2024-09-10 08:33:49 iter 1610: loss 1.1824, time 82.13ms
2024-09-10 08:33:49 iter 1620: loss 1.2182, time 81.64ms
2024-09-10 08:33:50 iter 1630: loss 1.2305, time 82.63ms
2024-09-10 08:33:51 iter 1640: loss 1.2217, time 81.61ms
2024-09-10 08:33:52 iter 1650: loss 1.2006, time 81.79ms
2024-09-10 08:33:53 iter 1660: loss 1.1926, time 83.52ms
2024-09-10 08:33:54 iter 1670: loss 1.2158, time 82.10ms
2024-09-10 08:33:54 iter 1680: loss 1.1809, time 82.29ms
2024-09-10 08:33:55 iter 1690: loss 1.1867, time 82.26ms
2024-09-10 08:33:56 iter 1700: loss 1.1811, time 81.26ms
2024-09-10 08:33:57 iter 1710: loss 1.1521, time 82.17ms
2024-09-10 08:33:58 iter 1720: loss 1.1738, time 82.19ms
2024-09-10 08:33:58 iter 1730: loss 1.2000, time 81.76ms
2024-09-10 08:33:59 iter 1740: loss 1.1912, time 81.92ms
2024-09-10 08:34:10 step 1750: train loss 1.1031, val loss 1.4737
2024-09-10 08:34:10 iter 1750: loss 1.1931, time 10561.59ms
2024-09-10 08:34:11 iter 1760: loss 1.1985, time 82.23ms
2024-09-10 08:34:12 iter 1770: loss 1.1937, time 82.12ms
2024-09-10 08:34:13 iter 1780: loss 1.1491, time 82.68ms
2024-09-10 08:34:14 iter 1790: loss 1.1813, time 81.56ms
2024-09-10 08:34:15 iter 1800: loss 1.1464, time 81.89ms
2024-09-10 08:34:15 iter 1810: loss 1.1750, time 81.90ms
2024-09-10 08:34:16 iter 1820: loss 1.1707, time 80.89ms
2024-09-10 08:34:17 iter 1830: loss 1.1935, time 82.20ms
2024-09-10 08:34:18 iter 1840: loss 1.1628, time 81.75ms
2024-09-10 08:34:19 iter 1850: loss 1.1836, time 83.29ms
2024-09-10 08:34:19 iter 1860: loss 1.2192, time 81.59ms
2024-09-10 08:34:20 iter 1870: loss 1.1757, time 83.18ms
2024-09-10 08:34:21 iter 1880: loss 1.1522, time 81.91ms
2024-09-10 08:34:22 iter 1890: loss 1.1582, time 83.38ms
2024-09-10 08:34:23 iter 1900: loss 1.1581, time 82.42ms
2024-09-10 08:34:24 iter 1910: loss 1.1647, time 81.78ms
2024-09-10 08:34:24 iter 1920: loss 1.1454, time 82.01ms
2024-09-10 08:34:25 iter 1930: loss 1.1772, time 82.36ms
2024-09-10 08:34:26 iter 1940: loss 1.1435, time 81.56ms
2024-09-10 08:34:27 iter 1950: loss 1.1283, time 81.79ms
2024-09-10 08:34:28 iter 1960: loss 1.1637, time 81.96ms
2024-09-10 08:34:29 iter 1970: loss 1.1394, time 82.66ms
2024-09-10 08:34:29 iter 1980: loss 1.1943, time 81.64ms
2024-09-10 08:34:30 iter 1990: loss 1.1371, time 81.83ms
2024-09-10 08:34:41 step 2000: train loss 1.0583, val loss 1.4753
2024-09-10 08:34:41 iter 2000: loss 1.1266, time 10560.48ms
2024-09-10 08:34:42 iter 2010: loss 1.1419, time 82.07ms
2024-09-10 08:34:43 iter 2020: loss 1.1338, time 81.35ms
2024-09-10 08:34:44 iter 2030: loss 1.1383, time 82.30ms
2024-09-10 08:34:45 iter 2040: loss 1.1403, time 81.66ms
2024-09-10 08:34:45 iter 2050: loss 1.1180, time 82.38ms
2024-09-10 08:34:46 iter 2060: loss 1.1109, time 83.76ms
2024-09-10 08:34:47 iter 2070: loss 1.1444, time 82.11ms
2024-09-10 08:34:48 iter 2080: loss 1.1104, time 83.61ms
2024-09-10 08:34:49 iter 2090: loss 1.1261, time 81.81ms
2024-09-10 08:34:49 iter 2100: loss 1.1291, time 81.76ms
2024-09-10 08:34:50 iter 2110: loss 1.1337, time 82.28ms
2024-09-10 08:34:51 iter 2120: loss 1.1420, time 81.95ms
2024-09-10 08:34:52 iter 2130: loss 1.1434, time 81.67ms
2024-09-10 08:34:53 iter 2140: loss 1.1049, time 81.64ms
2024-09-10 08:34:54 iter 2150: loss 1.1110, time 81.81ms
2024-09-10 08:34:54 iter 2160: loss 1.0916, time 81.81ms
2024-09-10 08:34:55 iter 2170: loss 1.1116, time 81.61ms
2024-09-10 08:34:56 iter 2180: loss 1.1187, time 81.89ms
2024-09-10 08:34:57 iter 2190: loss 1.1536, time 81.86ms
2024-09-10 08:34:58 iter 2200: loss 1.1429, time 83.06ms
2024-09-10 08:34:59 iter 2210: loss 1.1315, time 83.72ms
2024-09-10 08:34:59 iter 2220: loss 1.1213, time 81.97ms
2024-09-10 08:35:00 iter 2230: loss 1.0985, time 83.44ms
2024-09-10 08:35:01 iter 2240: loss 1.1202, time 81.68ms
2024-09-10 08:35:12 step 2250: train loss 1.0072, val loss 1.4814
2024-09-10 08:35:12 iter 2250: loss 1.1242, time 10540.55ms
2024-09-10 08:35:13 iter 2260: loss 1.1127, time 81.85ms
2024-09-10 08:35:14 iter 2270: loss 1.1006, time 83.22ms
2024-09-10 08:35:15 iter 2280: loss 1.0886, time 81.69ms
2024-09-10 08:35:15 iter 2290: loss 1.0860, time 83.82ms
2024-09-10 08:35:16 iter 2300: loss 1.1253, time 82.17ms
2024-09-10 08:35:17 iter 2310: loss 1.0860, time 82.53ms
2024-09-10 08:35:18 iter 2320: loss 1.0880, time 82.10ms
2024-09-10 08:35:19 iter 2330: loss 1.0931, time 81.19ms
2024-09-10 08:35:20 iter 2340: loss 1.0879, time 82.25ms
2024-09-10 08:35:20 iter 2350: loss 1.0717, time 81.79ms
2024-09-10 08:35:21 iter 2360: loss 1.0887, time 82.24ms
2024-09-10 08:35:22 iter 2370: loss 1.0977, time 81.61ms
2024-09-10 08:35:23 iter 2380: loss 1.0867, time 81.40ms
2024-09-10 08:35:24 iter 2390: loss 1.0869, time 82.01ms
2024-09-10 08:35:24 iter 2400: loss 1.0558, time 83.30ms
2024-09-10 08:35:25 iter 2410: loss 1.0872, time 82.72ms
2024-09-10 08:35:26 iter 2420: loss 1.0646, time 82.71ms
2024-09-10 08:35:27 iter 2430: loss 1.0532, time 82.18ms
2024-09-10 08:35:28 iter 2440: loss 1.0499, time 81.71ms
2024-09-10 08:35:29 iter 2450: loss 1.0769, time 81.81ms
2024-09-10 08:35:29 iter 2460: loss 1.0851, time 81.97ms
2024-09-10 08:35:30 iter 2470: loss 1.0787, time 82.91ms
2024-09-10 08:35:31 iter 2480: loss 1.0550, time 81.74ms
2024-09-10 08:35:32 iter 2490: loss 1.0598, time 82.30ms
2024-09-10 08:35:43 step 2500: train loss 0.9566, val loss 1.4911
2024-09-10 08:35:43 iter 2500: loss 1.0793, time 10559.74ms
2024-09-10 08:35:44 iter 2510: loss 1.0472, time 82.20ms
2024-09-10 08:35:45 iter 2520: loss 1.0594, time 81.80ms
2024-09-10 08:35:45 iter 2530: loss 1.0760, time 82.23ms
2024-09-10 08:35:46 iter 2540: loss 1.0415, time 82.41ms
2024-09-10 08:35:47 iter 2550: loss 1.0687, time 81.89ms
2024-09-10 08:35:48 iter 2560: loss 1.0587, time 82.21ms
2024-09-10 08:35:49 iter 2570: loss 1.0568, time 81.87ms
2024-09-10 08:35:50 iter 2580: loss 1.0508, time 81.54ms
2024-09-10 08:35:50 iter 2590: loss 1.0613, time 81.54ms
2024-09-10 08:35:51 iter 2600: loss 1.0680, time 82.66ms
2024-09-10 08:35:52 iter 2610: loss 1.0329, time 83.44ms
2024-09-10 08:35:53 iter 2620: loss 1.0531, time 81.40ms
2024-09-10 08:35:54 iter 2630: loss 1.0695, time 83.83ms
2024-09-10 08:35:54 iter 2640: loss 1.0173, time 82.22ms
2024-09-10 08:35:55 iter 2650: loss 1.0389, time 82.17ms
2024-09-10 08:35:56 iter 2660: loss 1.0514, time 81.80ms
2024-09-10 08:35:57 iter 2670: loss 1.0418, time 82.06ms
2024-09-10 08:35:58 iter 2680: loss 1.0402, time 81.19ms
2024-09-10 08:35:59 iter 2690: loss 1.0551, time 82.14ms
2024-09-10 08:35:59 iter 2700: loss 1.0134, time 81.50ms
2024-09-10 08:36:00 iter 2710: loss 1.0347, time 82.37ms
2024-09-10 08:36:01 iter 2720: loss 0.9986, time 81.97ms
2024-09-10 08:36:02 iter 2730: loss 1.0404, time 81.49ms
2024-09-10 08:36:03 iter 2740: loss 1.0318, time 81.56ms
2024-09-10 08:36:14 step 2750: train loss 0.9126, val loss 1.5159
2024-09-10 08:36:14 iter 2750: loss 1.0472, time 10545.26ms
2024-09-10 08:36:15 iter 2760: loss 1.0163, time 81.76ms
2024-09-10 08:36:15 iter 2770: loss 1.0419, time 81.28ms
2024-09-10 08:36:16 iter 2780: loss 1.0371, time 81.22ms
2024-09-10 08:36:17 iter 2790: loss 1.0138, time 81.80ms
2024-09-10 08:36:18 iter 2800: loss 1.0161, time 83.66ms
2024-09-10 08:36:19 iter 2810: loss 1.0200, time 82.39ms
2024-09-10 08:36:20 iter 2820: loss 1.0358, time 83.02ms
2024-09-10 08:36:20 iter 2830: loss 0.9830, time 81.84ms
2024-09-10 08:36:21 iter 2840: loss 1.0133, time 81.15ms
2024-09-10 08:36:22 iter 2850: loss 1.0156, time 81.35ms
2024-09-10 08:36:23 iter 2860: loss 0.9968, time 81.86ms
2024-09-10 08:36:24 iter 2870: loss 1.0159, time 81.76ms
2024-09-10 08:36:24 iter 2880: loss 1.0189, time 82.31ms
2024-09-10 08:36:25 iter 2890: loss 1.0079, time 82.36ms
2024-09-10 08:36:26 iter 2900: loss 1.0367, time 81.74ms
2024-09-10 08:36:27 iter 2910: loss 1.0185, time 81.63ms
2024-09-10 08:36:28 iter 2920: loss 0.9787, time 81.86ms
2024-09-10 08:36:29 iter 2930: loss 0.9975, time 83.13ms
2024-09-10 08:36:29 iter 2940: loss 0.9980, time 82.27ms
2024-09-10 08:36:30 iter 2950: loss 0.9932, time 83.97ms
2024-09-10 08:36:31 iter 2960: loss 1.0073, time 82.06ms
2024-09-10 08:36:32 iter 2970: loss 0.9964, time 81.84ms
2024-09-10 08:36:33 iter 2980: loss 1.0136, time 81.17ms
2024-09-10 08:36:34 iter 2990: loss 0.9871, time 82.98ms
2024-09-10 08:36:44 step 3000: train loss 0.8642, val loss 1.5382
2024-09-10 08:36:45 iter 3000: loss 0.9965, time 10532.71ms
2024-09-10 08:36:45 iter 3010: loss 1.0137, time 81.47ms
2024-09-10 08:36:46 iter 3020: loss 1.0094, time 81.84ms
2024-09-10 08:36:47 iter 3030: loss 1.0069, time 82.28ms
2024-09-10 08:36:48 iter 3040: loss 0.9885, time 81.92ms
2024-09-10 08:36:49 iter 3050: loss 0.9823, time 82.20ms
2024-09-10 08:36:49 iter 3060: loss 0.9914, time 81.32ms
2024-09-10 08:36:50 iter 3070: loss 0.9971, time 81.52ms
2024-09-10 08:36:51 iter 3080: loss 0.9934, time 81.09ms
2024-09-10 08:36:52 iter 3090: loss 1.0079, time 81.41ms
2024-09-10 08:36:53 iter 3100: loss 0.9708, time 81.37ms
2024-09-10 08:36:54 iter 3110: loss 0.9915, time 81.97ms
2024-09-10 08:36:54 iter 3120: loss 0.9838, time 82.68ms
2024-09-10 08:36:55 iter 3130: loss 0.9854, time 81.06ms
2024-09-10 08:36:56 iter 3140: loss 0.9942, time 84.22ms
2024-09-10 08:36:57 iter 3150: loss 0.9623, time 81.81ms
2024-09-10 08:36:58 iter 3160: loss 0.9696, time 82.08ms
2024-09-10 08:36:59 iter 3170: loss 0.9545, time 81.78ms
2024-09-10 08:36:59 iter 3180: loss 0.9558, time 82.06ms
2024-09-10 08:37:00 iter 3190: loss 0.9795, time 81.27ms
2024-09-10 08:37:01 iter 3200: loss 0.9868, time 81.67ms
2024-09-10 08:37:02 iter 3210: loss 0.9928, time 81.56ms
2024-09-10 08:37:03 iter 3220: loss 0.9836, time 81.83ms
2024-09-10 08:37:03 iter 3230: loss 0.9824, time 81.01ms
2024-09-10 08:37:04 iter 3240: loss 0.9511, time 81.90ms
2024-09-10 08:37:15 step 3250: train loss 0.8173, val loss 1.5660
2024-09-10 08:37:15 iter 3250: loss 0.9903, time 10539.00ms
2024-09-10 08:37:16 iter 3260: loss 0.9797, time 80.92ms
2024-09-10 08:37:17 iter 3270: loss 0.9679, time 82.00ms
2024-09-10 08:37:18 iter 3280: loss 0.9554, time 82.42ms
2024-09-10 08:37:19 iter 3290: loss 0.9562, time 83.35ms
2024-09-10 08:37:19 iter 3300: loss 0.9553, time 82.24ms
2024-09-10 08:37:20 iter 3310: loss 0.9568, time 82.63ms
2024-09-10 08:37:21 iter 3320: loss 0.9439, time 82.42ms
2024-09-10 08:37:22 iter 3330: loss 0.9451, time 81.75ms
2024-09-10 08:37:23 iter 3340: loss 0.9708, time 82.07ms
2024-09-10 08:37:24 iter 3350: loss 0.9724, time 81.61ms
2024-09-10 08:37:24 iter 3360: loss 0.9397, time 81.23ms
2024-09-10 08:37:25 iter 3370: loss 0.9382, time 81.33ms
2024-09-10 08:37:26 iter 3380: loss 0.9408, time 81.32ms
2024-09-10 08:37:27 iter 3390: loss 0.9359, time 81.31ms
2024-09-10 08:37:28 iter 3400: loss 0.9414, time 81.49ms
2024-09-10 08:37:29 iter 3410: loss 0.9470, time 81.54ms
2024-09-10 08:37:29 iter 3420: loss 0.9359, time 83.64ms
2024-09-10 08:37:30 iter 3430: loss 0.9364, time 82.11ms
2024-09-10 08:37:31 iter 3440: loss 0.9239, time 83.54ms
2024-09-10 08:37:32 iter 3450: loss 0.9292, time 81.28ms
2024-09-10 08:37:33 iter 3460: loss 0.9709, time 81.10ms
2024-09-10 08:37:33 iter 3470: loss 0.9240, time 81.42ms
2024-09-10 08:37:34 iter 3480: loss 0.9332, time 81.20ms
2024-09-10 08:37:35 iter 3490: loss 0.9469, time 81.33ms
2024-09-10 08:37:46 step 3500: train loss 0.7775, val loss 1.5837
2024-09-10 08:37:46 iter 3500: loss 0.9249, time 10535.40ms
2024-09-10 08:37:47 iter 3510: loss 0.9455, time 81.43ms
2024-09-10 08:37:48 iter 3520: loss 0.9081, time 81.05ms
2024-09-10 08:37:49 iter 3530: loss 0.9190, time 82.23ms
2024-09-10 08:37:49 iter 3540: loss 0.9415, time 81.77ms
2024-09-10 08:37:50 iter 3550: loss 0.9183, time 81.72ms
2024-09-10 08:37:51 iter 3560: loss 0.9481, time 81.97ms
2024-09-10 08:37:52 iter 3570: loss 0.9240, time 81.17ms
2024-09-10 08:37:53 iter 3580: loss 0.9338, time 82.23ms
2024-09-10 08:37:54 iter 3590: loss 0.9414, time 83.67ms
2024-09-10 08:37:54 iter 3600: loss 0.9071, time 82.21ms
2024-09-10 08:37:55 iter 3610: loss 0.9216, time 83.82ms
2024-09-10 08:37:56 iter 3620: loss 0.9353, time 81.78ms
2024-09-10 08:37:57 iter 3630: loss 0.8995, time 82.39ms
2024-09-10 08:37:58 iter 3640: loss 0.9245, time 81.88ms
2024-09-10 08:37:58 iter 3650: loss 0.9386, time 81.90ms
2024-09-10 08:37:59 iter 3660: loss 0.9111, time 81.85ms
2024-09-10 08:38:00 iter 3670: loss 0.9047, time 82.74ms
2024-09-10 08:38:01 iter 3680: loss 0.8887, time 82.10ms
2024-09-10 08:38:02 iter 3690: loss 0.9066, time 82.41ms
2024-09-10 08:38:03 iter 3700: loss 0.9077, time 82.26ms
2024-09-10 08:38:03 iter 3710: loss 0.8729, time 81.93ms
2024-09-10 08:38:04 iter 3720: loss 0.9070, time 81.88ms
2024-09-10 08:38:05 iter 3730: loss 0.9150, time 82.18ms
2024-09-10 08:38:06 iter 3740: loss 0.8878, time 83.80ms
2024-09-10 08:38:17 step 3750: train loss 0.7348, val loss 1.6171
2024-09-10 08:38:17 iter 3750: loss 0.9200, time 10536.22ms
2024-09-10 08:38:18 iter 3760: loss 0.8910, time 81.79ms
2024-09-10 08:38:19 iter 3770: loss 0.8913, time 82.04ms
2024-09-10 08:38:19 iter 3780: loss 0.9122, time 81.25ms
2024-09-10 08:38:20 iter 3790: loss 0.8715, time 81.64ms
2024-09-10 08:38:21 iter 3800: loss 0.8936, time 82.89ms
2024-09-10 08:38:22 iter 3810: loss 0.9173, time 81.95ms
2024-09-10 08:38:23 iter 3820: loss 0.9195, time 81.68ms
2024-09-10 08:38:24 iter 3830: loss 0.8981, time 81.58ms
2024-09-10 08:38:24 iter 3840: loss 0.8995, time 81.52ms
2024-09-10 08:38:25 iter 3850: loss 0.8917, time 81.33ms
2024-09-10 08:38:26 iter 3860: loss 0.8775, time 81.07ms
2024-09-10 08:38:27 iter 3870: loss 0.8723, time 81.70ms
2024-09-10 08:38:28 iter 3880: loss 0.8785, time 82.60ms
2024-09-10 08:38:28 iter 3890: loss 0.8968, time 81.51ms
2024-09-10 08:38:29 iter 3900: loss 0.9245, time 81.28ms
2024-09-10 08:38:30 iter 3910: loss 0.8821, time 81.46ms
2024-09-10 08:38:31 iter 3920: loss 0.9003, time 81.43ms
2024-09-10 08:38:32 iter 3930: loss 0.8933, time 83.55ms
2024-09-10 08:38:33 iter 3940: loss 0.8957, time 81.86ms
2024-09-10 08:38:33 iter 3950: loss 0.8789, time 83.25ms
2024-09-10 08:38:34 iter 3960: loss 0.8841, time 81.74ms
2024-09-10 08:38:35 iter 3970: loss 0.8480, time 82.07ms
2024-09-10 08:38:36 iter 3980: loss 0.8929, time 82.04ms
2024-09-10 08:38:37 iter 3990: loss 0.8725, time 81.37ms
2024-09-10 08:38:48 step 4000: train loss 0.6992, val loss 1.6311
2024-09-10 08:38:48 iter 4000: loss 0.8873, time 10535.35ms
2024-09-10 08:38:49 iter 4010: loss 0.8888, time 82.11ms
2024-09-10 08:38:49 iter 4020: loss 0.8417, time 81.48ms
2024-09-10 08:38:50 iter 4030: loss 0.8756, time 81.36ms
2024-09-10 08:38:51 iter 4040: loss 0.8683, time 81.64ms
2024-09-10 08:38:52 iter 4050: loss 0.8505, time 81.83ms
2024-09-10 08:38:53 iter 4060: loss 0.8726, time 81.90ms
2024-09-10 08:38:53 iter 4070: loss 0.8844, time 81.70ms
2024-09-10 08:38:54 iter 4080: loss 0.8878, time 81.36ms
2024-09-10 08:38:55 iter 4090: loss 0.8371, time 81.76ms
2024-09-10 08:38:56 iter 4100: loss 0.8814, time 82.75ms
2024-09-10 08:38:57 iter 4110: loss 0.8768, time 82.23ms
2024-09-10 08:38:58 iter 4120: loss 0.8413, time 83.90ms
2024-09-10 08:38:58 iter 4130: loss 0.8455, time 81.05ms
2024-09-10 08:38:59 iter 4140: loss 0.8725, time 81.78ms
2024-09-10 08:39:00 iter 4150: loss 0.8458, time 81.48ms
2024-09-10 08:39:01 iter 4160: loss 0.8477, time 81.70ms
2024-09-10 08:39:02 iter 4170: loss 0.8589, time 82.08ms
2024-09-10 08:39:03 iter 4180: loss 0.8733, time 83.59ms
2024-09-10 08:39:03 iter 4190: loss 0.8626, time 83.70ms
2024-09-10 08:39:04 iter 4200: loss 0.8531, time 81.63ms
2024-09-10 08:39:05 iter 4210: loss 0.8865, time 82.19ms
2024-09-10 08:39:06 iter 4220: loss 0.8628, time 87.15ms
2024-09-10 08:39:07 iter 4230: loss 0.8681, time 84.92ms
2024-09-10 08:39:08 iter 4240: loss 0.8564, time 81.53ms
2024-09-10 08:39:19 step 4250: train loss 0.6694, val loss 1.6555
2024-09-10 08:39:19 iter 4250: loss 0.8504, time 10688.43ms
2024-09-10 08:39:20 iter 4260: loss 0.8613, time 82.25ms
2024-09-10 08:39:20 iter 4270: loss 0.8330, time 85.88ms
2024-09-10 08:39:21 iter 4280: loss 0.8606, time 82.63ms
2024-09-10 08:39:22 iter 4290: loss 0.8369, time 85.41ms
2024-09-10 08:39:23 iter 4300: loss 0.8535, time 87.19ms
2024-09-10 08:39:24 iter 4310: loss 0.8476, time 85.22ms
2024-09-10 08:39:25 iter 4320: loss 0.8521, time 83.74ms
2024-09-10 08:39:25 iter 4330: loss 0.8621, time 85.21ms
2024-09-10 08:39:26 iter 4340: loss 0.8696, time 81.14ms
2024-09-10 08:39:27 iter 4350: loss 0.8500, time 81.19ms
2024-09-10 08:39:28 iter 4360: loss 0.8414, time 81.40ms
2024-09-10 08:39:29 iter 4370: loss 0.8241, time 82.80ms
2024-09-10 08:39:30 iter 4380: loss 0.8557, time 85.54ms
2024-09-10 08:39:30 iter 4390: loss 0.8291, time 81.69ms
2024-09-10 08:39:31 iter 4400: loss 0.8322, time 81.85ms
2024-09-10 08:39:32 iter 4410: loss 0.8483, time 81.20ms
2024-09-10 08:39:33 iter 4420: loss 0.8347, time 82.24ms
2024-09-10 08:39:34 iter 4430: loss 0.8424, time 81.72ms
2024-09-10 08:39:35 iter 4440: loss 0.8327, time 81.78ms
2024-09-10 08:39:35 iter 4450: loss 0.8542, time 81.78ms
2024-09-10 08:39:36 iter 4460: loss 0.8392, time 85.67ms
2024-09-10 08:39:37 iter 4470: loss 0.8537, time 81.26ms
2024-09-10 08:39:38 iter 4480: loss 0.8415, time 81.90ms
2024-09-10 08:39:39 iter 4490: loss 0.8134, time 83.78ms
2024-09-10 08:39:50 step 4500: train loss 0.6452, val loss 1.6703
2024-09-10 08:39:50 iter 4500: loss 0.8448, time 10785.32ms
2024-09-10 08:39:51 iter 4510: loss 0.8364, time 82.44ms
2024-09-10 08:39:52 iter 4520: loss 0.8304, time 82.07ms
2024-09-10 08:39:52 iter 4530: loss 0.8249, time 82.22ms
2024-09-10 08:39:53 iter 4540: loss 0.8181, time 81.79ms
2024-09-10 08:39:54 iter 4550: loss 0.8259, time 80.93ms
2024-09-10 08:39:55 iter 4560: loss 0.8329, time 81.77ms
2024-09-10 08:39:56 iter 4570: loss 0.8328, time 82.00ms
2024-09-10 08:39:57 iter 4580: loss 0.8059, time 81.99ms
2024-09-10 08:39:57 iter 4590: loss 0.8195, time 81.60ms
2024-09-10 08:39:58 iter 4600: loss 0.8349, time 81.74ms
2024-09-10 08:39:59 iter 4610: loss 0.8240, time 81.50ms
2024-09-10 08:40:00 iter 4620: loss 0.8093, time 80.92ms
2024-09-10 08:40:01 iter 4630: loss 0.8316, time 81.49ms
2024-09-10 08:40:01 iter 4640: loss 0.8347, time 81.10ms
2024-09-10 08:40:02 iter 4650: loss 0.8290, time 83.80ms
2024-09-10 08:40:03 iter 4660: loss 0.8479, time 81.46ms
2024-09-10 08:40:04 iter 4670: loss 0.8504, time 84.57ms
2024-09-10 08:40:05 iter 4680: loss 0.8596, time 81.27ms
2024-09-10 08:40:06 iter 4690: loss 0.8292, time 80.88ms
2024-09-10 08:40:06 iter 4700: loss 0.8208, time 81.97ms
2024-09-10 08:40:07 iter 4710: loss 0.8349, time 81.77ms
2024-09-10 08:40:08 iter 4720: loss 0.8239, time 81.23ms
2024-09-10 08:40:09 iter 4730: loss 0.8284, time 81.20ms
2024-09-10 08:40:10 iter 4740: loss 0.8137, time 80.84ms
2024-09-10 08:40:21 step 4750: train loss 0.6281, val loss 1.6867
2024-09-10 08:40:21 iter 4750: loss 0.8246, time 10620.50ms
2024-09-10 08:40:22 iter 4760: loss 0.8366, time 81.35ms
2024-09-10 08:40:22 iter 4770: loss 0.8358, time 81.44ms
2024-09-10 08:40:23 iter 4780: loss 0.8262, time 85.55ms
2024-09-10 08:40:24 iter 4790: loss 0.8138, time 81.39ms
2024-09-10 08:40:25 iter 4800: loss 0.8019, time 83.37ms
2024-09-10 08:40:26 iter 4810: loss 0.8323, time 81.66ms
2024-09-10 08:40:27 iter 4820: loss 0.8458, time 86.96ms
2024-09-10 08:40:27 iter 4830: loss 0.8273, time 85.27ms
2024-09-10 08:40:28 iter 4840: loss 0.8026, time 81.79ms
2024-09-10 08:40:29 iter 4850: loss 0.8023, time 81.58ms
2024-09-10 08:40:30 iter 4860: loss 0.8087, time 81.61ms
2024-09-10 08:40:31 iter 4870: loss 0.8099, time 81.78ms
2024-09-10 08:40:31 iter 4880: loss 0.8032, time 94.88ms
2024-09-10 08:40:32 iter 4890: loss 0.8225, time 81.80ms
2024-09-10 08:40:33 iter 4900: loss 0.8031, time 81.67ms
2024-09-10 08:40:34 iter 4910: loss 0.8130, time 82.59ms
2024-09-10 08:40:35 iter 4920: loss 0.8161, time 80.77ms
2024-09-10 08:40:36 iter 4930: loss 0.8251, time 82.45ms
2024-09-10 08:40:36 iter 4940: loss 0.8364, time 81.78ms
2024-09-10 08:40:37 iter 4950: loss 0.8330, time 81.52ms
2024-09-10 08:40:38 iter 4960: loss 0.8405, time 82.16ms
2024-09-10 08:40:39 iter 4970: loss 0.8002, time 81.13ms
2024-09-10 08:40:40 iter 4980: loss 0.7999, time 81.66ms
2024-09-10 08:40:41 iter 4990: loss 0.7947, time 81.21ms
2024-09-10 08:40:52 step 5000: train loss 0.6108, val loss 1.6974
2024-09-10 08:40:52 iter 5000: loss 0.7971, time 10629.06ms
2024-09-10 08:40:52 training done
2024-09-10 08:40:52 Best validation loss: 1.4736566543579102
2024-09-10 08:40:52 Total train time: 10.65 mins
2024-09-10 08:40:52 Loading meta from ../../data/shakespeare_char/meta.pkl...
2024-09-10 08:40:54 Sample 1:
2024-09-10 08:40:54  and thought of hand sleep for love;
2024-09-10 08:40:54 Lewd out in a divorce to have stall'd up,
2024-09-10 08:40:54 And I see the ground that breathed thy sad stale,
2024-09-10 08:40:54 My sovereign sovereign liege,
2024-09-10 08:40:54 And for thy dignity to the trumpets' shore.
2024-09-10 08:40:54 Farewell, good my lords; I'll to keep the chamber.
2024-09-10 08:40:54 
2024-09-10 08:40:54 Nurse:
2024-09-10 08:40:54 Will you go with us?
2024-09-10 08:40:54 
2024-09-10 08:40:54 JULIET:
2024-09-10 08:40:54 Ay, by and by, a reason where lies about it.
2024-09-10 08:40:54 
2024-09-10 08:40:54 Nurse:
2024-09-10 08:40:54 Anon, fellow, fellow, let's be so little, and as
2024-09-10 08:40:54 thou liest; and therefore be but the extremity of it.
2024-09-10 08:40:54 
2024-09-10 08:40:54 Nurse:
2024-09-10 08:40:54 Here comes the brazen of a woman's home; ther
2024-09-10 08:40:54 Inference time: 2.05 seconds
2024-09-10 08:40:54 Tokens per second: 244.46
2024-09-10 08:40:54 ---------------
2024-09-10 08:40:56 Sample 2:
2024-09-10 08:40:56  but my queen,
2024-09-10 08:40:56 That enter him here received, have I
2024-09-10 08:40:56 Of your cousin, as you shall answer him.
2024-09-10 08:40:56 
2024-09-10 08:40:56 DUKE VINCENTIO:
2024-09-10 08:40:56 Good even now, good my lord.
2024-09-10 08:40:56 
2024-09-10 08:40:56 Provost:
2024-09-10 08:40:56 So please you, sir, to pardon me, sir, as you
2024-09-10 08:40:56 shall not, draw your thoughts, if you would deny him well?
2024-09-10 08:40:56 
2024-09-10 08:40:56 DUKE VINCENTIO:
2024-09-10 08:40:56 I pray you, provost, as you were the rest?
2024-09-10 08:40:56 
2024-09-10 08:40:56 Provost:
2024-09-10 08:40:56 I have a little fellow will buy him one here; and thousand
2024-09-10 08:40:56 fury hence with disposed by the other deed where
2024-09-10 08:40:56 he is, in silly, or willingness so leave and feeling
2024-09-10 08:40:56 to know the busi
2024-09-10 08:40:56 Inference time: 2.31 seconds
2024-09-10 08:40:56 Tokens per second: 216.20
2024-09-10 08:40:56 ---------------
2024-09-10 08:40:58 Sample 3:
2024-09-10 08:40:58  his eye,
2024-09-10 08:40:58 Though all it be present, yet heavens like fire,
2024-09-10 08:40:58 It would be long to his new marriage.
2024-09-10 08:40:58 
2024-09-10 08:40:58 KING RICHARD II:
2024-09-10 08:40:58 Why, so hath a touch'd heir, and victory.
2024-09-10 08:40:58 What cursed manner of the earth to slaughter him?
2024-09-10 08:40:58 
2024-09-10 08:40:58 STANLEY:
2024-09-10 08:40:58 He lives me no like to such a grave;
2024-09-10 08:40:58 This is he that makes between with me all my king.
2024-09-10 08:40:58 
2024-09-10 08:40:58 KING RICHARD III:
2024-09-10 08:40:58 And if they were to be England's tongue.
2024-09-10 08:40:58 
2024-09-10 08:40:58 NORFOLK:
2024-09-10 08:40:58 I came to me from my heart
2024-09-10 08:40:58 To whom I am repeal'd: the soul I am in my first,
2024-09-10 08:40:58 Then to find their services of rough,
2024-09-10 08:40:58 And to thei
2024-09-10 08:40:58 Inference time: 2.19 seconds
2024-09-10 08:40:58 Tokens per second: 228.38
2024-09-10 08:40:58 ---------------
2024-09-10 08:41:00 Sample 4:
2024-09-10 08:41:00  gentleman what would be, of the beast
2024-09-10 08:41:00 From whence the promise of the people,
2024-09-10 08:41:00 His dear limits spatch'd in his face with his bright
2024-09-10 08:41:00 Whipp'd in their hellish ears: him I speak not
2024-09-10 08:41:00 To lie in pure and odd and a prince men
2024-09-10 08:41:00 Namely as I last to go about to the world.
2024-09-10 08:41:00 
2024-09-10 08:41:00 KING RICHARD III:
2024-09-10 08:41:00 Well, we pardon now, madam; but our needs are gone.
2024-09-10 08:41:00 
2024-09-10 08:41:00 QUEEN ELIZABETH:
2024-09-10 08:41:00 So will not needs be you silent and sorrow,
2024-09-10 08:41:00 When men of such good correction lives as he
2024-09-10 08:41:00 Were dead to the matter bear. Well, come forth;
2024-09-10 08:41:00 And cry 'Comme
2024-09-10 08:41:00 Inference time: 2.02 seconds
2024-09-10 08:41:00 Tokens per second: 247.98
2024-09-10 08:41:00 ---------------
2024-09-10 08:41:02 Sample 5:
2024-09-10 08:41:02  a brief
2024-09-10 08:41:02 And, why dost thou impeach my father? Is the woman?
2024-09-10 08:41:02 Once more, one wife, one fairer than died.
2024-09-10 08:41:02 The son of the field is one of his life, I think
2024-09-10 08:41:02 'Twixt my rage, and thy wife is thou canst:
2024-09-10 08:41:02 That shall make so much of things so long.
2024-09-10 08:41:02 
2024-09-10 08:41:02 Nurse:
2024-09-10 08:41:02 How canst thou speak?
2024-09-10 08:41:02 Speak of this day? Canst thou guess the straight?
2024-09-10 08:41:02 
2024-09-10 08:41:02 LADY CAPULET:
2024-09-10 08:41:02 Madam, then, end thy lamp in their hands?
2024-09-10 08:41:02 
2024-09-10 08:41:02 Nurse:
2024-09-10 08:41:02 Peter!
2024-09-10 08:41:02 
2024-09-10 08:41:02 JULIET:
2024-09-10 08:41:02 What's he that is in his power than to death!
2024-09-10 08:41:02 
2024-09-10 08:41:02 Nurse:
2024-09-10 08:41:02 Come hither, I know what this day is thine.
2024-09-10 08:41:02 
2024-09-10 08:41:02 L
2024-09-10 08:41:02 Inference time: 2.02 seconds
2024-09-10 08:41:02 Tokens per second: 247.65
2024-09-10 08:41:02 ---------------
2024-09-10 08:41:04 Sample 6:
2024-09-10 08:41:04  part
2024-09-10 08:41:04 That he shall shorten us all, with a puissant gold
2024-09-10 08:41:04 That they have ta'en not add.
2024-09-10 08:41:04 I shall tell thee, daughter, dear sun, all fall deed.
2024-09-10 08:41:04 
2024-09-10 08:41:04 FRIAR LAURENCE:
2024-09-10 08:41:04 She make knows when the contrary I speak:
2024-09-10 08:41:04 Nothing but for state for reverence when it began;
2024-09-10 08:41:04 Nor though it follows in virtues of the earth,
2024-09-10 08:41:04 From me restrain in such a chastisement of woe,
2024-09-10 08:41:04 Not like the empty of the princess.
2024-09-10 08:41:04 Dear life, what life colours did stay?
2024-09-10 08:41:04 
2024-09-10 08:41:04 ROMEO:
2024-09-10 08:41:04 I pray thee, did more no doubt, sir.
2024-09-10 08:41:04 
2024-09-10 08:41:04 BENVOLIO:
2024-09-10 08:41:04 What pleasure is that, s
2024-09-10 08:41:04 Inference time: 2.01 seconds
2024-09-10 08:41:04 Tokens per second: 248.23
2024-09-10 08:41:04 ---------------
2024-09-10 08:41:06 Sample 7:
2024-09-10 08:41:06  desire.
2024-09-10 08:41:06 
2024-09-10 08:41:06 SICINIUS:
2024-09-10 08:41:06 Why, how now, Caius?
2024-09-10 08:41:06 
2024-09-10 08:41:06 MENENIUS:
2024-09-10 08:41:06 What commands the people?
2024-09-10 08:41:06 
2024-09-10 08:41:06 CORIOLANUS:
2024-09-10 08:41:06 But they will take the from Rome, ho!
2024-09-10 08:41:06 
2024-09-10 08:41:06 CORIOLANUS:
2024-09-10 08:41:06 O my friend, be gone!
2024-09-10 08:41:06 
2024-09-10 08:41:06 Third Servingman:
2024-09-10 08:41:06 What do you neither?
2024-09-10 08:41:06 
2024-09-10 08:41:06 CORIOLANUS:
2024-09-10 08:41:06 Nay, pray; 'tis now I now remember where
2024-09-10 08:41:06 I sabella and the gods, and leave the people at this
2024-09-10 08:41:06 merit of the city. Now, for the poor gentleman roared, good
2024-09-10 08:41:06 butter-stood, being one--that they are made for them against a
2024-09-10 08:41:06 court-cat, and, a bay--prison, a bawd, a bawd, a baase man!
2024-09-10 08:41:06 
2024-09-10 08:41:06 AUTOLY
2024-09-10 08:41:06 Inference time: 1.97 seconds
2024-09-10 08:41:06 Tokens per second: 253.35
2024-09-10 08:41:06 ---------------
2024-09-10 08:41:08 Sample 8:
2024-09-10 08:41:08  chamber-maided men from the fatal throne.
2024-09-10 08:41:08 
2024-09-10 08:41:08 KING RICHARD II:
2024-09-10 08:41:08 You are packing fire for seven bringing in the world.
2024-09-10 08:41:08 Look, I say! the sight of your household together,
2024-09-10 08:41:08 As if you were dared by good to be yours,
2024-09-10 08:41:08 To take your hearts from blood and encounter in war.
2024-09-10 08:41:08 
2024-09-10 08:41:08 KING RICHARD II:
2024-09-10 08:41:08 So, madam, you are not baited with you.
2024-09-10 08:41:08 
2024-09-10 08:41:08 QUEEN ELIZABETH:
2024-09-10 08:41:08 What stay you to him?
2024-09-10 08:41:08 
2024-09-10 08:41:08 KING RICHARD III:
2024-09-10 08:41:08 Say, I will live to my uncle's hand.
2024-09-10 08:41:08 
2024-09-10 08:41:08 QUEEN ELIZABETH:
2024-09-10 08:41:08 What, wilt thou be?
2024-09-10 08:41:08 
2024-09-10 08:41:08 KING RICHARD III:
2024-09-10 08:41:08 Say, yonder son, shalt thou l
2024-09-10 08:41:08 Inference time: 2.08 seconds
2024-09-10 08:41:08 Tokens per second: 239.86
2024-09-10 08:41:08 ---------------
2024-09-10 08:41:10 Sample 9:
2024-09-10 08:41:10  Crosby:
2024-09-10 08:41:10 I have fought so much as a most unpleasant
2024-09-10 08:41:10 For your high reprobation. Your brother is coming;
2024-09-10 08:41:10 We have a theft of beastly blood on by the king,
2024-09-10 08:41:10 And therein you shall seem hungry from his treason.
2024-09-10 08:41:10 
2024-09-10 08:41:10 SICINIUS:
2024-09-10 08:41:10 We may, and with him; but jest with him.
2024-09-10 08:41:10 
2024-09-10 08:41:10 BRUTUS:
2024-09-10 08:41:10 Dispatch them of chastity.
2024-09-10 08:41:10 
2024-09-10 08:41:10 MENENIUS:
2024-09-10 08:41:10 You tell me of the Capitol.
2024-09-10 08:41:10 
2024-09-10 08:41:10 BRUTUS:
2024-09-10 08:41:10 What is it?
2024-09-10 08:41:10 
2024-09-10 08:41:10 MENENIUS:
2024-09-10 08:41:10 What else? why, look your peace.
2024-09-10 08:41:10 
2024-09-10 08:41:10 SICINIUS:
2024-09-10 08:41:10 So it do you tell your things together?
2024-09-10 08:41:10 
2024-09-10 08:41:10 MENENIUS:
2024-09-10 08:41:10 He must not speak of him; nor shall we must
2024-09-10 08:41:10 Inference time: 2.08 seconds
2024-09-10 08:41:10 Tokens per second: 240.25
2024-09-10 08:41:10 ---------------
2024-09-10 08:41:13 Sample 10:
2024-09-10 08:41:13  the next time shall possess to the fame.
2024-09-10 08:41:13 See where she shall seem the crown and defend,
2024-09-10 08:41:13 And say 'The valiant shall return on his hand.'
2024-09-10 08:41:13 Say who comes here? what noise is this?
2024-09-10 08:41:13 Had I not the mother was, when he born to speak?
2024-09-10 08:41:13 When he lived? where is the bastard was the woman?
2024-09-10 08:41:13 Here is one at least, where he should die,
2024-09-10 08:41:13 Had child'd Richard yield the strong, and the which heaven
2024-09-10 08:41:13 Will touch the armour of the secret truth,
2024-09-10 08:41:13 Which we were contradict and to prove a babier
2024-09-10 08:41:13 As bid my father hath been butch
2024-09-10 08:41:13 Inference time: 2.24 seconds
2024-09-10 08:41:13 Tokens per second: 223.14
2024-09-10 08:41:13 ---------------
2024-09-10 08:41:13 Average tokens per second: 238.95
2024-09-10 08:41:13 tokens per iteration will be: 16,384
2024-09-10 08:41:13 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:41:13 Initializing a new model from scratch
2024-09-10 08:41:13 number of parameters: 10.65M
2024-09-10 08:41:13 /app/AI-Scientist/templates/nanoGPT/experiment.py:462: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2024-09-10 08:41:13   scaler = torch.cuda.amp.GradScaler(enabled=(dtype == "float16"))
2024-09-10 08:41:13 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:41:13 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:41:13 using fused AdamW: True
2024-09-10 08:41:13 compiling the model... (takes a ~minute)
2024-09-10 08:41:27 step 0: train loss 4.2469, val loss 4.2417
2024-09-10 08:41:37 iter 0: loss 4.2478, time 24082.79ms
2024-09-10 08:41:38 iter 10: loss 3.2118, time 79.67ms
2024-09-10 08:41:39 iter 20: loss 2.7846, time 79.18ms
2024-09-10 08:41:39 iter 30: loss 2.6130, time 79.14ms
2024-09-10 08:41:40 iter 40: loss 2.5606, time 83.44ms
2024-09-10 08:41:41 iter 50: loss 2.5317, time 78.71ms
2024-09-10 08:41:42 iter 60: loss 2.4923, time 78.64ms
2024-09-10 08:41:43 iter 70: loss 2.4930, time 81.17ms
2024-09-10 08:41:43 iter 80: loss 2.5013, time 79.15ms
2024-09-10 08:41:44 iter 90: loss 2.4811, time 79.43ms
2024-09-10 08:41:45 iter 100: loss 2.4767, time 80.34ms
2024-09-10 08:41:46 iter 110: loss 2.4389, time 78.73ms
2024-09-10 08:41:47 iter 120: loss 2.4325, time 79.04ms
2024-09-10 08:41:47 iter 130: loss 2.4317, time 79.82ms
2024-09-10 08:41:48 iter 140: loss 2.3951, time 80.36ms
2024-09-10 08:41:49 iter 150: loss 2.4021, time 79.41ms
2024-09-10 08:41:50 iter 160: loss 2.3490, time 80.45ms
2024-09-10 08:41:51 iter 170: loss 2.3825, time 79.90ms
2024-09-10 08:41:51 iter 180: loss 2.2903, time 79.27ms
2024-09-10 08:41:52 iter 190: loss 2.2902, time 79.86ms
2024-09-10 08:41:53 iter 200: loss 2.2187, time 79.98ms
2024-09-10 08:41:54 iter 210: loss 2.1731, time 79.32ms
2024-09-10 08:41:55 iter 220: loss 2.1369, time 80.58ms
2024-09-10 08:41:55 iter 230: loss 2.0898, time 79.65ms
2024-09-10 08:41:56 iter 240: loss 2.0426, time 79.73ms
2024-09-10 08:42:07 step 250: train loss 1.9665, val loss 2.0709
2024-09-10 08:42:07 iter 250: loss 2.0434, time 10600.40ms
2024-09-10 08:42:08 iter 260: loss 2.0025, time 79.89ms
2024-09-10 08:42:09 iter 270: loss 1.9524, time 80.27ms
2024-09-10 08:42:10 iter 280: loss 1.9493, time 80.99ms
2024-09-10 08:42:11 iter 290: loss 1.9076, time 80.01ms
2024-09-10 08:42:11 iter 300: loss 1.9086, time 80.24ms
2024-09-10 08:42:12 iter 310: loss 1.8657, time 79.78ms
2024-09-10 08:42:13 iter 320: loss 1.8541, time 80.56ms
2024-09-10 08:42:14 iter 330: loss 1.8025, time 80.19ms
2024-09-10 08:42:15 iter 340: loss 1.8227, time 83.42ms
2024-09-10 08:42:15 iter 350: loss 1.8035, time 80.06ms
2024-09-10 08:42:16 iter 360: loss 1.7838, time 80.00ms
2024-09-10 08:42:17 iter 370: loss 1.7542, time 79.60ms
2024-09-10 08:42:18 iter 380: loss 1.7543, time 82.99ms
2024-09-10 08:42:19 iter 390: loss 1.7226, time 82.43ms
2024-09-10 08:42:19 iter 400: loss 1.7277, time 81.00ms
2024-09-10 08:42:20 iter 410: loss 1.7493, time 80.09ms
2024-09-10 08:42:21 iter 420: loss 1.6854, time 80.26ms
2024-09-10 08:42:22 iter 430: loss 1.6450, time 80.84ms
2024-09-10 08:42:23 iter 440: loss 1.6959, time 80.86ms
2024-09-10 08:42:23 iter 450: loss 1.6354, time 82.88ms
2024-09-10 08:42:24 iter 460: loss 1.6487, time 81.28ms
2024-09-10 08:42:25 iter 470: loss 1.6296, time 80.19ms
2024-09-10 08:42:26 iter 480: loss 1.6411, time 81.04ms
2024-09-10 08:42:27 iter 490: loss 1.6099, time 76.92ms
2024-09-10 08:42:38 step 500: train loss 1.5268, val loss 1.7211
2024-09-10 08:42:38 iter 500: loss 1.5511, time 10721.68ms
2024-09-10 08:42:39 iter 510: loss 1.6124, time 80.70ms
2024-09-10 08:42:40 iter 520: loss 1.5678, time 80.78ms
2024-09-10 08:42:40 iter 530: loss 1.5725, time 80.18ms
2024-09-10 08:42:41 iter 540: loss 1.5848, time 80.89ms
2024-09-10 08:42:42 iter 550: loss 1.5535, time 81.43ms
2024-09-10 08:42:43 iter 560: loss 1.5582, time 82.52ms
2024-09-10 08:42:44 iter 570: loss 1.5569, time 80.90ms
2024-09-10 08:42:44 iter 580: loss 1.5373, time 80.87ms
2024-09-10 08:42:45 iter 590: loss 1.5211, time 82.17ms
2024-09-10 08:42:46 iter 600: loss 1.5136, time 81.63ms
2024-09-10 08:42:47 iter 610: loss 1.5113, time 80.93ms
2024-09-10 08:42:48 iter 620: loss 1.5157, time 80.49ms
2024-09-10 08:42:49 iter 630: loss 1.5186, time 80.98ms
2024-09-10 08:42:49 iter 640: loss 1.4885, time 80.46ms
2024-09-10 08:42:50 iter 650: loss 1.5038, time 80.67ms
2024-09-10 08:42:51 iter 660: loss 1.4785, time 80.84ms
2024-09-10 08:42:52 iter 670: loss 1.4726, time 80.71ms
2024-09-10 08:42:53 iter 680: loss 1.4519, time 81.84ms
2024-09-10 08:42:53 iter 690: loss 1.4622, time 80.10ms
2024-09-10 08:42:54 iter 700: loss 1.4685, time 80.89ms
2024-09-10 08:42:55 iter 710: loss 1.4783, time 84.52ms
2024-09-10 08:42:56 iter 720: loss 1.4687, time 84.37ms
2024-09-10 08:42:57 iter 730: loss 1.4510, time 85.17ms
2024-09-10 08:42:58 iter 740: loss 1.4354, time 81.17ms
2024-09-10 08:43:09 step 750: train loss 1.3560, val loss 1.5790
2024-09-10 08:43:09 iter 750: loss 1.4341, time 10540.23ms
2024-09-10 08:43:09 iter 760: loss 1.4423, time 84.95ms
2024-09-10 08:43:10 iter 770: loss 1.4474, time 80.83ms
2024-09-10 08:43:11 iter 780: loss 1.4401, time 81.42ms
2024-09-10 08:43:12 iter 790: loss 1.4154, time 80.98ms
2024-09-10 08:43:13 iter 800: loss 1.4221, time 81.04ms
2024-09-10 08:43:14 iter 810: loss 1.4237, time 81.87ms
2024-09-10 08:43:14 iter 820: loss 1.3810, time 80.88ms
2024-09-10 08:43:15 iter 830: loss 1.3696, time 82.53ms
2024-09-10 08:43:16 iter 840: loss 1.4028, time 84.24ms
2024-09-10 08:43:17 iter 850: loss 1.3717, time 82.25ms
2024-09-10 08:43:18 iter 860: loss 1.3850, time 85.10ms
2024-09-10 08:43:19 iter 870: loss 1.3943, time 81.04ms
2024-09-10 08:43:19 iter 880: loss 1.3651, time 81.36ms
2024-09-10 08:43:20 iter 890: loss 1.3724, time 86.79ms
2024-09-10 08:43:21 iter 900: loss 1.3463, time 81.07ms
2024-09-10 08:43:22 iter 910: loss 1.3912, time 81.42ms
2024-09-10 08:43:23 iter 920: loss 1.3757, time 81.53ms
2024-09-10 08:43:23 iter 930: loss 1.3649, time 81.28ms
2024-09-10 08:43:24 iter 940: loss 1.3465, time 81.22ms
2024-09-10 08:43:25 iter 950: loss 1.3528, time 81.09ms
2024-09-10 08:43:26 iter 960: loss 1.3807, time 80.99ms
2024-09-10 08:43:27 iter 970: loss 1.3643, time 81.01ms
2024-09-10 08:43:28 iter 980: loss 1.3568, time 81.36ms
2024-09-10 08:43:28 iter 990: loss 1.3513, time 81.37ms
2024-09-10 08:43:39 step 1000: train loss 1.2710, val loss 1.5271
2024-09-10 08:43:39 iter 1000: loss 1.3214, time 10568.31ms
2024-09-10 08:43:40 iter 1010: loss 1.3450, time 81.21ms
2024-09-10 08:43:41 iter 1020: loss 1.3176, time 81.26ms
2024-09-10 08:43:42 iter 1030: loss 1.3521, time 81.50ms
2024-09-10 08:43:43 iter 1040: loss 1.3172, time 81.63ms
2024-09-10 08:43:44 iter 1050: loss 1.3338, time 81.14ms
2024-09-10 08:43:44 iter 1060: loss 1.3340, time 81.87ms
2024-09-10 08:43:45 iter 1070: loss 1.3515, time 81.58ms
2024-09-10 08:43:46 iter 1080: loss 1.3204, time 84.41ms
2024-09-10 08:43:47 iter 1090: loss 1.2725, time 84.52ms
2024-09-10 08:43:48 iter 1100: loss 1.3227, time 86.26ms
2024-09-10 08:43:49 iter 1110: loss 1.3253, time 81.97ms
2024-09-10 08:43:49 iter 1120: loss 1.2959, time 83.39ms
2024-09-10 08:43:50 iter 1130: loss 1.2910, time 81.54ms
2024-09-10 08:43:51 iter 1140: loss 1.3255, time 84.68ms
2024-09-10 08:43:52 iter 1150: loss 1.2679, time 81.14ms
2024-09-10 08:43:53 iter 1160: loss 1.3063, time 82.88ms
2024-09-10 08:43:54 iter 1170: loss 1.2838, time 82.17ms
2024-09-10 08:43:54 iter 1180: loss 1.2754, time 83.46ms
2024-09-10 08:43:55 iter 1190: loss 1.3143, time 84.94ms
2024-09-10 08:43:56 iter 1200: loss 1.2865, time 81.31ms
2024-09-10 08:43:57 iter 1210: loss 1.2898, time 80.45ms
2024-09-10 08:43:58 iter 1220: loss 1.3002, time 83.13ms
2024-09-10 08:43:59 iter 1230: loss 1.2973, time 81.95ms
2024-09-10 08:43:59 iter 1240: loss 1.2945, time 85.21ms
2024-09-10 08:44:11 step 1250: train loss 1.2016, val loss 1.4895
2024-09-10 08:44:11 iter 1250: loss 1.3031, time 10798.35ms
2024-09-10 08:44:12 iter 1260: loss 1.2966, time 81.55ms
2024-09-10 08:44:12 iter 1270: loss 1.3011, time 81.54ms
2024-09-10 08:44:13 iter 1280: loss 1.2714, time 82.93ms
2024-09-10 08:44:14 ^CTraceback (most recent call last):
2024-09-10 08:44:14   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 697, in <module>
2024-09-10 08:44:14     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 08:44:14                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:44:14   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 581, in train
2024-09-10 08:44:14     lossf = loss.item() * gradient_accumulation_steps
2024-09-10 08:44:14             ^^^^^^^^^^^
2024-09-10 08:44:14 KeyboardInterrupt
2024-09-10 08:44:15 
2024-09-10 08:44:24 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi experiment.py 
2024-09-10 08:44:24 
bash: vi: command not found
2024-09-10 08:44:27 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi
2024-09-10 08:44:27 
bash: vi: command not found
2024-09-10 08:44:34 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# apt-get vim
2024-09-10 08:44:34 
E: Invalid operation vim
2024-09-10 08:44:38 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# apt-get install vim
2024-09-10 08:44:38 

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 10%

Reading package lists... Done

2024-09-10 08:44:38 
Building dependency tree... 0%

Building dependency tree... 0%

Building dependency tree... 50%

Building dependency tree... 50%

Building dependency tree... Done

2024-09-10 08:44:38 
Reading state information... 0% 

Reading state information... 0%

Reading state information... Done

2024-09-10 08:44:38 E: Unable to locate package vim
2024-09-10 08:44:42 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi experiment.py 
2024-09-10 08:44:42 
bash: vi: command not found
2024-09-10 08:44:56 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# apt-get install vim
2024-09-10 08:44:56 

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 10%

Reading package lists... Done

2024-09-10 08:44:56 
Building dependency tree... 0%

Building dependency tree... 0%

Building dependency tree... 50%

Building dependency tree... 50%

Building dependency tree... Done

2024-09-10 08:44:56 
Reading state information... 0% 

Reading state information... 0%

Reading state information... Done

2024-09-10 08:44:56 E: Unable to locate package vim
2024-09-10 08:45:05 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# apt-get update
2024-09-10 08:45:05 

2000-01-01 00:00:00 [Working]
            
2001-01-01 00:00:00 http://deb.debian.org/debian bullseye InRelease [116 kB]
2024-09-10 08:45:05 
2000-01-01 00:00:00 [1 InRelease 0 B/116 kB 0%]
                              
2000-01-01 00:00:00 [Working]
            
2001-02-01 00:00:00 http://deb.debian.org/debian-security bullseye-security InRelease [27.2 kB]
2024-09-10 08:45:05 
2000-01-01 00:00:00 [2 InRelease 0 B/27.2 kB 0%]
                               
2000-01-01 00:00:00 [Working]
            
2001-03-01 00:00:00 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]
2024-09-10 08:45:05 
2000-01-01 00:00:00 [3 InRelease 0 B/44.1 kB 0%]
                               
2000-01-01 00:00:00 [Working]
2000-01-01 00:00:00 [Working]
            
2001-04-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 Packages [8066 kB]
2024-09-10 08:45:06 
2000-01-01 00:00:00 [4 Packages 0 B/8066 kB 0%]
2000-01-01 00:00:00 [4 Packages 684 kB/8066 kB 8%]
29% [4 Packages 1495 kB/8066 kB 19%]
1984-01-01 00:00:00 [4 Packages 7399 kB/8066 kB 92%]
                                    
1990-01-01 00:00:00 [Working]
1990-01-01 00:00:00 [4 Packages store 0 B] [Waiting for headers]
                                                
2001-05-01 00:00:00 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [288 kB]
2024-09-10 08:45:06 
1990-01-01 00:00:00 [4 Packages store 0 B] [5 Packages 0 B/288 kB 0%]
                                                     
1993-01-01 00:00:00 [4 Packages store 0 B] [Waiting for headers]
                                                
2001-06-01 00:00:00 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [18.8 kB]
2024-09-10 08:45:06 
1993-01-01 00:00:00 [4 Packages store 0 B] [6 Packages 633 B/18.8 kB 3%]
                                                        
1993-01-01 00:00:00 [4 Packages store 0 B]
1993-01-01 00:00:00 [4 Packages store 0 B]
                          
1996-01-01 00:00:00 [Working]
1996-01-01 00:00:00 [5 Packages store 0 B]
                          
1998-01-01 00:00:00 [Working]
1998-01-01 00:00:00 [6 Packages store 0 B]
                          
0100-01-01 00:00:00 [Working]
              
Fetched 8560 kB in 2s (5209 kB/s)
2024-09-10 08:45:07 
Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 93%

Reading package lists... 93%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... Done

2024-09-10 08:45:13 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# apt-get install vim
2024-09-10 08:45:13 

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 93%

Reading package lists... 93%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... 97%

Reading package lists... Done

2024-09-10 08:45:13 
Building dependency tree... 0%

Building dependency tree... 0%

Building dependency tree... 50%

Building dependency tree... 50%

Building dependency tree... Done

2024-09-10 08:45:13 
Reading state information... 0% 

Reading state information... 0%

Reading state information... Done

2024-09-10 08:45:13 The following additional packages will be installed:
2024-09-10 08:45:13   libgpm2 vim-common vim-runtime xxd
2024-09-10 08:45:13 Suggested packages:
2024-09-10 08:45:13   gpm ctags vim-doc vim-scripts
2024-09-10 08:45:13 The following NEW packages will be installed:
2024-09-10 08:45:13   libgpm2 vim vim-common vim-runtime xxd
2024-09-10 08:45:13 0 upgraded, 5 newly installed, 0 to remove and 40 not upgraded.
2024-09-10 08:45:13 Need to get 8174 kB of archives.
2024-09-10 08:45:13 After this operation, 36.9 MB of additional disk space will be used.
2024-09-10 08:45:16 Do you want to continue? [Y/n] y
2024-09-10 08:45:16 
2000-01-01 00:00:00 [Working]
            
2001-01-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 xxd amd64 2:8.2.2434-3+deb11u1 [192 kB]
2024-09-10 08:45:16 
2000-01-01 00:00:00 [1 xxd 0 B/192 kB 0%]
                        
2001-06-01 00:00:00 [Working]
            
2001-02-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 vim-common all 2:8.2.2434-3+deb11u1 [226 kB]
2024-09-10 08:45:16 
2001-06-01 00:00:00 [2 vim-common 0 B/226 kB 0%]
                               
2001-12-01 00:00:00 [Working]
             
2001-03-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 libgpm2 amd64 1.20.7-8 [35.6 kB]
2024-09-10 08:45:16 
2001-12-01 00:00:00 [3 libgpm2 10.9 kB/35.6 kB 31%]
                                   
16% [Working]
             
2001-04-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 vim-runtime all 2:8.2.2434-3+deb11u1 [6226 kB]
2024-09-10 08:45:16 
17% [4 vim-runtime 18.0 kB/6226 kB 0%]
1972-01-01 00:00:00 [4 vim-runtime 5676 kB/6226 kB 91%]
                                       
1981-01-01 00:00:00 [Working]
             
2001-05-01 00:00:00 http://deb.debian.org/debian bullseye/main amd64 vim amd64 2:8.2.2434-3+deb11u1 [1494 kB]
2024-09-10 08:45:16 
1982-01-01 00:00:00 [5 vim 19.7 kB/1494 kB 1%]
                              
0100-01-01 00:00:00 [Working]
              
Fetched 8174 kB in 1s (10.9 MB/s)
2024-09-10 08:45:16 debconf: delaying package configuration, since apt-utils is not installed
2024-09-10 08:45:17 Selecting previously unselected package xxd.
2024-09-10 08:45:17 (Reading database ... 
(Reading database ... 5%
(Reading database ... 10%
(Reading database ... 15%
(Reading database ... 20%
(Reading database ... 25%
(Reading database ... 30%
(Reading database ... 35%
(Reading database ... 40%
(Reading database ... 45%
(Reading database ... 50%
(Reading database ... 55%
(Reading database ... 60%
(Reading database ... 65%
(Reading database ... 70%
(Reading database ... 75%
(Reading database ... 80%
(Reading database ... 85%
(Reading database ... 90%
(Reading database ... 95%
(Reading database ... 100%
(Reading database ... 242519 files and directories currently installed.)
2024-09-10 08:45:17 Preparing to unpack .../xxd_2%3a8.2.2434-3+deb11u1_amd64.deb ...
2024-09-10 08:45:17 Unpacking xxd (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:17 Selecting previously unselected package vim-common.
2024-09-10 08:45:17 Preparing to unpack .../vim-common_2%3a8.2.2434-3+deb11u1_all.deb ...
2024-09-10 08:45:17 Unpacking vim-common (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:17 Selecting previously unselected package libgpm2:amd64.
2024-09-10 08:45:17 Preparing to unpack .../libgpm2_1.20.7-8_amd64.deb ...
2024-09-10 08:45:17 Unpacking libgpm2:amd64 (1.20.7-8) ...
2024-09-10 08:45:17 Selecting previously unselected package vim-runtime.
2024-09-10 08:45:17 Preparing to unpack .../vim-runtime_2%3a8.2.2434-3+deb11u1_all.deb ...
2024-09-10 08:45:17 Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'
2024-09-10 08:45:17 Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'
2024-09-10 08:45:17 Unpacking vim-runtime (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 Selecting previously unselected package vim.
2024-09-10 08:45:18 Preparing to unpack .../vim_2%3a8.2.2434-3+deb11u1_amd64.deb ...
2024-09-10 08:45:18 Unpacking vim (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 Setting up libgpm2:amd64 (1.20.7-8) ...
2024-09-10 08:45:18 Setting up xxd (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 Setting up vim-common (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 Setting up vim-runtime (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 Setting up vim (2:8.2.2434-3+deb11u1) ...
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode
2024-09-10 08:45:18 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode
2024-09-10 08:45:18 Processing triggers for libc-bin (2.31-13+deb11u10) ...
2024-09-10 08:45:18 Processing triggers for hicolor-icon-theme (0.17-2) ...
2024-09-10 08:45:23 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi experiment.py 
2024-09-10 08:45:23 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# cd templates/nanoGPT && python experiment.py --out_dir run_0 && python plot.py
2024-09-10 08:46:05 
bash: cd: templates/nanoGPT: No such file or directory
2024-09-10 08:46:19 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python experiment.py --out_dir run_0
2024-09-10 08:46:20 
tokens per iteration will be: 16,384
2024-09-10 08:46:20 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:46:20 Initializing a new model from scratch
2024-09-10 08:46:20 number of parameters: 10.65M
2024-09-10 08:46:20 /app/AI-Scientist/templates/nanoGPT/experiment.py:462: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2024-09-10 08:46:20   scaler = torch.cuda.amp.GradScaler(enabled=(dtype == "float16"))
2024-09-10 08:46:20 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:46:20 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:46:21 using fused AdamW: True
2024-09-10 08:46:21 compiling the model... (takes a ~minute)
2024-09-10 08:46:21 ^CTraceback (most recent call last):
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/compile_worker/__main__.py", line 7, in <module>
2024-09-10 08:46:21 Traceback (most recent call last):
2024-09-10 08:46:21     from torch._inductor.async_compile import pre_fork_setup
2024-09-10 08:46:21   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 697, in <module>
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1755, in <module>
2024-09-10 08:46:21     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 08:46:21                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 474, in train
2024-09-10 08:46:21     from .functional import *  # noqa: F403
2024-09-10 08:46:21     ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/functional.py", line 10, in <module>
2024-09-10 08:46:21     import torch.nn.functional as F
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/__init__.py", line 2, in <module>
2024-09-10 08:46:21     model = torch.compile(model)  # requires PyTorch 2.0
2024-09-10 08:46:21             ^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 2111, in compile
2024-09-10 08:46:21     backend = _TorchCompileInductorWrapper(mode, options, dynamic)
2024-09-10 08:46:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1901, in __init__
2024-09-10 08:46:21     from torch._inductor.compile_fx import compile_fx
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 63, in <module>
2024-09-10 08:46:21     from .fx_passes.joint_graph import joint_graph_passes
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/fx_passes/joint_graph.py", line 15, in <module>
2024-09-10 08:46:21     from ..pattern_matcher import (
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py", line 95, in <module>
2024-09-10 08:46:21     from .lowering import fallback_node_due_to_unsupported_type
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/lowering.py", line 6090, in <module>
2024-09-10 08:46:21     from .modules import *  # noqa: F403
2024-09-10 08:46:21     ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 1, in <module>
2024-09-10 08:46:21     from .module import Module
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 9, in <module>
2024-09-10 08:46:21     from torch._prims_common import DeviceLikeType
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/__init__.py", line 95, in <module>
2024-09-10 08:46:21     TensorSequenceType: TypeAlias = Union[List[TensorLikeType], Tuple[TensorLikeType, ...]]
2024-09-10 08:46:21                                           ~~~~^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/typing.py", line 373, in inner
2024-09-10 08:46:21     from . import kernel
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/kernel/__init__.py", line 1, in <module>
2024-09-10 08:46:21     @functools.wraps(func)
2024-09-10 08:46:21     from . import mm, mm_common, mm_plus_mm, unpack_mixed_mm
2024-09-10 08:46:21 
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/kernel/mm.py", line 40, in <module>
2024-09-10 08:46:21 KeyboardInterrupt
2024-09-10 08:46:21     mm_template = TritonTemplate(
2024-09-10 08:46:21                   ^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/select_algorithm.py", line 569, in __init__
2024-09-10 08:46:21     self.template = self._template_from_string(source)
2024-09-10 08:46:21                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/torch/_inductor/codegen/common.py", line 1958, in _template_from_string
2024-09-10 08:46:21     return env.from_string(source)
2024-09-10 08:46:21            ^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/environment.py", line 1108, in from_string
2024-09-10 08:46:21     return cls.from_code(self, self.compile(source), gs, None)
2024-09-10 08:46:21                                ^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/environment.py", line 760, in compile
2024-09-10 08:46:21     source = self._parse(source, name, filename)
2024-09-10 08:46:21              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/environment.py", line 619, in _parse
2024-09-10 08:46:21     return Parser(self, source, name, filename).parse()
2024-09-10 08:46:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/parser.py", line 1039, in parse
2024-09-10 08:46:21     result = nodes.Template(self.subparse(), lineno=1)
2024-09-10 08:46:21                             ^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/parser.py", line 1014, in subparse
2024-09-10 08:46:21     self.stream.expect("variable_end")
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/lexer.py", line 425, in expect
2024-09-10 08:46:21     return next(self)
2024-09-10 08:46:21            ^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/lexer.py", line 391, in __next__
2024-09-10 08:46:21     self.current = next(self._iter)
2024-09-10 08:46:21                    ^^^^^^^^^^^^^^^^
2024-09-10 08:46:21   File "/usr/local/lib/python3.11/site-packages/jinja2/lexer.py", line 667, in wrap
2024-09-10 08:46:21     yield Token(lineno, token, value)
2024-09-10 08:46:21 KeyboardInterrupt
2024-09-10 08:46:22 
2024-09-10 08:46:31 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# 
2024-09-10 08:46:33 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi experiment.py 
2024-09-10 08:46:33 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python experiment.py --out_dir run_0
2024-09-10 08:49:07 
tokens per iteration will be: 16,384
2024-09-10 08:49:07 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:49:07 Initializing a new model from scratch
2024-09-10 08:49:07 number of parameters: 10.65M
2024-09-10 08:49:07 Traceback (most recent call last):
2024-09-10 08:49:07   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 697, in <module>
2024-09-10 08:49:07     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 08:49:07                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:49:07   File "/app/AI-Scientist/templates/nanoGPT/experiment.py", line 462, in train
2024-09-10 08:49:07     scaler = torch.cuda.GradScaler(enabled=('cuda',dtype == "float16"))
2024-09-10 08:49:07              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 08:49:07 AttributeError: module 'torch.cuda' has no attribute 'GradScaler'
2024-09-10 08:49:17 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# vi experiment.py 
2024-09-10 08:49:17 
root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python experiment.py --out_dir run_0
2024-09-10 08:49:33 
tokens per iteration will be: 16,384
2024-09-10 08:49:33 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 08:49:33 Initializing a new model from scratch
2024-09-10 08:49:33 number of parameters: 10.65M
2024-09-10 08:49:33 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 08:49:33 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 08:49:33 using fused AdamW: True
2024-09-10 08:49:33 compiling the model... (takes a ~minute)
2024-09-10 08:49:51 step 0: train loss 4.2874, val loss 4.2823
2024-09-10 08:50:01 iter 0: loss 4.2654, time 26822.87ms
2024-09-10 08:50:02 iter 10: loss 3.2458, time 75.80ms
2024-09-10 08:50:03 iter 20: loss 2.7913, time 77.45ms
2024-09-10 08:50:03 iter 30: loss 2.6356, time 76.96ms
2024-09-10 08:50:04 iter 40: loss 2.5776, time 75.57ms
2024-09-10 08:50:05 iter 50: loss 2.5276, time 76.26ms
2024-09-10 08:50:06 iter 60: loss 2.5196, time 75.68ms
2024-09-10 08:50:06 iter 70: loss 2.4965, time 76.12ms
2024-09-10 08:50:07 iter 80: loss 2.4972, time 77.22ms
2024-09-10 08:50:08 iter 90: loss 2.4684, time 78.11ms
2024-09-10 08:50:09 iter 100: loss 2.4581, time 77.01ms
2024-09-10 08:50:09 iter 110: loss 2.4621, time 76.61ms
2024-09-10 08:50:10 iter 120: loss 2.4269, time 76.94ms
2024-09-10 08:50:11 iter 130: loss 2.4135, time 80.07ms
2024-09-10 08:50:12 iter 140: loss 2.4102, time 80.96ms
2024-09-10 08:50:13 iter 150: loss 2.4154, time 79.90ms
2024-09-10 08:50:13 iter 160: loss 2.3711, time 80.10ms
2024-09-10 08:50:14 iter 170: loss 2.3536, time 80.37ms
2024-09-10 08:50:15 iter 180: loss 2.3136, time 77.86ms
2024-09-10 08:50:16 iter 190: loss 2.2460, time 75.29ms
2024-09-10 08:50:17 iter 200: loss 2.2088, time 75.37ms
2024-09-10 08:50:17 iter 210: loss 2.1395, time 76.55ms
2024-09-10 08:50:18 iter 220: loss 2.1555, time 76.89ms
2024-09-10 08:50:19 iter 230: loss 2.0689, time 76.12ms
2024-09-10 08:50:20 iter 240: loss 2.0782, time 76.72ms
2024-09-10 08:50:30 step 250: train loss 1.9666, val loss 2.0637
2024-09-10 08:50:30 iter 250: loss 2.0319, time 10017.79ms
2024-09-10 08:50:31 iter 260: loss 1.9711, time 77.81ms
2024-09-10 08:50:32 iter 270: loss 1.9823, time 79.05ms
2024-09-10 08:50:33 iter 280: loss 1.9747, time 77.16ms
2024-09-10 08:50:33 iter 290: loss 1.9169, time 76.59ms
2024-09-10 08:50:34 iter 300: loss 1.9030, time 77.22ms
2024-09-10 08:50:35 iter 310: loss 1.8634, time 77.47ms
2024-09-10 08:50:36 iter 320: loss 1.8454, time 77.29ms
2024-09-10 08:50:36 iter 330: loss 1.8202, time 77.37ms
2024-09-10 08:50:37 iter 340: loss 1.7915, time 77.73ms
2024-09-10 08:50:38 iter 350: loss 1.8222, time 77.65ms
2024-09-10 08:50:39 iter 360: loss 1.7695, time 76.36ms
2024-09-10 08:50:40 iter 370: loss 1.7414, time 77.70ms
2024-09-10 08:50:40 iter 380: loss 1.7268, time 77.79ms
2024-09-10 08:50:41 iter 390: loss 1.7294, time 78.52ms
2024-09-10 08:50:42 iter 400: loss 1.7585, time 78.45ms
2024-09-10 08:50:43 iter 410: loss 1.6970, time 77.62ms
2024-09-10 08:50:43 iter 420: loss 1.7136, time 78.45ms
2024-09-10 08:50:44 iter 430: loss 1.6851, time 78.59ms
2024-09-10 08:50:45 iter 440: loss 1.6446, time 78.17ms
2024-09-10 08:50:46 iter 450: loss 1.6561, time 78.54ms
2024-09-10 08:50:47 iter 460: loss 1.6000, time 80.72ms
2024-09-10 08:50:47 iter 470: loss 1.6446, time 78.34ms
2024-09-10 08:50:48 iter 480: loss 1.6188, time 81.02ms
2024-09-10 08:50:49 iter 490: loss 1.6017, time 79.05ms
2024-09-10 08:51:00 step 500: train loss 1.5235, val loss 1.7251
2024-09-10 08:51:00 iter 500: loss 1.6008, time 10426.01ms
2024-09-10 08:51:01 iter 510: loss 1.6059, time 77.93ms
2024-09-10 08:51:02 iter 520: loss 1.5888, time 79.02ms
2024-09-10 08:51:02 iter 530: loss 1.5578, time 80.06ms
2024-09-10 08:51:03 iter 540: loss 1.6173, time 77.59ms
2024-09-10 08:51:04 iter 550: loss 1.5555, time 78.80ms
2024-09-10 08:51:05 iter 560: loss 1.5652, time 79.48ms
2024-09-10 08:51:05 iter 570: loss 1.5705, time 79.20ms
2024-09-10 08:51:06 iter 580: loss 1.5255, time 79.25ms
2024-09-10 08:51:07 iter 590: loss 1.4974, time 77.64ms
2024-09-10 08:51:08 iter 600: loss 1.5135, time 79.08ms
2024-09-10 08:51:09 iter 610: loss 1.5434, time 79.61ms
2024-09-10 08:51:09 iter 620: loss 1.5305, time 80.26ms
2024-09-10 08:51:10 iter 630: loss 1.5064, time 78.57ms
2024-09-10 08:51:11 iter 640: loss 1.4698, time 79.24ms
2024-09-10 08:51:12 iter 650: loss 1.4977, time 78.67ms
2024-09-10 08:51:13 iter 660: loss 1.5071, time 78.40ms
2024-09-10 08:51:13 iter 670: loss 1.4365, time 79.61ms
2024-09-10 08:51:14 iter 680: loss 1.5092, time 79.47ms
2024-09-10 08:51:15 iter 690: loss 1.4637, time 79.00ms
2024-09-10 08:51:16 iter 700: loss 1.4875, time 80.07ms
2024-09-10 08:51:17 iter 710: loss 1.4535, time 79.93ms
2024-09-10 08:51:17 iter 720: loss 1.4402, time 80.59ms
2024-09-10 08:51:18 iter 730: loss 1.4239, time 78.71ms
2024-09-10 08:51:19 iter 740: loss 1.4252, time 78.77ms
2024-09-10 08:51:30 step 750: train loss 1.3611, val loss 1.5887
2024-09-10 08:51:30 iter 750: loss 1.4248, time 10263.55ms
2024-09-10 08:51:31 iter 760: loss 1.4461, time 79.68ms
2024-09-10 08:51:31 iter 770: loss 1.4196, time 79.44ms
2024-09-10 08:51:32 iter 780: loss 1.4156, time 80.08ms
2024-09-10 08:51:33 iter 790: loss 1.4165, time 81.41ms
2024-09-10 08:51:34 iter 800: loss 1.4320, time 80.04ms
2024-09-10 08:51:35 iter 810: loss 1.4060, time 79.85ms
2024-09-10 08:51:35 iter 820: loss 1.4083, time 82.72ms
2024-09-10 08:51:36 iter 830: loss 1.3889, time 83.22ms
2024-09-10 08:51:37 iter 840: loss 1.3932, time 81.03ms
2024-09-10 08:51:38 iter 850: loss 1.3926, time 79.37ms
2024-09-10 08:51:39 iter 860: loss 1.3955, time 81.91ms
2024-09-10 08:51:39 iter 870: loss 1.3964, time 80.35ms
2024-09-10 08:51:40 iter 880: loss 1.3714, time 80.16ms
2024-09-10 08:51:41 iter 890: loss 1.3890, time 80.30ms
2024-09-10 08:51:42 iter 900: loss 1.3752, time 80.39ms
2024-09-10 08:51:43 iter 910: loss 1.3204, time 80.30ms
2024-09-10 08:51:44 iter 920: loss 1.3663, time 81.49ms
2024-09-10 08:51:44 iter 930: loss 1.3614, time 80.21ms
2024-09-10 08:51:45 iter 940: loss 1.3484, time 80.16ms
2024-09-10 08:51:46 iter 950: loss 1.3526, time 81.41ms
2024-09-10 08:51:47 iter 960: loss 1.3600, time 80.59ms
2024-09-10 08:51:48 iter 970: loss 1.3572, time 80.63ms
2024-09-10 08:51:48 iter 980: loss 1.3503, time 79.96ms
2024-09-10 08:51:49 iter 990: loss 1.3468, time 84.82ms
2024-09-10 08:52:00 step 1000: train loss 1.2733, val loss 1.5277
2024-09-10 08:52:00 iter 1000: loss 1.3374, time 10670.22ms
2024-09-10 08:52:01 iter 1010: loss 1.3423, time 81.99ms
2024-09-10 08:52:02 iter 1020: loss 1.3095, time 80.32ms
2024-09-10 08:52:03 iter 1030: loss 1.3414, time 82.52ms
2024-09-10 08:52:04 iter 1040: loss 1.3587, time 80.88ms
2024-09-10 08:52:04 iter 1050: loss 1.2995, time 81.80ms
2024-09-10 08:52:05 iter 1060: loss 1.3408, time 81.05ms
2024-09-10 08:52:06 iter 1070: loss 1.3334, time 80.03ms
2024-09-10 08:52:07 iter 1080: loss 1.3390, time 81.93ms
2024-09-10 08:52:08 iter 1090: loss 1.3506, time 81.55ms
2024-09-10 08:52:09 iter 1100: loss 1.3230, time 80.27ms
2024-09-10 08:52:09 iter 1110: loss 1.2952, time 67.85ms
2024-09-10 08:52:10 iter 1120: loss 1.3018, time 81.74ms
2024-09-10 08:52:11 iter 1130: loss 1.2946, time 85.84ms
2024-09-10 08:52:12 iter 1140: loss 1.3025, time 82.18ms
2024-09-10 08:52:13 iter 1150: loss 1.3059, time 80.66ms
2024-09-10 08:52:13 iter 1160: loss 1.3313, time 80.75ms
2024-09-10 08:52:14 iter 1170: loss 1.2992, time 80.72ms
2024-09-10 08:52:15 iter 1180: loss 1.3254, time 81.40ms
2024-09-10 08:52:16 iter 1190: loss 1.2722, time 80.80ms
2024-09-10 08:52:17 iter 1200: loss 1.2921, time 80.16ms
2024-09-10 08:52:17 iter 1210: loss 1.2698, time 81.45ms
2024-09-10 08:52:18 iter 1220: loss 1.3124, time 81.11ms
2024-09-10 08:52:19 iter 1230: loss 1.2958, time 81.16ms
2024-09-10 08:52:20 iter 1240: loss 1.3056, time 81.15ms
2024-09-10 08:52:31 step 1250: train loss 1.2048, val loss 1.5005
2024-09-10 08:52:31 iter 1250: loss 1.2752, time 10430.44ms
2024-09-10 08:52:32 iter 1260: loss 1.2836, time 80.64ms
2024-09-10 08:52:32 iter 1270: loss 1.2689, time 80.54ms
2024-09-10 08:52:33 iter 1280: loss 1.2564, time 81.69ms
2024-09-10 08:52:34 iter 1290: loss 1.2838, time 81.18ms
2024-09-10 08:52:35 iter 1300: loss 1.3071, time 81.19ms
2024-09-10 08:52:36 iter 1310: loss 1.2383, time 82.26ms
2024-09-10 08:52:37 iter 1320: loss 1.3046, time 81.18ms
2024-09-10 08:52:37 iter 1330: loss 1.2599, time 81.89ms
2024-09-10 08:52:38 iter 1340: loss 1.3030, time 81.39ms
2024-09-10 08:52:39 iter 1350: loss 1.2550, time 81.64ms
2024-09-10 08:52:40 iter 1360: loss 1.2768, time 81.08ms
2024-09-10 08:52:41 iter 1370: loss 1.2585, time 81.08ms
2024-09-10 08:52:41 iter 1380: loss 1.2647, time 81.35ms
2024-09-10 08:52:42 iter 1390: loss 1.2455, time 81.62ms
2024-09-10 08:52:43 iter 1400: loss 1.2528, time 81.31ms
2024-09-10 08:52:44 iter 1410: loss 1.2555, time 81.23ms
2024-09-10 08:52:45 iter 1420: loss 1.2694, time 81.07ms
2024-09-10 08:52:46 iter 1430: loss 1.2401, time 80.81ms
2024-09-10 08:52:46 iter 1440: loss 1.2574, time 81.24ms
2024-09-10 08:52:47 iter 1450: loss 1.2293, time 81.40ms
2024-09-10 08:52:48 iter 1460: loss 1.2405, time 81.13ms
2024-09-10 08:52:49 iter 1470: loss 1.2245, time 81.14ms
2024-09-10 08:52:50 iter 1480: loss 1.2133, time 81.60ms
2024-09-10 08:52:50 iter 1490: loss 1.2430, time 81.09ms
2024-09-10 08:53:01 step 1500: train loss 1.1527, val loss 1.4826
2024-09-10 08:53:01 iter 1500: loss 1.1834, time 10440.96ms
2024-09-10 08:53:02 iter 1510: loss 1.2377, time 81.48ms
2024-09-10 08:53:03 iter 1520: loss 1.2269, time 81.19ms
2024-09-10 08:53:04 iter 1530: loss 1.2528, time 82.12ms
2024-09-10 08:53:05 iter 1540: loss 1.1961, time 81.36ms
2024-09-10 08:53:05 iter 1550: loss 1.2383, time 81.65ms
2024-09-10 08:53:06 iter 1560: loss 1.2040, time 82.76ms
2024-09-10 08:53:07 iter 1570: loss 1.2271, time 80.36ms
2024-09-10 08:53:08 iter 1580: loss 1.2076, time 81.05ms
2024-09-10 08:53:09 iter 1590: loss 1.1922, time 81.37ms
2024-09-10 08:53:10 iter 1600: loss 1.1992, time 81.25ms
2024-09-10 08:53:10 iter 1610: loss 1.2372, time 81.28ms
2024-09-10 08:53:11 iter 1620: loss 1.1830, time 82.52ms
2024-09-10 08:53:12 iter 1630: loss 1.2110, time 80.22ms
2024-09-10 08:53:13 iter 1640: loss 1.2007, time 82.38ms
2024-09-10 08:53:14 iter 1650: loss 1.1760, time 81.07ms
2024-09-10 08:53:14 iter 1660: loss 1.2196, time 82.34ms
2024-09-10 08:53:15 iter 1670: loss 1.1978, time 79.91ms
2024-09-10 08:53:16 iter 1680: loss 1.1970, time 81.12ms
2024-09-10 08:53:17 iter 1690: loss 1.1958, time 81.40ms
2024-09-10 08:53:18 iter 1700: loss 1.1869, time 82.33ms
2024-09-10 08:53:18 iter 1710: loss 1.1744, time 81.46ms
2024-09-10 08:53:19 iter 1720: loss 1.1841, time 81.14ms
2024-09-10 08:53:20 iter 1730: loss 1.2032, time 81.51ms
2024-09-10 08:53:21 iter 1740: loss 1.1734, time 81.87ms
2024-09-10 08:53:32 step 1750: train loss 1.1039, val loss 1.4722
2024-09-10 08:53:32 iter 1750: loss 1.1840, time 10452.89ms
2024-09-10 08:53:33 iter 1760: loss 1.1867, time 81.14ms
2024-09-10 08:53:34 iter 1770: loss 1.1928, time 80.11ms
2024-09-10 08:53:34 iter 1780: loss 1.1971, time 81.50ms
2024-09-10 08:53:35 iter 1790: loss 1.1964, time 81.11ms
2024-09-10 08:53:36 iter 1800: loss 1.1742, time 81.51ms
2024-09-10 08:53:37 iter 1810: loss 1.1609, time 82.06ms
2024-09-10 08:53:38 iter 1820: loss 1.1647, time 81.80ms
2024-09-10 08:53:38 iter 1830: loss 1.1721, time 81.65ms
2024-09-10 08:53:39 iter 1840: loss 1.1625, time 81.49ms
2024-09-10 08:53:40 iter 1850: loss 1.1632, time 81.73ms
2024-09-10 08:53:41 iter 1860: loss 1.1769, time 81.01ms
2024-09-10 08:53:42 iter 1870: loss 1.1388, time 81.34ms
2024-09-10 08:53:43 iter 1880: loss 1.1849, time 81.25ms
2024-09-10 08:53:43 iter 1890: loss 1.1785, time 81.08ms
2024-09-10 08:53:44 iter 1900: loss 1.1280, time 81.73ms
2024-09-10 08:53:45 iter 1910: loss 1.1713, time 81.89ms
2024-09-10 08:53:46 iter 1920: loss 1.1774, time 80.26ms
2024-09-10 08:53:47 iter 1930: loss 1.1516, time 81.21ms
2024-09-10 08:53:47 iter 1940: loss 1.1266, time 84.03ms
2024-09-10 08:53:48 iter 1950: loss 1.1376, time 81.72ms
2024-09-10 08:53:49 iter 1960: loss 1.1492, time 81.85ms
2024-09-10 08:53:50 iter 1970: loss 1.1547, time 84.00ms
2024-09-10 08:53:51 iter 1980: loss 1.1521, time 80.44ms
2024-09-10 08:53:52 iter 1990: loss 1.1502, time 84.57ms
2024-09-10 08:54:03 step 2000: train loss 1.0606, val loss 1.4879
2024-09-10 08:54:03 iter 2000: loss 1.1315, time 10496.41ms
2024-09-10 08:54:03 iter 2010: loss 1.1258, time 81.15ms
2024-09-10 08:54:04 iter 2020: loss 1.1254, time 81.36ms
2024-09-10 08:54:05 iter 2030: loss 1.1566, time 82.29ms
2024-09-10 08:54:06 iter 2040: loss 1.1441, time 82.39ms
2024-09-10 08:54:07 iter 2050: loss 1.1187, time 82.19ms
2024-09-10 08:54:07 iter 2060: loss 1.1088, time 82.08ms
2024-09-10 08:54:08 iter 2070: loss 1.1247, time 81.18ms
2024-09-10 08:54:09 iter 2080: loss 1.1176, time 81.37ms
2024-09-10 08:54:10 iter 2090: loss 1.1402, time 81.12ms
2024-09-10 08:54:11 iter 2100: loss 1.1267, time 81.25ms
2024-09-10 08:54:12 iter 2110: loss 1.1303, time 81.52ms
2024-09-10 08:54:12 iter 2120: loss 1.1354, time 82.85ms
2024-09-10 08:54:13 iter 2130: loss 1.1362, time 81.84ms
2024-09-10 08:54:14 iter 2140: loss 1.1420, time 82.09ms
2024-09-10 08:54:15 iter 2150: loss 1.1234, time 83.07ms
2024-09-10 08:54:16 iter 2160: loss 1.1432, time 81.76ms
2024-09-10 08:54:16 iter 2170: loss 1.1356, time 81.95ms
2024-09-10 08:54:17 iter 2180: loss 1.1133, time 81.57ms
2024-09-10 08:54:18 iter 2190: loss 1.1035, time 81.93ms
2024-09-10 08:54:19 iter 2200: loss 1.1259, time 81.56ms
2024-09-10 08:54:20 iter 2210: loss 1.1217, time 81.22ms
2024-09-10 08:54:21 iter 2220: loss 1.1243, time 80.99ms
2024-09-10 08:54:21 iter 2230: loss 1.1214, time 81.50ms
2024-09-10 08:54:22 iter 2240: loss 1.1248, time 81.54ms
2024-09-10 08:54:33 step 2250: train loss 1.0118, val loss 1.4951
2024-09-10 08:54:33 iter 2250: loss 1.1130, time 10457.20ms
2024-09-10 08:54:34 iter 2260: loss 1.1058, time 81.42ms
2024-09-10 08:54:35 iter 2270: loss 1.1308, time 81.19ms
2024-09-10 08:54:36 iter 2280: loss 1.0980, time 82.36ms
2024-09-10 08:54:36 iter 2290: loss 1.1442, time 81.92ms
2024-09-10 08:54:37 iter 2300: loss 1.1228, time 81.57ms
2024-09-10 08:54:38 iter 2310: loss 1.0987, time 81.76ms
2024-09-10 08:54:39 iter 2320: loss 1.0938, time 81.79ms
2024-09-10 08:54:40 iter 2330: loss 1.0977, time 82.33ms
2024-09-10 08:54:41 iter 2340: loss 1.1170, time 81.35ms
2024-09-10 08:54:41 iter 2350: loss 1.1033, time 81.25ms
2024-09-10 08:54:42 iter 2360: loss 1.1126, time 80.00ms
2024-09-10 08:54:43 iter 2370: loss 1.0897, time 81.62ms
2024-09-10 08:54:44 iter 2380: loss 1.0819, time 82.55ms
2024-09-10 08:54:45 iter 2390: loss 1.0794, time 80.39ms
2024-09-10 08:54:45 iter 2400: loss 1.0781, time 81.25ms
2024-09-10 08:54:46 iter 2410: loss 1.0767, time 81.56ms
2024-09-10 08:54:47 iter 2420: loss 1.0791, time 81.51ms
2024-09-10 08:54:48 iter 2430: loss 1.0588, time 82.30ms
2024-09-10 08:54:49 iter 2440: loss 1.0604, time 80.44ms
2024-09-10 08:54:50 iter 2450: loss 1.0689, time 81.99ms
2024-09-10 08:54:50 iter 2460: loss 1.0882, time 82.16ms
2024-09-10 08:54:51 iter 2470: loss 1.0970, time 82.02ms
2024-09-10 08:54:52 iter 2480: loss 1.0877, time 82.23ms
2024-09-10 08:54:53 iter 2490: loss 1.0522, time 81.96ms
2024-09-10 08:55:04 step 2500: train loss 0.9606, val loss 1.5089
2024-09-10 08:55:04 iter 2500: loss 1.0825, time 10473.75ms
2024-09-10 08:55:05 iter 2510: loss 1.0699, time 81.38ms
2024-09-10 08:55:05 iter 2520: loss 1.0451, time 81.32ms
2024-09-10 08:55:06 iter 2530: loss 1.0516, time 81.85ms
2024-09-10 08:55:07 iter 2540: loss 1.0568, time 82.79ms
2024-09-10 08:55:08 iter 2550: loss 1.0678, time 81.20ms
2024-09-10 08:55:09 iter 2560: loss 1.0583, time 81.19ms
2024-09-10 08:55:10 iter 2570: loss 1.0647, time 81.49ms
2024-09-10 08:55:10 iter 2580: loss 1.0801, time 81.44ms
2024-09-10 08:55:11 iter 2590: loss 1.0636, time 82.96ms
2024-09-10 08:55:12 iter 2600: loss 1.0663, time 81.47ms
2024-09-10 08:55:13 iter 2610: loss 1.0530, time 82.03ms
2024-09-10 08:55:14 iter 2620: loss 1.0451, time 81.55ms
2024-09-10 08:55:14 iter 2630: loss 1.0261, time 81.52ms
2024-09-10 08:55:15 iter 2640: loss 1.0418, time 81.35ms
2024-09-10 08:55:16 iter 2650: loss 1.0635, time 81.75ms
2024-09-10 08:55:17 iter 2660: loss 1.0503, time 82.65ms
2024-09-10 08:55:18 iter 2670: loss 1.0203, time 81.89ms
2024-09-10 08:55:19 iter 2680: loss 1.0445, time 82.40ms
2024-09-10 08:55:19 iter 2690: loss 1.0519, time 81.45ms
2024-09-10 08:55:20 iter 2700: loss 1.0208, time 81.51ms
2024-09-10 08:55:21 iter 2710: loss 1.0506, time 81.67ms
2024-09-10 08:55:22 iter 2720: loss 1.0425, time 83.13ms
2024-09-10 08:55:23 iter 2730: loss 1.0584, time 81.72ms
2024-09-10 08:55:23 iter 2740: loss 1.0216, time 81.79ms
2024-09-10 08:55:34 step 2750: train loss 0.9165, val loss 1.5200
2024-09-10 08:55:34 iter 2750: loss 1.0316, time 10498.29ms
2024-09-10 08:55:35 iter 2760: loss 1.0226, time 81.93ms
2024-09-10 08:55:36 iter 2770: loss 1.0188, time 82.11ms
2024-09-10 08:55:37 iter 2780: loss 1.0135, time 83.40ms
2024-09-10 08:55:38 iter 2790: loss 1.0375, time 81.78ms
2024-09-10 08:55:39 iter 2800: loss 1.0148, time 81.96ms
2024-09-10 08:55:39 iter 2810: loss 1.0420, time 82.03ms
2024-09-10 08:55:40 iter 2820: loss 1.0229, time 81.76ms
2024-09-10 08:55:41 iter 2830: loss 1.0332, time 81.72ms
2024-09-10 08:55:42 iter 2840: loss 0.9991, time 81.60ms
2024-09-10 08:55:43 iter 2850: loss 1.0294, time 81.57ms
2024-09-10 08:55:44 iter 2860: loss 1.0277, time 81.83ms
2024-09-10 08:55:44 iter 2870: loss 1.0073, time 81.17ms
2024-09-10 08:55:45 iter 2880: loss 1.0363, time 82.22ms
2024-09-10 08:55:46 iter 2890: loss 1.0053, time 82.30ms
2024-09-10 08:55:47 iter 2900: loss 0.9994, time 82.87ms
2024-09-10 08:55:48 iter 2910: loss 1.0408, time 81.27ms
2024-09-10 08:55:48 iter 2920: loss 1.0051, time 82.40ms
2024-09-10 08:55:49 iter 2930: loss 0.9968, time 81.69ms
2024-09-10 08:55:50 iter 2940: loss 0.9847, time 81.55ms
2024-09-10 08:55:51 iter 2950: loss 1.0179, time 81.98ms
2024-09-10 08:55:52 iter 2960: loss 0.9986, time 81.47ms
2024-09-10 08:55:53 iter 2970: loss 0.9890, time 81.43ms
2024-09-10 08:55:53 iter 2980: loss 0.9983, time 81.74ms
2024-09-10 08:55:54 iter 2990: loss 0.9814, time 82.48ms
2024-09-10 08:56:05 step 3000: train loss 0.8666, val loss 1.5407
2024-09-10 08:56:05 iter 3000: loss 0.9880, time 10496.18ms
2024-09-10 08:56:06 iter 3010: loss 0.9909, time 82.10ms
2024-09-10 08:56:07 iter 3020: loss 1.0007, time 82.16ms
2024-09-10 08:56:08 iter 3030: loss 1.0117, time 82.43ms
2024-09-10 08:56:08 iter 3040: loss 1.0183, time 81.35ms
2024-09-10 08:56:09 iter 3050: loss 0.9826, time 81.87ms
2024-09-10 08:56:10 iter 3060: loss 0.9984, time 81.73ms
2024-09-10 08:56:11 iter 3070: loss 1.0151, time 82.12ms
2024-09-10 08:56:12 iter 3080: loss 0.9983, time 81.76ms
2024-09-10 08:56:13 iter 3090: loss 0.9898, time 81.14ms
2024-09-10 08:56:13 iter 3100: loss 0.9964, time 81.31ms
2024-09-10 08:56:14 iter 3110: loss 0.9755, time 82.15ms
2024-09-10 08:56:15 iter 3120: loss 0.9983, time 82.34ms
2024-09-10 08:56:16 iter 3130: loss 0.9741, time 81.75ms
2024-09-10 08:56:17 iter 3140: loss 0.9737, time 82.13ms
2024-09-10 08:56:18 iter 3150: loss 0.9925, time 82.36ms
2024-09-10 08:56:18 iter 3160: loss 1.0073, time 82.58ms
2024-09-10 08:56:19 iter 3170: loss 0.9608, time 81.72ms
2024-09-10 08:56:20 iter 3180: loss 0.9769, time 82.23ms
2024-09-10 08:56:21 iter 3190: loss 0.9983, time 82.15ms
2024-09-10 08:56:22 iter 3200: loss 0.9642, time 81.99ms
2024-09-10 08:56:22 iter 3210: loss 0.9711, time 81.67ms
2024-09-10 08:56:23 iter 3220: loss 0.9561, time 82.44ms
2024-09-10 08:56:24 iter 3230: loss 0.9512, time 82.09ms
2024-09-10 08:56:25 iter 3240: loss 0.9553, time 82.14ms
2024-09-10 08:56:36 step 3250: train loss 0.8224, val loss 1.5724
2024-09-10 08:56:36 iter 3250: loss 0.9682, time 10496.61ms
2024-09-10 08:56:37 iter 3260: loss 0.9690, time 81.56ms
2024-09-10 08:56:38 iter 3270: loss 0.9687, time 81.00ms
2024-09-10 08:56:38 iter 3280: loss 0.9503, time 81.93ms
2024-09-10 08:56:39 iter 3290: loss 0.9517, time 82.33ms
2024-09-10 08:56:40 iter 3300: loss 0.9366, time 81.87ms
2024-09-10 08:56:41 iter 3310: loss 0.9519, time 82.43ms
2024-09-10 08:56:42 iter 3320: loss 0.9652, time 80.66ms
2024-09-10 08:56:42 iter 3330: loss 0.9606, time 82.04ms
2024-09-10 08:56:43 iter 3340: loss 0.9528, time 82.29ms
2024-09-10 08:56:44 iter 3350: loss 0.9604, time 82.60ms
2024-09-10 08:56:45 iter 3360: loss 0.9244, time 81.45ms
2024-09-10 08:56:46 iter 3370: loss 0.9569, time 81.29ms
2024-09-10 08:56:47 iter 3380: loss 0.9398, time 82.58ms
2024-09-10 08:56:47 iter 3390: loss 0.9482, time 81.87ms
2024-09-10 08:56:48 iter 3400: loss 0.9558, time 81.76ms
2024-09-10 08:56:49 iter 3410: loss 0.9447, time 81.70ms
2024-09-10 08:56:50 iter 3420: loss 0.9522, time 82.55ms
2024-09-10 08:56:51 iter 3430: loss 0.9467, time 81.92ms
2024-09-10 08:56:51 iter 3440: loss 0.9768, time 82.72ms
2024-09-10 08:56:52 iter 3450: loss 0.9565, time 80.73ms
2024-09-10 08:56:53 iter 3460: loss 0.9473, time 84.04ms
2024-09-10 08:56:54 iter 3470: loss 0.9460, time 82.09ms
2024-09-10 08:56:55 iter 3480: loss 0.9488, time 80.16ms
2024-09-10 08:56:56 iter 3490: loss 0.9144, time 81.33ms
2024-09-10 08:57:07 step 3500: train loss 0.7822, val loss 1.5846
2024-09-10 08:57:07 iter 3500: loss 0.9063, time 10513.86ms
2024-09-10 08:57:07 iter 3510: loss 0.9196, time 80.55ms
2024-09-10 08:57:08 iter 3520: loss 0.9272, time 81.48ms
2024-09-10 08:57:09 iter 3530: loss 0.9547, time 80.67ms
2024-09-10 08:57:10 iter 3540: loss 0.9358, time 82.31ms
2024-09-10 08:57:11 iter 3550: loss 0.9306, time 83.18ms
2024-09-10 08:57:12 iter 3560: loss 0.9555, time 82.97ms
2024-09-10 08:57:12 iter 3570: loss 0.9400, time 81.92ms
2024-09-10 08:57:13 iter 3580: loss 0.9299, time 82.00ms
2024-09-10 08:57:14 iter 3590: loss 0.9212, time 81.99ms
2024-09-10 08:57:15 iter 3600: loss 0.9246, time 82.23ms
2024-09-10 08:57:16 iter 3610: loss 0.9122, time 81.65ms
2024-09-10 08:57:16 iter 3620: loss 0.9068, time 81.15ms
2024-09-10 08:57:17 iter 3630: loss 0.9293, time 82.16ms
2024-09-10 08:57:18 iter 3640: loss 0.9089, time 81.07ms
2024-09-10 08:57:19 iter 3650: loss 0.9121, time 82.52ms
2024-09-10 08:57:20 iter 3660: loss 0.9331, time 81.85ms
2024-09-10 08:57:21 iter 3670: loss 0.9388, time 82.04ms
2024-09-10 08:57:21 iter 3680: loss 0.9069, time 81.83ms
2024-09-10 08:57:22 iter 3690: loss 0.9295, time 81.02ms
2024-09-10 08:57:23 iter 3700: loss 0.8808, time 82.31ms
2024-09-10 08:57:24 iter 3710: loss 0.8813, time 82.35ms
2024-09-10 08:57:25 iter 3720: loss 0.9079, time 82.13ms
2024-09-10 08:57:25 iter 3730: loss 0.9091, time 82.03ms
2024-09-10 08:57:26 iter 3740: loss 0.9022, time 81.96ms
2024-09-10 08:57:37 step 3750: train loss 0.7420, val loss 1.6122
2024-09-10 08:57:37 iter 3750: loss 0.9036, time 10494.77ms
2024-09-10 08:57:38 iter 3760: loss 0.9370, time 81.73ms
2024-09-10 08:57:39 iter 3770: loss 0.9291, time 81.31ms
2024-09-10 08:57:40 iter 3780: loss 0.9190, time 81.42ms
2024-09-10 08:57:41 iter 3790: loss 0.8965, time 81.96ms
2024-09-10 08:57:41 iter 3800: loss 0.9116, time 81.48ms
2024-09-10 08:57:42 iter 3810: loss 0.9217, time 81.15ms
2024-09-10 08:57:43 iter 3820: loss 0.8966, time 81.73ms
2024-09-10 08:57:44 iter 3830: loss 0.9001, time 81.46ms
2024-09-10 08:57:45 iter 3840: loss 0.8912, time 81.27ms
2024-09-10 08:57:46 iter 3850: loss 0.8880, time 81.71ms
2024-09-10 08:57:46 iter 3860: loss 0.8707, time 81.52ms
2024-09-10 08:57:47 iter 3870: loss 0.8878, time 82.02ms
2024-09-10 08:57:48 iter 3880: loss 0.8917, time 81.80ms
2024-09-10 08:57:49 iter 3890: loss 0.8902, time 82.29ms
2024-09-10 08:57:50 iter 3900: loss 0.8924, time 81.56ms
2024-09-10 08:57:50 iter 3910: loss 0.8904, time 81.48ms
2024-09-10 08:57:51 iter 3920: loss 0.8691, time 81.62ms
2024-09-10 08:57:52 iter 3930: loss 0.8935, time 81.47ms
2024-09-10 08:57:53 iter 3940: loss 0.8710, time 81.18ms
2024-09-10 08:57:54 iter 3950: loss 0.8763, time 81.27ms
2024-09-10 08:57:55 iter 3960: loss 0.9065, time 81.84ms
2024-09-10 08:57:55 iter 3970: loss 0.9015, time 81.61ms
2024-09-10 08:57:56 iter 3980: loss 0.8941, time 82.86ms
2024-09-10 08:57:57 iter 3990: loss 0.8778, time 80.93ms
2024-09-10 08:58:08 step 4000: train loss 0.7104, val loss 1.6326
2024-09-10 08:58:08 iter 4000: loss 0.8589, time 10662.61ms
2024-09-10 08:58:09 iter 4010: loss 0.8823, time 82.31ms
2024-09-10 08:58:10 iter 4020: loss 0.8848, time 81.38ms
2024-09-10 08:58:11 iter 4030: loss 0.8844, time 82.49ms
2024-09-10 08:58:11 iter 4040: loss 0.8772, time 82.32ms
2024-09-10 08:58:12 iter 4050: loss 0.8692, time 81.68ms
2024-09-10 08:58:13 iter 4060: loss 0.8652, time 82.64ms
2024-09-10 08:58:14 iter 4070: loss 0.8631, time 80.43ms
2024-09-10 08:58:15 iter 4080: loss 0.8911, time 81.31ms
2024-09-10 08:58:16 iter 4090: loss 0.8537, time 81.57ms
2024-09-10 08:58:16 iter 4100: loss 0.9084, time 81.41ms
2024-09-10 08:58:17 iter 4110: loss 0.8699, time 81.47ms
2024-09-10 08:58:18 iter 4120: loss 0.8757, time 81.56ms
2024-09-10 08:58:19 iter 4130: loss 0.8534, time 80.20ms
2024-09-10 08:58:20 iter 4140: loss 0.8786, time 81.51ms
2024-09-10 08:58:20 iter 4150: loss 0.8766, time 81.23ms
2024-09-10 08:58:21 iter 4160: loss 0.8587, time 80.47ms
2024-09-10 08:58:22 iter 4170: loss 0.8640, time 81.47ms
2024-09-10 08:58:23 iter 4180: loss 0.8711, time 81.22ms
2024-09-10 08:58:24 iter 4190: loss 0.8744, time 81.76ms
2024-09-10 08:58:25 iter 4200: loss 0.8594, time 81.71ms
2024-09-10 08:58:25 iter 4210: loss 0.8695, time 81.71ms
2024-09-10 08:58:26 iter 4220: loss 0.8599, time 81.69ms
2024-09-10 08:58:27 iter 4230: loss 0.8831, time 81.60ms
2024-09-10 08:58:28 iter 4240: loss 0.8619, time 82.90ms
2024-09-10 08:58:39 step 4250: train loss 0.6789, val loss 1.6606
2024-09-10 08:58:39 iter 4250: loss 0.8712, time 10593.16ms
2024-09-10 08:58:40 iter 4260: loss 0.8580, time 81.35ms
2024-09-10 08:58:41 iter 4270: loss 0.8676, time 83.06ms
2024-09-10 08:58:41 iter 4280: loss 0.8550, time 80.83ms
2024-09-10 08:58:42 iter 4290: loss 0.8265, time 81.88ms
2024-09-10 08:58:43 iter 4300: loss 0.8311, time 82.74ms
2024-09-10 08:58:44 iter 4310: loss 0.8509, time 82.60ms
2024-09-10 08:58:45 iter 4320: loss 0.8371, time 81.60ms
2024-09-10 08:58:45 iter 4330: loss 0.8572, time 82.26ms
2024-09-10 08:58:46 iter 4340: loss 0.8373, time 82.03ms
2024-09-10 08:58:47 iter 4350: loss 0.8394, time 81.91ms
2024-09-10 08:58:48 iter 4360: loss 0.8569, time 81.04ms
2024-09-10 08:58:49 iter 4370: loss 0.8557, time 82.24ms
2024-09-10 08:58:50 iter 4380: loss 0.8404, time 83.17ms
2024-09-10 08:58:50 iter 4390: loss 0.8644, time 83.13ms
2024-09-10 08:58:51 iter 4400: loss 0.8528, time 81.31ms
2024-09-10 08:58:52 iter 4410: loss 0.8691, time 81.84ms
2024-09-10 08:58:53 iter 4420: loss 0.8656, time 81.47ms
2024-09-10 08:58:54 iter 4430: loss 0.8505, time 80.94ms
2024-09-10 08:58:55 iter 4440: loss 0.8486, time 80.27ms
2024-09-10 08:58:55 iter 4450: loss 0.8498, time 81.76ms
2024-09-10 08:58:56 iter 4460: loss 0.8343, time 82.12ms
2024-09-10 08:58:57 iter 4470: loss 0.8565, time 80.38ms
2024-09-10 08:58:58 iter 4480: loss 0.8390, time 81.57ms
2024-09-10 08:58:59 iter 4490: loss 0.8421, time 80.88ms
2024-09-10 08:59:10 step 4500: train loss 0.6536, val loss 1.6826
2024-09-10 08:59:10 iter 4500: loss 0.8645, time 10566.17ms
2024-09-10 08:59:11 iter 4510: loss 0.8465, time 82.20ms
2024-09-10 08:59:11 iter 4520: loss 0.8404, time 81.84ms
2024-09-10 08:59:12 iter 4530: loss 0.8480, time 81.85ms
2024-09-10 08:59:13 iter 4540: loss 0.8499, time 81.94ms
2024-09-10 08:59:14 iter 4550: loss 0.8656, time 82.37ms
2024-09-10 08:59:15 iter 4560: loss 0.8388, time 81.86ms
2024-09-10 08:59:15 iter 4570: loss 0.8393, time 81.45ms
2024-09-10 08:59:16 iter 4580: loss 0.8573, time 81.65ms
2024-09-10 08:59:17 iter 4590: loss 0.8550, time 81.86ms
2024-09-10 08:59:18 iter 4600: loss 0.8275, time 80.63ms
2024-09-10 08:59:19 iter 4610: loss 0.8716, time 81.95ms
2024-09-10 08:59:20 iter 4620: loss 0.8397, time 82.09ms
2024-09-10 08:59:20 iter 4630: loss 0.8289, time 86.04ms
2024-09-10 08:59:21 iter 4640: loss 0.8427, time 81.58ms
2024-09-10 08:59:22 iter 4650: loss 0.8522, time 81.60ms
2024-09-10 08:59:23 iter 4660: loss 0.8551, time 84.21ms
2024-09-10 08:59:24 iter 4670: loss 0.8370, time 82.79ms
2024-09-10 08:59:25 iter 4680: loss 0.8538, time 79.89ms
2024-09-10 08:59:25 iter 4690: loss 0.8425, time 83.71ms
2024-09-10 08:59:26 iter 4700: loss 0.8250, time 82.27ms
2024-09-10 08:59:27 iter 4710: loss 0.7939, time 82.43ms
2024-09-10 08:59:28 iter 4720: loss 0.8335, time 83.45ms
2024-09-10 08:59:29 iter 4730: loss 0.8216, time 81.52ms
2024-09-10 08:59:30 iter 4740: loss 0.8381, time 87.33ms
2024-09-10 08:59:41 step 4750: train loss 0.6368, val loss 1.6987
2024-09-10 08:59:41 iter 4750: loss 0.7984, time 10721.38ms
2024-09-10 08:59:42 iter 4760: loss 0.8225, time 81.98ms
2024-09-10 08:59:42 iter 4770: loss 0.7972, time 81.40ms
2024-09-10 08:59:43 iter 4780: loss 0.8033, time 82.67ms
2024-09-10 08:59:44 iter 4790: loss 0.8310, time 82.20ms
2024-09-10 08:59:45 iter 4800: loss 0.8204, time 82.04ms
2024-09-10 08:59:46 iter 4810: loss 0.8373, time 82.73ms
2024-09-10 08:59:47 iter 4820: loss 0.8223, time 81.91ms
2024-09-10 08:59:47 iter 4830: loss 0.8215, time 81.26ms
2024-09-10 08:59:48 iter 4840: loss 0.8352, time 81.65ms
2024-09-10 08:59:49 iter 4850: loss 0.8272, time 81.43ms
2024-09-10 08:59:50 iter 4860: loss 0.8180, time 81.63ms
2024-09-10 08:59:51 iter 4870: loss 0.8134, time 81.47ms
2024-09-10 08:59:51 iter 4880: loss 0.8294, time 82.04ms
2024-09-10 08:59:52 iter 4890: loss 0.8063, time 81.85ms
2024-09-10 08:59:53 iter 4900: loss 0.8046, time 81.85ms
2024-09-10 08:59:54 iter 4910: loss 0.8298, time 82.14ms
2024-09-10 08:59:55 iter 4920: loss 0.8169, time 82.19ms
2024-09-10 08:59:56 iter 4930: loss 0.8194, time 82.32ms
2024-09-10 08:59:56 iter 4940: loss 0.8044, time 82.06ms
2024-09-10 08:59:57 iter 4950: loss 0.8335, time 82.30ms
2024-09-10 08:59:58 iter 4960: loss 0.8294, time 81.55ms
2024-09-10 08:59:59 iter 4970: loss 0.7962, time 81.45ms
2024-09-10 09:00:00 iter 4980: loss 0.7961, time 81.53ms
2024-09-10 09:00:00 iter 4990: loss 0.8295, time 82.28ms
2024-09-10 09:00:11 step 5000: train loss 0.6228, val loss 1.7090
2024-09-10 09:00:12 iter 5000: loss 0.8208, time 10498.16ms
2024-09-10 09:00:12 training done
2024-09-10 09:00:12 Best validation loss: 1.4722304344177246
2024-09-10 09:00:12 Total train time: 10.62 mins
2024-09-10 09:00:12 Loading meta from ../../data/shakespeare_char/meta.pkl...
2024-09-10 09:00:14 Sample 1:
2024-09-10 09:00:14  beasts;
2024-09-10 09:00:14 And yet, to the world see how shortly coals
2024-09-10 09:00:14 Upon the sacred mouths, ere he did strike
2024-09-10 09:00:14 When piece was made by Angelo he made.
2024-09-10 09:00:14 
2024-09-10 09:00:14 ISABELLA:
2024-09-10 09:00:14 O, good sir!
2024-09-10 09:00:14 
2024-09-10 09:00:14 ANGELO:
2024-09-10 09:00:14 Well, then, we have found it so.
2024-09-10 09:00:14 
2024-09-10 09:00:14 ISABELLA:
2024-09-10 09:00:14 O, 'tis gone, Tranio. Gentlemen, go,
2024-09-10 09:00:14 I'll quit thee Jove's with flower.
2024-09-10 09:00:14 
2024-09-10 09:00:14 LUCIO:
2024-09-10 09:00:14 Grace, do you say 'tis time to know him where
2024-09-10 09:00:14 I was a dull told upon him that is counterpoised.
2024-09-10 09:00:14 
2024-09-10 09:00:14 ISABELLA:
2024-09-10 09:00:14 I think it is: and he has he told me to that;
2024-09-10 09:00:14 the true is conducted to his life, his gracious pains
2024-09-10 09:00:14 ens
2024-09-10 09:00:14 Inference time: 2.27 seconds
2024-09-10 09:00:14 Tokens per second: 220.07
2024-09-10 09:00:14 ---------------
2024-09-10 09:00:16 Sample 2:
2024-09-10 09:00:16  sunder from the dead?
2024-09-10 09:00:16 Or else this executioner will I stay?
2024-09-10 09:00:16 
2024-09-10 09:00:16 TRANIO:
2024-09-10 09:00:16 No, but I'll do what I can to have her:
2024-09-10 09:00:16 There is my consent, not am I sent to the purpose.
2024-09-10 09:00:16 
2024-09-10 09:00:16 TRANIO:
2024-09-10 09:00:16 I hope so, sir, if it be not a man condemn'd
2024-09-10 09:00:16 By selfsame so fast as the air words,
2024-09-10 09:00:16 And set it down, and after me.
2024-09-10 09:00:16 
2024-09-10 09:00:16 LUCENTIO:
2024-09-10 09:00:16 Now, sir, you know no time hath kept till you
2024-09-10 09:00:16 But to the determined hate you of your love.
2024-09-10 09:00:16 Therefore I'll be consul, I see, you know not
2024-09-10 09:00:16 The duke is dead; and, indeed, till the very medlar
2024-09-10 09:00:16 Will be with your
2024-09-10 09:00:16 Inference time: 1.95 seconds
2024-09-10 09:00:16 Tokens per second: 256.87
2024-09-10 09:00:16 ---------------
2024-09-10 09:00:18 Sample 3:
2024-09-10 09:00:18  bastard me that hold she lose.
2024-09-10 09:00:18 
2024-09-10 09:00:18 JOHN OF GAUNT:
2024-09-10 09:00:18 I have heard her princely father, and she is
2024-09-10 09:00:18 In my life all my body to see her bed.
2024-09-10 09:00:18 In brief, fair sun, and post all things farewell.
2024-09-10 09:00:18 What comfort have I, that made I call thee down?
2024-09-10 09:00:18 
2024-09-10 09:00:18 DUKE OF YORK:
2024-09-10 09:00:18 Peace, Thomas Mowbray, Duke of Norfolk, Montague,
2024-09-10 09:00:18 With death hated matter aid of their deaths;
2024-09-10 09:00:18 And let him rest with death, and to make him more
2024-09-10 09:00:18 Than mine own own his grace suffer'd in the malmsey-breed.
2024-09-10 09:00:18 
2024-09-10 09:00:18 NORTHUMBERLAND:
2024-09-10 09:00:18 My lord, the Duke of Lancaster is 
2024-09-10 09:00:18 Inference time: 2.31 seconds
2024-09-10 09:00:18 Tokens per second: 216.20
2024-09-10 09:00:18 ---------------
2024-09-10 09:00:20 Sample 4:
2024-09-10 09:00:20  let us all to meet
2024-09-10 09:00:20 Me for a well-weary, that I may name thy power
2024-09-10 09:00:20 And discover to answer: but here it is none of
2024-09-10 09:00:20 To speak what is my design.
2024-09-10 09:00:20 
2024-09-10 09:00:20 MENENIUS:
2024-09-10 09:00:20 Be it then gone,
2024-09-10 09:00:20 Let her speak again.
2024-09-10 09:00:20 
2024-09-10 09:00:20 SICINIUS:
2024-09-10 09:00:20 Shall the curtain?
2024-09-10 09:00:20 
2024-09-10 09:00:20 MENENIUS:
2024-09-10 09:00:20 Neither, better than he comes.
2024-09-10 09:00:20 
2024-09-10 09:00:20 CORIOLANUS:
2024-09-10 09:00:20 Nay, he comes.
2024-09-10 09:00:20 
2024-09-10 09:00:20 VOLUMNIA:
2024-09-10 09:00:20 I had rather have some been as these are
2024-09-10 09:00:20 The streets of Rome are his powerful instruction.
2024-09-10 09:00:20 There is the range and the bear whipp'd their verge
2024-09-10 09:00:20 Would black the soldiers: then there we our like them,
2024-09-10 09:00:20 A
2024-09-10 09:00:20 Inference time: 2.02 seconds
2024-09-10 09:00:20 Tokens per second: 247.96
2024-09-10 09:00:20 ---------------
2024-09-10 09:00:22 Sample 5:
2024-09-10 09:00:22  will remain the down of allegiance
2024-09-10 09:00:22 Till the sun sets in his flower: if he stood and his unswept,
2024-09-10 09:00:22 He thinks from his most choice would not sleep
2024-09-10 09:00:22 A parcel fellow my mistress shall be done with him.
2024-09-10 09:00:22 He that from mercy of this most gracious prince,
2024-09-10 09:00:22 Is in a lists, a good steed as it is a mile,
2024-09-10 09:00:22 To be your fresh and such a nature leaves
2024-09-10 09:00:22 Of your kinsman's point.
2024-09-10 09:00:22 
2024-09-10 09:00:22 ISABELLA:
2024-09-10 09:00:22 That's the matter.
2024-09-10 09:00:22 
2024-09-10 09:00:22 DUKE VINCENTIO:
2024-09-10 09:00:22 I protest you, sir, a devilish of
2024-09-10 09:00:22 your lord's service; that I have aim'd you at home, that you s
2024-09-10 09:00:22 Inference time: 1.94 seconds
2024-09-10 09:00:22 Tokens per second: 258.01
2024-09-10 09:00:22 ---------------
2024-09-10 09:00:24 Sample 6:
2024-09-10 09:00:24  my father's son,
2024-09-10 09:00:24 She would live, but that they love it so.
2024-09-10 09:00:24 Ah, what, shall I be married of this?
2024-09-10 09:00:24 
2024-09-10 09:00:24 TYRREL:
2024-09-10 09:00:24 Uncle, I fear it not.
2024-09-10 09:00:24 
2024-09-10 09:00:24 KING RICHARD III:
2024-09-10 09:00:24 Norfolk, that is my fortune begins to a happy death.
2024-09-10 09:00:24 
2024-09-10 09:00:24 BUCKINGHAM:
2024-09-10 09:00:24 I will not stay with thee, my lord; but looks in them,
2024-09-10 09:00:24 To herself containing that do move her brothers;
2024-09-10 09:00:24 But I confess I to come as to this manner.
2024-09-10 09:00:24 
2024-09-10 09:00:24 DUCHESS OF YORK:
2024-09-10 09:00:24 When it was wont to do the young prince,
2024-09-10 09:00:24 Though yet he had not so been in his beard,
2024-09-10 09:00:24 He would have shed me in his living, 
2024-09-10 09:00:24 Inference time: 1.96 seconds
2024-09-10 09:00:24 Tokens per second: 255.59
2024-09-10 09:00:24 ---------------
2024-09-10 09:00:26 Sample 7:
2024-09-10 09:00:26  with a man be absolute a creature; and
2024-09-10 09:00:26 therein your high will, I would not buy a discourse with
2024-09-10 09:00:26 him.
2024-09-10 09:00:26 
2024-09-10 09:00:26 Clown:
2024-09-10 09:00:26 What have you been a man but a man to be a very toad. He
2024-09-10 09:00:26 shall have a fool, sir. What's your name?
2024-09-10 09:00:26 
2024-09-10 09:00:26 AUTOLYCUS:
2024-09-10 09:00:26 I say, but how I am a tall fellow?
2024-09-10 09:00:26 
2024-09-10 09:00:26 Clown:
2024-09-10 09:00:26 Nay, sit was a flatterer, boy.
2024-09-10 09:00:26 
2024-09-10 09:00:26 AUTOLYCUS:
2024-09-10 09:00:26 If, then, what sayest thou?
2024-09-10 09:00:26 
2024-09-10 09:00:26 Clown:
2024-09-10 09:00:26 O honest sir, how is it your father?
2024-09-10 09:00:26 
2024-09-10 09:00:26 AUTOLYCUS:
2024-09-10 09:00:26 Now, sir, how shall this man?
2024-09-10 09:00:26 
2024-09-10 09:00:26 Clown:
2024-09-10 09:00:26 Look, here comes the friar; let's stay alone.
2024-09-10 09:00:26 
2024-09-10 09:00:26 Shepherd:
2024-09-10 09:00:26 How do you say
2024-09-10 09:00:26 Inference time: 2.07 seconds
2024-09-10 09:00:26 Tokens per second: 241.96
2024-09-10 09:00:26 ---------------
2024-09-10 09:00:28 Sample 8:
2024-09-10 09:00:28  do it know
2024-09-10 09:00:28 The fair daughter of her love or one.
2024-09-10 09:00:28 
2024-09-10 09:00:28 LEONTES:
2024-09-10 09:00:28 She is within my speech.
2024-09-10 09:00:28 
2024-09-10 09:00:28 PAULINA:
2024-09-10 09:00:28 Well, sir, no more.
2024-09-10 09:00:28 
2024-09-10 09:00:28 LEONTES:
2024-09-10 09:00:28 Thou wilt be more of good light: thou dost, good
2024-09-10 09:00:28 That might have spent of your company.
2024-09-10 09:00:28 
2024-09-10 09:00:28 PAULINA:
2024-09-10 09:00:28 Despair not, my lord;
2024-09-10 09:00:28 I love him not for any other, but that
2024-09-10 09:00:28 I knew not what did not proper arm him, but he
2024-09-10 09:00:28 Does to put an other to death.
2024-09-10 09:00:28 
2024-09-10 09:00:28 PAULINA:
2024-09-10 09:00:28 I would thought there were to have thee, though thou
2024-09-10 09:00:28 Overheard the stronger of court, where then the gods
2024-09-10 09:00:28 Ride in earth.
2024-09-10 09:00:28 
2024-09-10 09:00:28 LEONTES
2024-09-10 09:00:28 Inference time: 2.03 seconds
2024-09-10 09:00:28 Tokens per second: 246.62
2024-09-10 09:00:28 ---------------
2024-09-10 09:00:30 Sample 9:
2024-09-10 09:00:30  slanderous lips,
2024-09-10 09:00:30 Or some poor little air wings may show for me
2024-09-10 09:00:30 The compass of friend with lips and parts
2024-09-10 09:00:30 Make with all obstinence for the stocks
2024-09-10 09:00:30 And part in murdering still work out:
2024-09-10 09:00:30 Great All-Smiles, and in lament may speak.--
2024-09-10 09:00:30 For the proudest day servants his head, make him up
2024-09-10 09:00:30 Her hatred, and tear her visiting his strange,
2024-09-10 09:00:30 Who, might hang butter'd by his power
2024-09-10 09:00:30 Which he hath here and discovered himself,
2024-09-10 09:00:30 Which we have made to demand.
2024-09-10 09:00:30 
2024-09-10 09:00:30 SICINIUS:
2024-09-10 09:00:30 Indeed, the capital service
2024-09-10 09:00:30 Doth hold on the Capito
2024-09-10 09:00:30 Inference time: 2.00 seconds
2024-09-10 09:00:30 Tokens per second: 250.57
2024-09-10 09:00:30 ---------------
2024-09-10 09:00:32 Sample 10:
2024-09-10 09:00:32  makes me with enrich one or sorrow:
2024-09-10 09:00:32 That once more, for thou hear me shine own.
2024-09-10 09:00:32 
2024-09-10 09:00:32 QUEEN MARGARET:
2024-09-10 09:00:32 Would I were so far too! I have learn'd on me;
2024-09-10 09:00:32 But let I see thy woes were soft like thousands,
2024-09-10 09:00:32 In that what should be punish'd with my tears.
2024-09-10 09:00:32 So, here comes my Edward, and thy dear love!
2024-09-10 09:00:32 
2024-09-10 09:00:32 GLOUCESTER:
2024-09-10 09:00:32 Sweet prince, o'er lambs of the time craves,
2024-09-10 09:00:32 And I will fight in Padua here.
2024-09-10 09:00:32 
2024-09-10 09:00:32 QUEEN MARGARET:
2024-09-10 09:00:32 O, here comes the noble lord of York;
2024-09-10 09:00:32 The part bethinks there were two sugger shades,
2024-09-10 09:00:32 That look upon, which 
2024-09-10 09:00:32 Inference time: 2.17 seconds
2024-09-10 09:00:32 Tokens per second: 230.80
2024-09-10 09:00:32 ---------------
2024-09-10 09:00:32 Average tokens per second: 242.46
2024-09-10 09:00:32 tokens per iteration will be: 16,384
2024-09-10 09:00:32 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 09:00:32 Initializing a new model from scratch
2024-09-10 09:00:32 number of parameters: 10.65M
2024-09-10 09:00:32 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 09:00:32 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 09:00:32 using fused AdamW: True
2024-09-10 09:00:32 compiling the model... (takes a ~minute)
2024-09-10 09:00:47 step 0: train loss 4.2372, val loss 4.2295
2024-09-10 09:00:56 iter 0: loss 4.2337, time 23207.56ms
2024-09-10 09:00:57 iter 10: loss 3.2253, time 77.29ms
2024-09-10 09:00:57 iter 20: loss 2.7690, time 79.66ms
2024-09-10 09:00:58 iter 30: loss 2.6201, time 78.81ms
2024-09-10 09:00:59 iter 40: loss 2.5386, time 79.15ms
2024-09-10 09:01:00 iter 50: loss 2.5373, time 79.54ms
2024-09-10 09:01:01 iter 60: loss 2.4993, time 78.41ms
2024-09-10 09:01:01 iter 70: loss 2.4940, time 79.73ms
2024-09-10 09:01:02 iter 80: loss 2.4951, time 79.60ms
2024-09-10 09:01:03 iter 90: loss 2.4797, time 79.38ms
2024-09-10 09:01:04 iter 100: loss 2.4710, time 81.35ms
2024-09-10 09:01:05 iter 110: loss 2.4349, time 79.29ms
2024-09-10 09:01:05 iter 120: loss 2.4511, time 79.64ms
2024-09-10 09:01:06 iter 130: loss 2.4280, time 79.63ms
2024-09-10 09:01:07 iter 140: loss 2.4239, time 80.46ms
2024-09-10 09:01:08 iter 150: loss 2.3581, time 79.53ms
2024-09-10 09:01:09 iter 160: loss 2.3831, time 80.06ms
2024-09-10 09:01:09 iter 170: loss 2.3348, time 79.53ms
2024-09-10 09:01:10 iter 180: loss 2.3208, time 80.05ms
2024-09-10 09:01:11 iter 190: loss 2.2745, time 79.77ms
2024-09-10 09:01:12 iter 200: loss 2.2435, time 79.45ms
2024-09-10 09:01:13 iter 210: loss 2.1682, time 79.53ms
2024-09-10 09:01:13 iter 220: loss 2.1203, time 79.38ms
2024-09-10 09:01:14 iter 230: loss 2.0868, time 79.45ms
2024-09-10 09:01:15 iter 240: loss 2.0645, time 79.19ms
2024-09-10 09:01:26 step 250: train loss 1.9798, val loss 2.0763
2024-09-10 09:01:26 iter 250: loss 2.0295, time 10345.23ms
2024-09-10 09:01:27 iter 260: loss 2.0106, time 80.49ms
2024-09-10 09:01:28 iter 270: loss 1.9528, time 80.37ms
2024-09-10 09:01:28 iter 280: loss 1.9459, time 80.29ms
2024-09-10 09:01:29 iter 290: loss 1.9136, time 80.33ms
2024-09-10 09:01:30 iter 300: loss 1.9053, time 80.33ms
2024-09-10 09:01:31 iter 310: loss 1.9029, time 80.61ms
2024-09-10 09:01:32 iter 320: loss 1.8563, time 80.17ms
2024-09-10 09:01:32 iter 330: loss 1.8628, time 80.80ms
2024-09-10 09:01:33 iter 340: loss 1.7898, time 80.39ms
2024-09-10 09:01:34 iter 350: loss 1.7859, time 80.66ms
2024-09-10 09:01:35 iter 360: loss 1.7920, time 79.29ms
2024-09-10 09:01:36 iter 370: loss 1.7833, time 80.44ms
2024-09-10 09:01:36 iter 380: loss 1.7417, time 80.81ms
2024-09-10 09:01:37 iter 390: loss 1.7395, time 80.19ms
2024-09-10 09:01:38 iter 400: loss 1.7086, time 80.81ms
2024-09-10 09:01:39 iter 410: loss 1.7075, time 80.15ms
2024-09-10 09:01:40 iter 420: loss 1.7106, time 80.49ms
2024-09-10 09:01:40 iter 430: loss 1.6862, time 80.82ms
2024-09-10 09:01:41 iter 440: loss 1.6635, time 80.25ms
2024-09-10 09:01:42 iter 450: loss 1.6408, time 80.47ms
2024-09-10 09:01:43 iter 460: loss 1.6860, time 80.67ms
2024-09-10 09:01:44 iter 470: loss 1.6460, time 80.51ms
2024-09-10 09:01:45 iter 480: loss 1.6512, time 81.16ms
2024-09-10 09:01:45 iter 490: loss 1.6066, time 80.99ms
2024-09-10 09:01:56 step 500: train loss 1.5399, val loss 1.7382
2024-09-10 09:01:56 iter 500: loss 1.6114, time 10423.27ms
2024-09-10 09:01:57 iter 510: loss 1.6046, time 82.25ms
2024-09-10 09:01:58 iter 520: loss 1.5759, time 81.12ms
2024-09-10 09:01:59 iter 530: loss 1.6006, time 80.10ms
2024-09-10 09:02:00 iter 540: loss 1.5560, time 81.26ms
2024-09-10 09:02:00 iter 550: loss 1.5516, time 81.45ms
2024-09-10 09:02:01 iter 560: loss 1.5594, time 81.03ms
2024-09-10 09:02:02 iter 570: loss 1.5711, time 81.95ms
2024-09-10 09:02:03 iter 580: loss 1.5121, time 82.33ms
2024-09-10 09:02:04 iter 590: loss 1.5310, time 81.62ms
2024-09-10 09:02:04 iter 600: loss 1.4987, time 81.80ms
2024-09-10 09:02:05 iter 610: loss 1.5355, time 81.30ms
2024-09-10 09:02:06 iter 620: loss 1.4913, time 81.24ms
2024-09-10 09:02:07 iter 630: loss 1.5459, time 80.94ms
2024-09-10 09:02:08 iter 640: loss 1.4688, time 80.79ms
2024-09-10 09:02:09 iter 650: loss 1.4911, time 82.35ms
2024-09-10 09:02:09 iter 660: loss 1.4869, time 83.07ms
2024-09-10 09:02:10 iter 670: loss 1.4875, time 81.37ms
2024-09-10 09:02:11 iter 680: loss 1.4490, time 82.77ms
2024-09-10 09:02:12 iter 690: loss 1.4629, time 81.24ms
2024-09-10 09:02:13 iter 700: loss 1.4599, time 81.24ms
2024-09-10 09:02:13 iter 710: loss 1.4836, time 80.85ms
2024-09-10 09:02:14 iter 720: loss 1.4444, time 81.79ms
2024-09-10 09:02:15 iter 730: loss 1.4509, time 81.46ms
2024-09-10 09:02:16 iter 740: loss 1.4351, time 81.62ms
2024-09-10 09:02:27 step 750: train loss 1.3645, val loss 1.5986
2024-09-10 09:02:27 iter 750: loss 1.4447, time 10488.67ms
2024-09-10 09:02:28 iter 760: loss 1.4078, time 81.65ms
2024-09-10 09:02:28 iter 770: loss 1.4155, time 81.16ms
2024-09-10 09:02:29 iter 780: loss 1.4294, time 80.02ms
2024-09-10 09:02:30 iter 790: loss 1.3891, time 81.52ms
2024-09-10 09:02:31 iter 800: loss 1.4146, time 81.16ms
2024-09-10 09:02:32 iter 810: loss 1.4287, time 81.49ms
2024-09-10 09:02:33 iter 820: loss 1.4086, time 81.39ms
2024-09-10 09:02:33 iter 830: loss 1.4004, time 81.31ms
2024-09-10 09:02:34 iter 840: loss 1.3962, time 81.37ms
2024-09-10 09:02:35 iter 850: loss 1.4218, time 81.47ms
2024-09-10 09:02:36 iter 860: loss 1.3699, time 81.50ms
2024-09-10 09:02:37 iter 870: loss 1.4014, time 81.23ms
2024-09-10 09:02:38 iter 880: loss 1.3627, time 82.28ms
2024-09-10 09:02:38 iter 890: loss 1.3736, time 81.31ms
2024-09-10 09:02:39 iter 900: loss 1.3565, time 81.27ms
2024-09-10 09:02:40 iter 910: loss 1.3995, time 82.53ms
2024-09-10 09:02:41 iter 920: loss 1.3743, time 82.74ms
2024-09-10 09:02:42 iter 930: loss 1.3585, time 81.58ms
2024-09-10 09:02:42 iter 940: loss 1.3704, time 81.72ms
2024-09-10 09:02:43 iter 950: loss 1.3501, time 81.52ms
2024-09-10 09:02:44 iter 960: loss 1.3858, time 81.48ms
2024-09-10 09:02:45 iter 970: loss 1.3737, time 81.55ms
2024-09-10 09:02:46 iter 980: loss 1.3707, time 81.67ms
2024-09-10 09:02:47 iter 990: loss 1.3098, time 81.44ms
2024-09-10 09:02:57 step 1000: train loss 1.2731, val loss 1.5156
2024-09-10 09:02:58 iter 1000: loss 1.3429, time 10514.11ms
2024-09-10 09:02:58 iter 1010: loss 1.3625, time 82.26ms
2024-09-10 09:02:59 iter 1020: loss 1.3523, time 81.94ms
2024-09-10 09:03:00 iter 1030: loss 1.3240, time 81.78ms
2024-09-10 09:03:01 iter 1040: loss 1.2989, time 81.83ms
2024-09-10 09:03:02 iter 1050: loss 1.2945, time 81.79ms
2024-09-10 09:03:02 iter 1060: loss 1.3083, time 81.73ms
2024-09-10 09:03:03 iter 1070: loss 1.3249, time 81.16ms
2024-09-10 09:03:04 iter 1080: loss 1.3269, time 81.91ms
2024-09-10 09:03:05 iter 1090: loss 1.3613, time 82.43ms
2024-09-10 09:03:06 iter 1100: loss 1.2908, time 82.01ms
2024-09-10 09:03:07 iter 1110: loss 1.3148, time 81.90ms
2024-09-10 09:03:07 iter 1120: loss 1.2979, time 82.34ms
2024-09-10 09:03:08 iter 1130: loss 1.3076, time 82.73ms
2024-09-10 09:03:09 iter 1140: loss 1.2941, time 82.17ms
2024-09-10 09:03:10 iter 1150: loss 1.2859, time 82.30ms
2024-09-10 09:03:11 iter 1160: loss 1.2795, time 83.85ms
2024-09-10 09:03:12 iter 1170: loss 1.2795, time 82.52ms
2024-09-10 09:03:12 iter 1180: loss 1.2793, time 81.59ms
2024-09-10 09:03:13 iter 1190: loss 1.3292, time 81.95ms
2024-09-10 09:03:14 iter 1200: loss 1.2958, time 81.29ms
2024-09-10 09:03:15 iter 1210: loss 1.2991, time 82.49ms
2024-09-10 09:03:16 iter 1220: loss 1.2898, time 82.73ms
2024-09-10 09:03:16 iter 1230: loss 1.2523, time 82.51ms
2024-09-10 09:03:17 iter 1240: loss 1.2711, time 81.23ms
2024-09-10 09:03:28 step 1250: train loss 1.2110, val loss 1.5015
2024-09-10 09:03:28 iter 1250: loss 1.3036, time 10542.13ms
2024-09-10 09:03:29 iter 1260: loss 1.2959, time 82.07ms
2024-09-10 09:03:30 iter 1270: loss 1.2947, time 82.57ms
2024-09-10 09:03:31 iter 1280: loss 1.2696, time 82.42ms
2024-09-10 09:03:32 iter 1290: loss 1.2951, time 82.47ms
2024-09-10 09:03:32 iter 1300: loss 1.2972, time 81.32ms
2024-09-10 09:03:33 iter 1310: loss 1.2466, time 81.65ms
2024-09-10 09:03:34 iter 1320: loss 1.2845, time 82.10ms
2024-09-10 09:03:35 iter 1330: loss 1.2707, time 82.41ms
2024-09-10 09:03:36 iter 1340: loss 1.2437, time 81.73ms
2024-09-10 09:03:37 iter 1350: loss 1.2474, time 82.49ms
2024-09-10 09:03:37 iter 1360: loss 1.2929, time 81.67ms
2024-09-10 09:03:38 iter 1370: loss 1.2675, time 82.43ms
2024-09-10 09:03:39 iter 1380: loss 1.2534, time 82.00ms
2024-09-10 09:03:40 iter 1390: loss 1.2654, time 81.79ms
2024-09-10 09:03:41 iter 1400: loss 1.2596, time 82.58ms
2024-09-10 09:03:42 iter 1410: loss 1.2655, time 82.47ms
2024-09-10 09:03:42 iter 1420: loss 1.2375, time 81.45ms
2024-09-10 09:03:43 iter 1430: loss 1.2297, time 82.36ms
2024-09-10 09:03:44 iter 1440: loss 1.2157, time 81.96ms
2024-09-10 09:03:45 iter 1450: loss 1.2688, time 81.39ms
2024-09-10 09:03:46 iter 1460: loss 1.2399, time 81.97ms
2024-09-10 09:03:46 iter 1470: loss 1.2200, time 82.91ms
2024-09-10 09:03:47 iter 1480: loss 1.2142, time 82.08ms
2024-09-10 09:03:48 iter 1490: loss 1.2516, time 81.99ms
2024-09-10 09:03:59 step 1500: train loss 1.1520, val loss 1.4825
2024-09-10 09:03:59 iter 1500: loss 1.2477, time 10545.98ms
2024-09-10 09:04:00 iter 1510: loss 1.2335, time 81.04ms
2024-09-10 09:04:01 iter 1520: loss 1.2358, time 81.97ms
2024-09-10 09:04:02 iter 1530: loss 1.2184, time 81.64ms
2024-09-10 09:04:02 iter 1540: loss 1.2445, time 81.29ms
2024-09-10 09:04:03 iter 1550: loss 1.2269, time 82.88ms
2024-09-10 09:04:04 iter 1560: loss 1.2510, time 81.72ms
2024-09-10 09:04:05 iter 1570: loss 1.2018, time 83.27ms
2024-09-10 09:04:06 iter 1580: loss 1.1962, time 82.08ms
2024-09-10 09:04:07 iter 1590: loss 1.2002, time 81.15ms
2024-09-10 09:04:07 iter 1600: loss 1.2175, time 81.66ms
2024-09-10 09:04:08 iter 1610: loss 1.1849, time 82.04ms
2024-09-10 09:04:09 iter 1620: loss 1.2211, time 82.51ms
2024-09-10 09:04:10 iter 1630: loss 1.2341, time 82.36ms
2024-09-10 09:04:11 iter 1640: loss 1.2276, time 82.28ms
2024-09-10 09:04:12 iter 1650: loss 1.1981, time 81.96ms
2024-09-10 09:04:12 iter 1660: loss 1.1973, time 81.68ms
2024-09-10 09:04:13 iter 1670: loss 1.2231, time 81.63ms
2024-09-10 09:04:14 iter 1680: loss 1.1740, time 81.66ms
2024-09-10 09:04:15 iter 1690: loss 1.1932, time 82.41ms
2024-09-10 09:04:16 iter 1700: loss 1.1823, time 81.97ms
2024-09-10 09:04:16 iter 1710: loss 1.1507, time 82.13ms
2024-09-10 09:04:17 iter 1720: loss 1.1699, time 81.92ms
2024-09-10 09:04:18 iter 1730: loss 1.1988, time 82.85ms
2024-09-10 09:04:19 iter 1740: loss 1.1818, time 82.15ms
2024-09-10 09:04:30 step 1750: train loss 1.1060, val loss 1.4880
2024-09-10 09:04:30 iter 1750: loss 1.1941, time 10546.99ms
2024-09-10 09:04:31 iter 1760: loss 1.2019, time 82.35ms
2024-09-10 09:04:32 iter 1770: loss 1.1968, time 81.73ms
2024-09-10 09:04:32 iter 1780: loss 1.1415, time 80.84ms
2024-09-10 09:04:33 iter 1790: loss 1.1870, time 82.32ms
2024-09-10 09:04:34 iter 1800: loss 1.1470, time 82.72ms
2024-09-10 09:04:35 iter 1810: loss 1.1724, time 82.22ms
2024-09-10 09:04:36 iter 1820: loss 1.1718, time 82.48ms
2024-09-10 09:04:37 iter 1830: loss 1.1904, time 82.30ms
2024-09-10 09:04:37 iter 1840: loss 1.1669, time 81.79ms
2024-09-10 09:04:38 iter 1850: loss 1.1874, time 81.78ms
2024-09-10 09:04:39 iter 1860: loss 1.2259, time 81.96ms
2024-09-10 09:04:40 iter 1870: loss 1.1788, time 82.83ms
2024-09-10 09:04:41 iter 1880: loss 1.1621, time 82.29ms
2024-09-10 09:04:42 iter 1890: loss 1.1583, time 81.84ms
2024-09-10 09:04:42 iter 1900: loss 1.1549, time 81.79ms
2024-09-10 09:04:43 iter 1910: loss 1.1636, time 81.83ms
2024-09-10 09:04:44 iter 1920: loss 1.1482, time 82.33ms
2024-09-10 09:04:45 iter 1930: loss 1.1745, time 82.20ms
2024-09-10 09:04:46 iter 1940: loss 1.1496, time 82.57ms
2024-09-10 09:04:46 iter 1950: loss 1.1300, time 81.38ms
2024-09-10 09:04:47 iter 1960: loss 1.1571, time 82.04ms
2024-09-10 09:04:48 iter 1970: loss 1.1399, time 81.93ms
2024-09-10 09:04:49 iter 1980: loss 1.1979, time 81.84ms
2024-09-10 09:04:50 iter 1990: loss 1.1419, time 82.28ms
2024-09-10 09:05:01 step 2000: train loss 1.0602, val loss 1.4847
2024-09-10 09:05:01 iter 2000: loss 1.1340, time 10551.33ms
2024-09-10 09:05:02 iter 2010: loss 1.1430, time 81.66ms
2024-09-10 09:05:02 iter 2020: loss 1.1242, time 82.87ms
2024-09-10 09:05:03 iter 2030: loss 1.1421, time 82.34ms
2024-09-10 09:05:04 iter 2040: loss 1.1374, time 82.08ms
2024-09-10 09:05:05 iter 2050: loss 1.1229, time 83.92ms
2024-09-10 09:05:06 iter 2060: loss 1.1134, time 82.57ms
2024-09-10 09:05:07 iter 2070: loss 1.1400, time 82.01ms
2024-09-10 09:05:07 iter 2080: loss 1.1182, time 82.16ms
2024-09-10 09:05:08 iter 2090: loss 1.1292, time 82.51ms
2024-09-10 09:05:09 iter 2100: loss 1.1427, time 82.20ms
2024-09-10 09:05:10 iter 2110: loss 1.1411, time 82.38ms
2024-09-10 09:05:11 iter 2120: loss 1.1372, time 81.40ms
2024-09-10 09:05:12 iter 2130: loss 1.1411, time 82.30ms
2024-09-10 09:05:12 iter 2140: loss 1.1092, time 83.34ms
2024-09-10 09:05:13 iter 2150: loss 1.1084, time 82.78ms
2024-09-10 09:05:14 iter 2160: loss 1.1030, time 82.34ms
2024-09-10 09:05:15 iter 2170: loss 1.1203, time 82.24ms
2024-09-10 09:05:16 iter 2180: loss 1.1201, time 81.65ms
2024-09-10 09:05:17 iter 2190: loss 1.1579, time 82.61ms
2024-09-10 09:05:17 iter 2200: loss 1.1409, time 83.09ms
2024-09-10 09:05:18 iter 2210: loss 1.1273, time 81.83ms
2024-09-10 09:05:19 iter 2220: loss 1.1255, time 82.59ms
2024-09-10 09:05:20 iter 2230: loss 1.0958, time 82.06ms
2024-09-10 09:05:21 iter 2240: loss 1.1191, time 82.02ms
2024-09-10 09:05:32 step 2250: train loss 1.0083, val loss 1.4894
2024-09-10 09:05:32 iter 2250: loss 1.1268, time 10556.20ms
2024-09-10 09:05:33 iter 2260: loss 1.1108, time 82.39ms
2024-09-10 09:05:33 iter 2270: loss 1.0966, time 82.49ms
2024-09-10 09:05:34 iter 2280: loss 1.0842, time 82.43ms
2024-09-10 09:05:35 iter 2290: loss 1.0868, time 82.85ms
2024-09-10 09:05:36 iter 2300: loss 1.1317, time 81.98ms
2024-09-10 09:05:37 iter 2310: loss 1.0914, time 82.60ms
2024-09-10 09:05:38 iter 2320: loss 1.0865, time 82.12ms
2024-09-10 09:05:38 iter 2330: loss 1.0927, time 82.37ms
2024-09-10 09:05:39 iter 2340: loss 1.0955, time 82.39ms
2024-09-10 09:05:40 iter 2350: loss 1.0773, time 82.11ms
2024-09-10 09:05:41 iter 2360: loss 1.1009, time 82.25ms
2024-09-10 09:05:42 iter 2370: loss 1.0958, time 81.42ms
2024-09-10 09:05:42 iter 2380: loss 1.0939, time 82.49ms
2024-09-10 09:05:43 iter 2390: loss 1.0876, time 81.94ms
2024-09-10 09:05:44 iter 2400: loss 1.0500, time 82.64ms
2024-09-10 09:05:45 iter 2410: loss 1.0975, time 81.71ms
2024-09-10 09:05:46 iter 2420: loss 1.0665, time 82.43ms
2024-09-10 09:05:47 iter 2430: loss 1.0576, time 82.00ms
2024-09-10 09:05:47 iter 2440: loss 1.0432, time 82.68ms
2024-09-10 09:05:48 iter 2450: loss 1.0754, time 81.95ms
2024-09-10 09:05:49 iter 2460: loss 1.0945, time 82.45ms
2024-09-10 09:05:50 iter 2470: loss 1.0803, time 82.06ms
2024-09-10 09:05:51 iter 2480: loss 1.0582, time 82.19ms
2024-09-10 09:05:52 iter 2490: loss 1.0657, time 82.44ms
2024-09-10 09:06:03 step 2500: train loss 0.9582, val loss 1.4994
2024-09-10 09:06:03 iter 2500: loss 1.0689, time 10552.77ms
2024-09-10 09:06:03 iter 2510: loss 1.0511, time 82.45ms
2024-09-10 09:06:04 iter 2520: loss 1.0653, time 81.96ms
2024-09-10 09:06:05 iter 2530: loss 1.0765, time 82.53ms
2024-09-10 09:06:06 iter 2540: loss 1.0515, time 81.28ms
2024-09-10 09:06:07 iter 2550: loss 1.0712, time 82.94ms
2024-09-10 09:06:08 iter 2560: loss 1.0548, time 82.41ms
2024-09-10 09:06:08 iter 2570: loss 1.0500, time 81.63ms
2024-09-10 09:06:09 iter 2580: loss 1.0522, time 82.52ms
2024-09-10 09:06:10 iter 2590: loss 1.0648, time 83.06ms
2024-09-10 09:06:11 iter 2600: loss 1.0624, time 81.37ms
2024-09-10 09:06:12 iter 2610: loss 1.0349, time 82.21ms
2024-09-10 09:06:13 iter 2620: loss 1.0466, time 82.17ms
2024-09-10 09:06:13 iter 2630: loss 1.0610, time 82.21ms
2024-09-10 09:06:14 iter 2640: loss 1.0184, time 82.22ms
2024-09-10 09:06:15 iter 2650: loss 1.0352, time 82.97ms
2024-09-10 09:06:16 iter 2660: loss 1.0499, time 82.82ms
2024-09-10 09:06:17 iter 2670: loss 1.0430, time 82.15ms
2024-09-10 09:06:17 iter 2680: loss 1.0365, time 82.27ms
2024-09-10 09:06:18 iter 2690: loss 1.0558, time 82.19ms
2024-09-10 09:06:19 iter 2700: loss 1.0099, time 82.60ms
2024-09-10 09:06:20 iter 2710: loss 1.0310, time 82.45ms
2024-09-10 09:06:21 iter 2720: loss 0.9935, time 82.30ms
2024-09-10 09:06:22 iter 2730: loss 1.0366, time 82.94ms
2024-09-10 09:06:22 iter 2740: loss 1.0341, time 81.80ms
2024-09-10 09:06:33 step 2750: train loss 0.9123, val loss 1.5253
2024-09-10 09:06:33 iter 2750: loss 1.0489, time 10547.72ms
2024-09-10 09:06:34 iter 2760: loss 1.0118, time 82.20ms
2024-09-10 09:06:35 iter 2770: loss 1.0426, time 81.96ms
2024-09-10 09:06:36 iter 2780: loss 1.0413, time 82.47ms
2024-09-10 09:06:37 iter 2790: loss 1.0216, time 82.07ms
2024-09-10 09:06:38 iter 2800: loss 1.0170, time 82.04ms
2024-09-10 09:06:38 iter 2810: loss 1.0167, time 82.53ms
2024-09-10 09:06:39 iter 2820: loss 1.0365, time 82.44ms
2024-09-10 09:06:40 iter 2830: loss 0.9811, time 82.37ms
2024-09-10 09:06:41 iter 2840: loss 1.0055, time 82.82ms
2024-09-10 09:06:42 iter 2850: loss 1.0102, time 82.36ms
2024-09-10 09:06:43 iter 2860: loss 0.9980, time 82.78ms
2024-09-10 09:06:43 iter 2870: loss 1.0282, time 82.77ms
2024-09-10 09:06:44 iter 2880: loss 1.0278, time 82.08ms
2024-09-10 09:06:45 iter 2890: loss 1.0128, time 82.01ms
2024-09-10 09:06:46 iter 2900: loss 1.0342, time 81.95ms
2024-09-10 09:06:47 iter 2910: loss 1.0185, time 81.57ms
2024-09-10 09:06:48 iter 2920: loss 0.9817, time 81.61ms
2024-09-10 09:06:48 iter 2930: loss 1.0045, time 82.16ms
2024-09-10 09:06:49 iter 2940: loss 1.0002, time 81.68ms
2024-09-10 09:06:50 iter 2950: loss 0.9953, time 82.34ms
2024-09-10 09:06:51 iter 2960: loss 1.0043, time 81.72ms
2024-09-10 09:06:52 iter 2970: loss 0.9957, time 82.41ms
2024-09-10 09:06:52 iter 2980: loss 1.0123, time 81.83ms
2024-09-10 09:06:53 iter 2990: loss 1.0002, time 82.60ms
2024-09-10 09:07:04 step 3000: train loss 0.8673, val loss 1.5459
2024-09-10 09:07:04 iter 3000: loss 0.9929, time 10539.11ms
2024-09-10 09:07:05 iter 3010: loss 1.0104, time 82.42ms
2024-09-10 09:07:06 iter 3020: loss 1.0144, time 81.50ms
2024-09-10 09:07:07 iter 3030: loss 1.0026, time 82.23ms
2024-09-10 09:07:08 iter 3040: loss 0.9882, time 81.96ms
2024-09-10 09:07:08 iter 3050: loss 0.9883, time 82.55ms
2024-09-10 09:07:09 iter 3060: loss 0.9935, time 82.40ms
2024-09-10 09:07:10 iter 3070: loss 0.9949, time 82.05ms
2024-09-10 09:07:11 iter 3080: loss 0.9934, time 81.99ms
2024-09-10 09:07:12 iter 3090: loss 0.9975, time 82.76ms
2024-09-10 09:07:13 iter 3100: loss 0.9806, time 81.82ms
2024-09-10 09:07:13 iter 3110: loss 0.9909, time 81.61ms
2024-09-10 09:07:14 iter 3120: loss 0.9941, time 82.54ms
2024-09-10 09:07:15 iter 3130: loss 0.9894, time 82.52ms
2024-09-10 09:07:16 iter 3140: loss 0.9961, time 82.12ms
2024-09-10 09:07:17 iter 3150: loss 0.9592, time 82.28ms
2024-09-10 09:07:18 iter 3160: loss 0.9790, time 81.54ms
2024-09-10 09:07:18 iter 3170: loss 0.9615, time 81.89ms
2024-09-10 09:07:19 iter 3180: loss 0.9499, time 82.75ms
2024-09-10 09:07:20 iter 3190: loss 0.9821, time 82.75ms
2024-09-10 09:07:21 iter 3200: loss 0.9883, time 82.18ms
2024-09-10 09:07:22 iter 3210: loss 1.0055, time 81.75ms
2024-09-10 09:07:23 iter 3220: loss 0.9804, time 81.80ms
2024-09-10 09:07:23 iter 3230: loss 0.9830, time 81.86ms
2024-09-10 09:07:24 iter 3240: loss 0.9502, time 82.29ms
2024-09-10 09:07:35 step 3250: train loss 0.8176, val loss 1.5705
2024-09-10 09:07:35 iter 3250: loss 0.9943, time 10534.01ms
2024-09-10 09:07:36 iter 3260: loss 0.9854, time 82.90ms
2024-09-10 09:07:37 iter 3270: loss 0.9665, time 82.30ms
2024-09-10 09:07:38 iter 3280: loss 0.9596, time 81.87ms
2024-09-10 09:07:39 iter 3290: loss 0.9606, time 81.76ms
2024-09-10 09:07:39 iter 3300: loss 0.9543, time 81.86ms
2024-09-10 09:07:40 iter 3310: loss 0.9642, time 82.23ms
2024-09-10 09:07:41 iter 3320: loss 0.9442, time 82.34ms
2024-09-10 09:07:42 iter 3330: loss 0.9602, time 81.75ms
2024-09-10 09:07:43 iter 3340: loss 0.9726, time 82.39ms
2024-09-10 09:07:43 iter 3350: loss 0.9799, time 81.94ms
2024-09-10 09:07:44 iter 3360: loss 0.9449, time 81.41ms
2024-09-10 09:07:45 iter 3370: loss 0.9375, time 81.31ms
2024-09-10 09:07:46 iter 3380: loss 0.9467, time 82.04ms
2024-09-10 09:07:47 iter 3390: loss 0.9427, time 81.75ms
2024-09-10 09:07:48 iter 3400: loss 0.9499, time 82.20ms
2024-09-10 09:07:48 iter 3410: loss 0.9498, time 81.70ms
2024-09-10 09:07:49 iter 3420: loss 0.9327, time 82.23ms
2024-09-10 09:07:50 iter 3430: loss 0.9347, time 81.90ms
2024-09-10 09:07:51 iter 3440: loss 0.9319, time 82.17ms
2024-09-10 09:07:52 iter 3450: loss 0.9225, time 81.64ms
2024-09-10 09:07:53 iter 3460: loss 0.9764, time 82.61ms
2024-09-10 09:07:53 iter 3470: loss 0.9257, time 82.24ms
2024-09-10 09:07:54 iter 3480: loss 0.9376, time 82.54ms
2024-09-10 09:07:55 iter 3490: loss 0.9524, time 82.14ms
2024-09-10 09:08:06 step 3500: train loss 0.7797, val loss 1.5831
2024-09-10 09:08:06 iter 3500: loss 0.9292, time 10552.60ms
2024-09-10 09:08:07 iter 3510: loss 0.9473, time 81.92ms
2024-09-10 09:08:08 iter 3520: loss 0.9073, time 81.96ms
2024-09-10 09:08:09 iter 3530: loss 0.9196, time 83.29ms
2024-09-10 09:08:09 iter 3540: loss 0.9433, time 81.88ms
2024-09-10 09:08:10 iter 3550: loss 0.9218, time 82.06ms
2024-09-10 09:08:11 iter 3560: loss 0.9548, time 82.26ms
2024-09-10 09:08:12 iter 3570: loss 0.9254, time 82.27ms
2024-09-10 09:08:13 iter 3580: loss 0.9253, time 81.77ms
2024-09-10 09:08:13 iter 3590: loss 0.9415, time 81.94ms
2024-09-10 09:08:14 iter 3600: loss 0.9137, time 82.60ms
2024-09-10 09:08:15 iter 3610: loss 0.9241, time 82.25ms
2024-09-10 09:08:16 iter 3620: loss 0.9421, time 82.38ms
2024-09-10 09:08:17 iter 3630: loss 0.9096, time 82.02ms
2024-09-10 09:08:18 iter 3640: loss 0.9300, time 81.87ms
2024-09-10 09:08:18 iter 3650: loss 0.9330, time 82.70ms
2024-09-10 09:08:19 iter 3660: loss 0.9165, time 82.47ms
2024-09-10 09:08:20 iter 3670: loss 0.9028, time 82.16ms
2024-09-10 09:08:21 iter 3680: loss 0.8983, time 83.18ms
2024-09-10 09:08:22 iter 3690: loss 0.9058, time 82.47ms
2024-09-10 09:08:23 iter 3700: loss 0.9127, time 81.74ms
2024-09-10 09:08:23 iter 3710: loss 0.8870, time 82.17ms
2024-09-10 09:08:24 iter 3720: loss 0.9027, time 81.33ms
2024-09-10 09:08:25 iter 3730: loss 0.9100, time 82.21ms
2024-09-10 09:08:26 iter 3740: loss 0.8892, time 82.16ms
2024-09-10 09:08:37 step 3750: train loss 0.7379, val loss 1.6212
2024-09-10 09:08:37 iter 3750: loss 0.9191, time 10547.58ms
2024-09-10 09:08:38 iter 3760: loss 0.8859, time 82.27ms
2024-09-10 09:08:39 iter 3770: loss 0.8861, time 81.48ms
2024-09-10 09:08:39 iter 3780: loss 0.9116, time 82.07ms
2024-09-10 09:08:40 iter 3790: loss 0.8735, time 81.87ms
2024-09-10 09:08:41 iter 3800: loss 0.9014, time 82.16ms
2024-09-10 09:08:42 iter 3810: loss 0.9208, time 82.11ms
2024-09-10 09:08:43 iter 3820: loss 0.9183, time 82.37ms
2024-09-10 09:08:44 iter 3830: loss 0.8940, time 81.72ms
2024-09-10 09:08:44 iter 3840: loss 0.9036, time 82.48ms
2024-09-10 09:08:45 iter 3850: loss 0.9011, time 81.96ms
2024-09-10 09:08:46 iter 3860: loss 0.8755, time 82.22ms
2024-09-10 09:08:47 iter 3870: loss 0.8779, time 82.56ms
2024-09-10 09:08:48 iter 3880: loss 0.8725, time 82.11ms
2024-09-10 09:08:48 iter 3890: loss 0.8891, time 82.57ms
2024-09-10 09:08:49 iter 3900: loss 0.9284, time 81.58ms
2024-09-10 09:08:50 iter 3910: loss 0.8912, time 82.57ms
2024-09-10 09:08:51 iter 3920: loss 0.8972, time 82.14ms
2024-09-10 09:08:52 iter 3930: loss 0.8958, time 82.14ms
2024-09-10 09:08:53 iter 3940: loss 0.8958, time 81.68ms
2024-09-10 09:08:53 iter 3950: loss 0.8837, time 81.43ms
2024-09-10 09:08:54 iter 3960: loss 0.8744, time 81.32ms
2024-09-10 09:08:55 iter 3970: loss 0.8533, time 82.71ms
2024-09-10 09:08:56 iter 3980: loss 0.8965, time 82.29ms
2024-09-10 09:08:57 iter 3990: loss 0.8857, time 82.48ms
2024-09-10 09:09:08 step 4000: train loss 0.7033, val loss 1.6349
2024-09-10 09:09:08 iter 4000: loss 0.8823, time 10537.12ms
2024-09-10 09:09:09 iter 4010: loss 0.8909, time 81.97ms
2024-09-10 09:09:09 iter 4020: loss 0.8434, time 82.16ms
2024-09-10 09:09:10 iter 4030: loss 0.8894, time 82.16ms
2024-09-10 09:09:11 iter 4040: loss 0.8681, time 82.13ms
2024-09-10 09:09:12 iter 4050: loss 0.8468, time 81.49ms
2024-09-10 09:09:13 iter 4060: loss 0.8716, time 81.47ms
2024-09-10 09:09:14 iter 4070: loss 0.8840, time 82.50ms
2024-09-10 09:09:14 iter 4080: loss 0.8929, time 82.13ms
2024-09-10 09:09:15 iter 4090: loss 0.8455, time 81.92ms
2024-09-10 09:09:16 iter 4100: loss 0.8823, time 82.29ms
2024-09-10 09:09:17 iter 4110: loss 0.8890, time 81.63ms
2024-09-10 09:09:18 iter 4120: loss 0.8508, time 82.82ms
2024-09-10 09:09:19 iter 4130: loss 0.8571, time 82.81ms
2024-09-10 09:09:19 iter 4140: loss 0.8722, time 81.79ms
2024-09-10 09:09:20 iter 4150: loss 0.8510, time 82.47ms
2024-09-10 09:09:21 iter 4160: loss 0.8465, time 81.13ms
2024-09-10 09:09:22 iter 4170: loss 0.8708, time 82.73ms
2024-09-10 09:09:23 iter 4180: loss 0.8764, time 81.96ms
2024-09-10 09:09:23 iter 4190: loss 0.8585, time 83.35ms
2024-09-10 09:09:24 iter 4200: loss 0.8539, time 81.36ms
2024-09-10 09:09:25 iter 4210: loss 0.8751, time 82.12ms
2024-09-10 09:09:26 iter 4220: loss 0.8652, time 81.98ms
2024-09-10 09:09:27 iter 4230: loss 0.8636, time 83.15ms
2024-09-10 09:09:28 iter 4240: loss 0.8618, time 82.36ms
2024-09-10 09:09:39 step 4250: train loss 0.6742, val loss 1.6604
2024-09-10 09:09:39 iter 4250: loss 0.8611, time 10754.13ms
2024-09-10 09:09:40 iter 4260: loss 0.8702, time 85.64ms
2024-09-10 09:09:41 iter 4270: loss 0.8452, time 84.24ms
2024-09-10 09:09:41 iter 4280: loss 0.8736, time 85.22ms
2024-09-10 09:09:42 iter 4290: loss 0.8431, time 82.29ms
2024-09-10 09:09:43 iter 4300: loss 0.8548, time 82.09ms
2024-09-10 09:09:44 iter 4310: loss 0.8460, time 82.00ms
2024-09-10 09:09:45 iter 4320: loss 0.8599, time 86.18ms
2024-09-10 09:09:46 iter 4330: loss 0.8712, time 81.90ms
2024-09-10 09:09:46 iter 4340: loss 0.8755, time 81.88ms
2024-09-10 09:09:47 iter 4350: loss 0.8549, time 84.89ms
2024-09-10 09:09:48 iter 4360: loss 0.8430, time 83.06ms
2024-09-10 09:09:49 iter 4370: loss 0.8267, time 82.55ms
2024-09-10 09:09:50 iter 4380: loss 0.8497, time 82.13ms
2024-09-10 09:09:51 iter 4390: loss 0.8410, time 82.44ms
2024-09-10 09:09:51 iter 4400: loss 0.8280, time 82.06ms
2024-09-10 09:09:52 iter 4410: loss 0.8410, time 82.42ms
2024-09-10 09:09:53 iter 4420: loss 0.8363, time 82.14ms
2024-09-10 09:09:54 iter 4430: loss 0.8488, time 82.63ms
2024-09-10 09:09:55 iter 4440: loss 0.8308, time 82.04ms
2024-09-10 09:09:56 iter 4450: loss 0.8485, time 82.13ms
2024-09-10 09:09:56 iter 4460: loss 0.8428, time 81.98ms
2024-09-10 09:09:57 iter 4470: loss 0.8532, time 81.82ms
2024-09-10 09:09:58 iter 4480: loss 0.8481, time 82.13ms
2024-09-10 09:09:59 iter 4490: loss 0.8170, time 81.97ms
2024-09-10 09:10:10 step 4500: train loss 0.6488, val loss 1.6833
2024-09-10 09:10:10 iter 4500: loss 0.8473, time 10575.35ms
2024-09-10 09:10:11 iter 4510: loss 0.8472, time 82.53ms
2024-09-10 09:10:12 iter 4520: loss 0.8384, time 82.31ms
2024-09-10 09:10:12 iter 4530: loss 0.8389, time 82.03ms
2024-09-10 09:10:13 iter 4540: loss 0.8192, time 82.35ms
2024-09-10 09:10:14 iter 4550: loss 0.8250, time 82.91ms
2024-09-10 09:10:15 iter 4560: loss 0.8385, time 82.31ms
2024-09-10 09:10:16 iter 4570: loss 0.8303, time 82.23ms
2024-09-10 09:10:16 iter 4580: loss 0.8125, time 82.29ms
2024-09-10 09:10:17 iter 4590: loss 0.8252, time 82.49ms
2024-09-10 09:10:18 iter 4600: loss 0.8342, time 82.03ms
2024-09-10 09:10:19 iter 4610: loss 0.8294, time 81.63ms
2024-09-10 09:10:20 iter 4620: loss 0.8097, time 82.28ms
2024-09-10 09:10:21 iter 4630: loss 0.8232, time 82.37ms
2024-09-10 09:10:21 iter 4640: loss 0.8342, time 81.95ms
2024-09-10 09:10:22 iter 4650: loss 0.8304, time 82.83ms
2024-09-10 09:10:23 iter 4660: loss 0.8321, time 82.35ms
2024-09-10 09:10:24 iter 4670: loss 0.8559, time 82.84ms
2024-09-10 09:10:25 iter 4680: loss 0.8672, time 81.79ms
2024-09-10 09:10:26 iter 4690: loss 0.8296, time 81.99ms
2024-09-10 09:10:26 iter 4700: loss 0.8212, time 81.32ms
2024-09-10 09:10:27 iter 4710: loss 0.8363, time 81.55ms
2024-09-10 09:10:28 iter 4720: loss 0.8212, time 82.07ms
2024-09-10 09:10:29 iter 4730: loss 0.8307, time 81.97ms
2024-09-10 09:10:30 iter 4740: loss 0.8163, time 82.53ms
2024-09-10 09:10:41 step 4750: train loss 0.6324, val loss 1.6920
2024-09-10 09:10:41 iter 4750: loss 0.8269, time 10546.03ms
2024-09-10 09:10:42 iter 4760: loss 0.8370, time 80.83ms
2024-09-10 09:10:42 iter 4770: loss 0.8345, time 81.57ms
2024-09-10 09:10:43 iter 4780: loss 0.8192, time 82.45ms
2024-09-10 09:10:44 iter 4790: loss 0.8157, time 82.09ms
2024-09-10 09:10:45 iter 4800: loss 0.8044, time 83.65ms
2024-09-10 09:10:46 iter 4810: loss 0.8361, time 82.00ms
2024-09-10 09:10:46 iter 4820: loss 0.8527, time 81.81ms
2024-09-10 09:10:47 iter 4830: loss 0.8337, time 82.47ms
2024-09-10 09:10:48 iter 4840: loss 0.8082, time 82.00ms
2024-09-10 09:10:49 iter 4850: loss 0.7984, time 82.01ms
2024-09-10 09:10:50 iter 4860: loss 0.8134, time 82.49ms
2024-09-10 09:10:51 iter 4870: loss 0.8161, time 81.67ms
2024-09-10 09:10:51 iter 4880: loss 0.8085, time 81.78ms
2024-09-10 09:10:52 iter 4890: loss 0.8078, time 81.67ms
2024-09-10 09:10:53 iter 4900: loss 0.8064, time 82.40ms
2024-09-10 09:10:54 iter 4910: loss 0.8145, time 82.34ms
2024-09-10 09:10:55 iter 4920: loss 0.8121, time 81.72ms
2024-09-10 09:10:56 iter 4930: loss 0.8285, time 81.78ms
2024-09-10 09:10:56 iter 4940: loss 0.8348, time 82.53ms
2024-09-10 09:10:57 iter 4950: loss 0.8309, time 82.01ms
2024-09-10 09:10:58 iter 4960: loss 0.8406, time 82.01ms
2024-09-10 09:10:59 iter 4970: loss 0.7947, time 80.38ms
2024-09-10 09:11:00 iter 4980: loss 0.7895, time 83.00ms
2024-09-10 09:11:00 iter 4990: loss 0.8049, time 82.25ms
2024-09-10 09:11:11 step 5000: train loss 0.6139, val loss 1.7101
2024-09-10 09:11:12 iter 5000: loss 0.7998, time 10554.82ms
2024-09-10 09:11:12 training done
2024-09-10 09:11:12 Best validation loss: 1.4824634790420532
2024-09-10 09:11:12 Total train time: 10.65 mins
2024-09-10 09:11:12 Loading meta from ../../data/shakespeare_char/meta.pkl...
2024-09-10 09:11:14 Sample 1:
2024-09-10 09:11:14  and thoughts,
2024-09-10 09:11:14 Nor deserved in love thee to be heard,
2024-09-10 09:11:14 Or, if thou shalt be a beggar, be obey,
2024-09-10 09:11:14 Lest, having thy birth, thy temple faithful prince,
2024-09-10 09:11:14 Whereto execute, thy father, state thee from France:
2024-09-10 09:11:14 And thou resign his minds that we have heard
2024-09-10 09:11:14 The provost benefit of Henry loyal seat.
2024-09-10 09:11:14 
2024-09-10 09:11:14 YORK:
2024-09-10 09:11:14 A greater gift!
2024-09-10 09:11:14 The lords that his fearful hate will not right;
2024-09-10 09:11:14 Which buckle grows before his subjects have drown
2024-09-10 09:11:14 Which he should have the truth: his lord, and his fest:
2024-09-10 09:11:14 As if he be past, that his soul should 
2024-09-10 09:11:14 Inference time: 2.10 seconds
2024-09-10 09:11:14 Tokens per second: 238.64
2024-09-10 09:11:14 ---------------
2024-09-10 09:11:16 Sample 2:
2024-09-10 09:11:16  but that
2024-09-10 09:11:16 she shall be the short murderer, as I said, for the
2024-09-10 09:11:16 thing is a hard she as a man does not a preservant to
2024-09-10 09:11:16 pecker; has he helds, or how he did so professes to hear
2024-09-10 09:11:16 it: it must be so, that thou must be cheap of wine.
2024-09-10 09:11:16 
2024-09-10 09:11:16 TYBALT:
2024-09-10 09:11:16 It is, sir, the mayor and hear it so;
2024-09-10 09:11:16 It shall be so; I have a style mother.
2024-09-10 09:11:16 You must not so much beloved as I did:
2024-09-10 09:11:16 That you have not dream'd fast so, you love me;
2024-09-10 09:11:16 For I have a blush of your tent, beseech you
2024-09-10 09:11:16 Not still my sovereign, heaven shall bear the heart.
2024-09-10 09:11:16 
2024-09-10 09:11:16 BUCK
2024-09-10 09:11:16 Inference time: 2.10 seconds
2024-09-10 09:11:16 Tokens per second: 237.96
2024-09-10 09:11:16 ---------------
2024-09-10 09:11:18 Sample 3:
2024-09-10 09:11:18  his eye,
2024-09-10 09:11:18 Though all infant, is not fill'd with him.
2024-09-10 09:11:18 What doth the king success who forced him?
2024-09-10 09:11:18 
2024-09-10 09:11:18 GLOUCESTER:
2024-09-10 09:11:18 The Earl of Wiltshire I live on your husband.
2024-09-10 09:11:18 
2024-09-10 09:11:18 LADY ANNE:
2024-09-10 09:11:18 I thank my liege, and that you might not be.
2024-09-10 09:11:18 
2024-09-10 09:11:18 GLOUCESTER:
2024-09-10 09:11:18 If I did tell thee here as thou liest,
2024-09-10 09:11:18 Wert thou our cousin'st and behold the head,
2024-09-10 09:11:18 The midwife o'ersta'en and dangerous thoughts,
2024-09-10 09:11:18 The carrive of your caps in the sea,
2024-09-10 09:11:18 Your princess shall have a pure to help.
2024-09-10 09:11:18 
2024-09-10 09:11:18 GLOUCESTER:
2024-09-10 09:11:18 I cannot tell; but yet yet I wish them thence.
2024-09-10 09:11:18 But in 
2024-09-10 09:11:18 Inference time: 1.98 seconds
2024-09-10 09:11:18 Tokens per second: 252.31
2024-09-10 09:11:18 ---------------
2024-09-10 09:11:20 Sample 4:
2024-09-10 09:11:20  gentleman was the man; and that
2024-09-10 09:11:20 He lives to prize the commons' soldiers, which she was
2024-09-10 09:11:20 tender o' the commonwealth with her base and looks
2024-09-10 09:11:20 Than where she did content in their two soldiers:
2024-09-10 09:11:20 Somewhat, man, man, already a poor word,
2024-09-10 09:11:20 Of the world's worthy blood to blood,
2024-09-10 09:11:20 Where look on when this is she and love.
2024-09-10 09:11:20 
2024-09-10 09:11:20 Servant:
2024-09-10 09:11:20 Marry, gentle Richard, how she longs of her
2024-09-10 09:11:20 Gloucester'd my lord, that blood makes you serve,
2024-09-10 09:11:20 That robb'd my son's son should not such fair appointed:
2024-09-10 09:11:20 They could not lick on me; whic
2024-09-10 09:11:20 Inference time: 2.12 seconds
2024-09-10 09:11:20 Tokens per second: 235.36
2024-09-10 09:11:20 ---------------
2024-09-10 09:11:22 Sample 5:
2024-09-10 09:11:22  a bloody sense
2024-09-10 09:11:22 That ever envy be her long bed,
2024-09-10 09:11:22 Let me impale here the morning's life.
2024-09-10 09:11:22 
2024-09-10 09:11:22 KING EDWARD IV:
2024-09-10 09:11:22 Then, Warwick, after was it to die.
2024-09-10 09:11:22 
2024-09-10 09:11:22 GLOUCESTER:
2024-09-10 09:11:22 Then, Edward and thy Lord of Wiltshire;
2024-09-10 09:11:22 When, all I will make thee a king and peace.
2024-09-10 09:11:22 
2024-09-10 09:11:22 CLARENCE:
2024-09-10 09:11:22 Madam, I have pass'd you to your majesty.
2024-09-10 09:11:22 
2024-09-10 09:11:22 GLOUCESTER:
2024-09-10 09:11:22 My lord, with all such descent amongst you.
2024-09-10 09:11:22 
2024-09-10 09:11:22 LADY ANNE:
2024-09-10 09:11:22 What with one 'word's black talk of life?
2024-09-10 09:11:22 
2024-09-10 09:11:22 GLOUCESTER:
2024-09-10 09:11:22 Before this one that I be call'd for us?
2024-09-10 09:11:22 
2024-09-10 09:11:22 LADY ANNE:
2024-09-10 09:11:22 It is most unlucky; therefore to 
2024-09-10 09:11:22 Inference time: 2.04 seconds
2024-09-10 09:11:22 Tokens per second: 245.43
2024-09-10 09:11:22 ---------------
2024-09-10 09:11:24 Sample 6:
2024-09-10 09:11:24  death, that the gentleman shall be wife.
2024-09-10 09:11:24 
2024-09-10 09:11:24 GLOUCESTER:
2024-09-10 09:11:24 Marry, the mayor talk of comfort these wars.
2024-09-10 09:11:24 
2024-09-10 09:11:24 HASTINGS:
2024-09-10 09:11:24 Come on, we have with you; and come buy on me;
2024-09-10 09:11:24 Since the part is not for the maid of York
2024-09-10 09:11:24 Hath been bent to demand.
2024-09-10 09:11:24 
2024-09-10 09:11:24 HASTINGS:
2024-09-10 09:11:24 They are the contrary.
2024-09-10 09:11:24 
2024-09-10 09:11:24 GLOUCESTER:
2024-09-10 09:11:24 As I did, my lord.
2024-09-10 09:11:24 
2024-09-10 09:11:24 GLOUCESTER:
2024-09-10 09:11:24 The duke is slain, I will knock the crown.
2024-09-10 09:11:24 
2024-09-10 09:11:24 YORK:
2024-09-10 09:11:24 The king is not prisoner than your highness' close.
2024-09-10 09:11:24 
2024-09-10 09:11:24 JOHN OF GAUNT:
2024-09-10 09:11:24 But then I know my mind: I hold thee my usure:
2024-09-10 09:11:24 Think when thou shalt think, t
2024-09-10 09:11:24 Inference time: 2.08 seconds
2024-09-10 09:11:24 Tokens per second: 240.66
2024-09-10 09:11:24 ---------------
2024-09-10 09:11:26 Sample 7:
2024-09-10 09:11:26  desires
2024-09-10 09:11:26 To prove his native recreation: therefore, I
2024-09-10 09:11:26 Live, by my hand, for by Edward's welcome hither.
2024-09-10 09:11:26 
2024-09-10 09:11:26 KING EDWARD IV:
2024-09-10 09:11:26 Have done, my lords: the one friends for our side;
2024-09-10 09:11:26 For God's sake, lords, I'll quit alone.
2024-09-10 09:11:26 
2024-09-10 09:11:26 LADY GREY:
2024-09-10 09:11:26 What said you to make, I come to jest?
2024-09-10 09:11:26 
2024-09-10 09:11:26 KING EDWARD IV:
2024-09-10 09:11:26 What say you but such a time to cross;
2024-09-10 09:11:26 But I'll give them to crave a better down,
2024-09-10 09:11:26 For giving him to the hand that cut a strange for the
2024-09-10 09:11:26 That made your son, which was so quarrell'd with him?
2024-09-10 09:11:26 Being not a love to her uncle, 
2024-09-10 09:11:26 Inference time: 2.22 seconds
2024-09-10 09:11:26 Tokens per second: 225.26
2024-09-10 09:11:26 ---------------
2024-09-10 09:11:28 Sample 8:
2024-09-10 09:11:28  chamber-maids up their fortune's bowels
2024-09-10 09:11:28 And bid them battle, rough apply when it shall
2024-09-10 09:11:28 Shall be twice the angry that we may make not
2024-09-10 09:11:28 Relent in the death of them. The power doth require
2024-09-10 09:11:28 The gods give us there! O, that I should have lack'd down,
2024-09-10 09:11:28 On the secret beams of my fair presence,
2024-09-10 09:11:28 And stand all my sins of it.
2024-09-10 09:11:28 
2024-09-10 09:11:28 DUKE VINCENTIO:
2024-09-10 09:11:28 He is a most gracious husband.
2024-09-10 09:11:28 
2024-09-10 09:11:28 ANGELO:
2024-09-10 09:11:28 I would all the worst that I have of your father:
2024-09-10 09:11:28 So I have heard you, sir, but you shall shame
2024-09-10 09:11:28 My answer to her complaining. Sta
2024-09-10 09:11:28 Inference time: 2.07 seconds
2024-09-10 09:11:28 Tokens per second: 241.03
2024-09-10 09:11:28 ---------------
2024-09-10 09:11:30 Sample 9:
2024-09-10 09:11:30  Crosby:
2024-09-10 09:11:30 I have fought so old a subject cannot be
2024-09-10 09:11:30 More cause-denied man i' the leave of Mowbray's wars,
2024-09-10 09:11:30 And with the high, where he should be contented
2024-09-10 09:11:30 Unto a parlous conqueror, that he hath
2024-09-10 09:11:30 Been blind with the wall, but as I do not,
2024-09-10 09:11:30 Were there in being plucked by, that the beard so
2024-09-10 09:11:30 Of my brother's life before.
2024-09-10 09:11:30 
2024-09-10 09:11:30 MENENIUS:
2024-09-10 09:11:30 I am afruitful in the senate-house;
2024-09-10 09:11:30 If for he have purpose to have a Coriolanus
2024-09-10 09:11:30 In honour to his beard, his purpose, his left
2024-09-10 09:11:30 In his meed, his looks and therein me wanting like
2024-09-10 09:11:30 
2024-09-10 09:11:30 Inference time: 2.16 seconds
2024-09-10 09:11:30 Tokens per second: 231.07
2024-09-10 09:11:30 ---------------
2024-09-10 09:11:33 Sample 10:
2024-09-10 09:11:33  the next mother should be
2024-09-10 09:11:33 In murder peace with me? There is no man in charge
2024-09-10 09:11:33 That ever enters now me now, who hath done:
2024-09-10 09:11:33 Besides, the people are made good medlars,
2024-09-10 09:11:33 For I have done already to see them.
2024-09-10 09:11:33 
2024-09-10 09:11:33 BRUTUS:
2024-09-10 09:11:33 Come, good Marcius,
2024-09-10 09:11:33 You must not pass to the Capitol, there the Volsces
2024-09-10 09:11:33 Plough and holy woe asleep: he's as loath too hot
2024-09-10 09:11:33 several in the fardel of the people,
2024-09-10 09:11:33 and to the devotion of a woman's breast;
2024-09-10 09:11:33 Not the state hath been seen twine even
2024-09-10 09:11:33 To take their fancy of hopes: they are their purpo
2024-09-10 09:11:33 Inference time: 2.32 seconds
2024-09-10 09:11:33 Tokens per second: 215.28
2024-09-10 09:11:33 ---------------
2024-09-10 09:11:33 Average tokens per second: 236.30
2024-09-10 09:11:33 tokens per iteration will be: 16,384
2024-09-10 09:11:33 found vocab_size = 65 (inside ../../data/shakespeare_char/meta.pkl)
2024-09-10 09:11:33 Initializing a new model from scratch
2024-09-10 09:11:33 number of parameters: 10.65M
2024-09-10 09:11:33 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 09:11:33 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 09:11:33 using fused AdamW: True
2024-09-10 09:11:33 compiling the model... (takes a ~minute)
2024-09-10 09:11:48 step 0: train loss 4.2468, val loss 4.2417
2024-09-10 09:11:57 iter 0: loss 4.2478, time 23545.08ms
2024-09-10 09:11:58 iter 10: loss 3.2118, time 82.46ms
2024-09-10 09:11:58 iter 20: loss 2.7845, time 79.60ms
2024-09-10 09:11:59 iter 30: loss 2.6130, time 79.72ms
2024-09-10 09:12:00 iter 40: loss 2.5606, time 80.67ms
2024-09-10 09:12:01 iter 50: loss 2.5318, time 79.41ms
2024-09-10 09:12:02 iter 60: loss 2.4923, time 78.32ms
2024-09-10 09:12:02 iter 70: loss 2.4931, time 79.64ms
2024-09-10 09:12:03 iter 80: loss 2.5009, time 79.41ms
2024-09-10 09:12:04 iter 90: loss 2.4810, time 85.22ms
2024-09-10 09:12:05 iter 100: loss 2.4783, time 82.92ms
2024-09-10 09:12:06 iter 110: loss 2.4373, time 84.95ms
2024-09-10 09:12:06 iter 120: loss 2.4323, time 79.33ms
2024-09-10 09:12:07 iter 130: loss 2.4338, time 79.31ms
2024-09-10 09:12:08 iter 140: loss 2.3993, time 79.84ms
2024-09-10 09:12:09 iter 150: loss 2.3935, time 82.04ms
2024-09-10 09:12:10 iter 160: loss 2.3948, time 83.44ms
2024-09-10 09:12:10 iter 170: loss 2.3794, time 82.60ms
2024-09-10 09:12:11 iter 180: loss 2.3004, time 83.49ms
2024-09-10 09:12:12 iter 190: loss 2.2923, time 80.59ms
2024-09-10 09:12:13 iter 200: loss 2.2268, time 80.03ms
2024-09-10 09:12:14 iter 210: loss 2.1760, time 84.17ms
2024-09-10 09:12:15 iter 220: loss 2.1442, time 79.46ms
2024-09-10 09:12:15 iter 230: loss 2.0961, time 80.19ms
2024-09-10 09:12:16 iter 240: loss 2.0588, time 78.35ms
2024-09-10 09:12:27 step 250: train loss 1.9762, val loss 2.0787
2024-09-10 09:12:27 iter 250: loss 2.0447, time 10354.07ms
2024-09-10 09:12:28 iter 260: loss 2.0012, time 80.82ms
2024-09-10 09:12:29 iter 270: loss 1.9551, time 80.31ms
2024-09-10 09:12:29 iter 280: loss 1.9476, time 80.72ms
2024-09-10 09:12:30 iter 290: loss 1.9210, time 80.34ms
2024-09-10 09:12:31 iter 300: loss 1.9158, time 80.71ms
2024-09-10 09:12:32 iter 310: loss 1.8689, time 79.99ms
2024-09-10 09:12:33 iter 320: loss 1.8636, time 80.54ms
2024-09-10 09:12:33 iter 330: loss 1.8159, time 81.86ms
2024-09-10 09:12:34 iter 340: loss 1.8258, time 81.57ms
2024-09-10 09:12:35 iter 350: loss 1.8043, time 80.51ms
2024-09-10 09:12:36 iter 360: loss 1.7841, time 81.64ms
2024-09-10 09:12:37 iter 370: loss 1.7552, time 80.95ms
2024-09-10 09:12:38 iter 380: loss 1.7568, time 80.39ms
2024-09-10 09:12:38 iter 390: loss 1.7226, time 80.80ms
2024-09-10 09:12:39 iter 400: loss 1.7270, time 79.45ms
2024-09-10 09:12:40 iter 410: loss 1.7522, time 79.97ms
2024-09-10 09:12:41 iter 420: loss 1.7007, time 81.64ms
2024-09-10 09:12:42 iter 430: loss 1.6493, time 79.59ms
2024-09-10 09:12:42 iter 440: loss 1.7030, time 79.97ms
2024-09-10 09:12:43 iter 450: loss 1.6406, time 80.52ms
2024-09-10 09:12:44 iter 460: loss 1.6684, time 80.07ms
2024-09-10 09:12:45 iter 470: loss 1.6341, time 81.03ms
2024-09-10 09:12:46 iter 480: loss 1.6439, time 80.55ms
2024-09-10 09:12:46 iter 490: loss 1.6081, time 80.75ms
2024-09-10 09:12:57 step 500: train loss 1.5310, val loss 1.7224
2024-09-10 09:12:57 iter 500: loss 1.5594, time 10511.92ms
2024-09-10 09:12:58 iter 510: loss 1.6244, time 81.44ms
2024-09-10 09:12:59 iter 520: loss 1.5777, time 82.37ms
2024-09-10 09:13:00 iter 530: loss 1.5810, time 81.78ms
2024-09-10 09:13:01 iter 540: loss 1.5887, time 81.14ms
2024-09-10 09:13:02 iter 550: loss 1.5569, time 84.18ms
2024-09-10 09:13:02 iter 560: loss 1.5594, time 82.57ms
2024-09-10 09:13:03 iter 570: loss 1.5566, time 82.40ms
2024-09-10 09:13:04 iter 580: loss 1.5382, time 81.41ms
2024-09-10 09:13:05 iter 590: loss 1.5204, time 82.95ms
2024-09-10 09:13:06 iter 600: loss 1.5161, time 81.15ms
2024-09-10 09:13:06 iter 610: loss 1.5165, time 81.89ms
2024-09-10 09:13:07 iter 620: loss 1.5238, time 81.66ms
2024-09-10 09:13:08 iter 630: loss 1.5220, time 81.14ms
2024-09-10 09:13:09 iter 640: loss 1.4960, time 81.29ms
2024-09-10 09:13:10 iter 650: loss 1.5091, time 81.77ms
2024-09-10 09:13:11 iter 660: loss 1.4865, time 81.71ms
2024-09-10 09:13:11 iter 670: loss 1.4768, time 82.54ms
2024-09-10 09:13:12 iter 680: loss 1.4599, time 81.62ms
2024-09-10 09:13:13 iter 690: loss 1.4669, time 82.35ms
2024-09-10 09:13:14 iter 700: loss 1.4737, time 80.45ms
2024-09-10 09:13:15 iter 710: loss 1.4866, time 81.99ms
2024-09-10 09:13:15 iter 720: loss 1.4716, time 81.70ms
2024-09-10 09:13:16 iter 730: loss 1.4599, time 84.76ms
2024-09-10 09:13:17 iter 740: loss 1.4268, time 82.40ms
2024-09-10 09:13:28 step 750: train loss 1.3628, val loss 1.5733
2024-09-10 09:13:28 iter 750: loss 1.4377, time 10746.59ms
2024-09-10 09:13:29 iter 760: loss 1.4431, time 82.40ms
2024-09-10 09:13:30 iter 770: loss 1.4582, time 81.99ms
2024-09-10 09:13:31 iter 780: loss 1.4393, time 82.25ms
2024-09-10 09:13:32 iter 790: loss 1.4237, time 81.54ms
2024-09-10 09:13:32 iter 800: loss 1.4315, time 83.40ms
2024-09-10 09:13:33 iter 810: loss 1.4297, time 81.94ms
2024-09-10 09:13:34 iter 820: loss 1.3831, time 81.61ms
2024-09-10 09:13:35 iter 830: loss 1.3799, time 81.91ms
2024-09-10 09:13:36 iter 840: loss 1.4065, time 81.88ms
2024-09-10 09:13:37 iter 850: loss 1.3775, time 81.24ms
2024-09-10 09:13:37 iter 860: loss 1.3874, time 81.44ms
2024-09-10 09:13:38 iter 870: loss 1.3958, time 81.35ms
2024-09-10 09:13:39 iter 880: loss 1.3665, time 81.42ms
2024-09-10 09:13:40 iter 890: loss 1.3730, time 81.64ms
2024-09-10 09:13:41 iter 900: loss 1.3490, time 81.48ms
2024-09-10 09:13:42 iter 910: loss 1.3942, time 81.39ms
2024-09-10 09:13:42 iter 920: loss 1.3783, time 81.49ms
2024-09-10 09:13:43 iter 930: loss 1.3659, time 81.67ms
2024-09-10 09:13:44 iter 940: loss 1.3509, time 83.01ms
2024-09-10 09:13:45 iter 950: loss 1.3597, time 87.65ms
2024-09-10 09:13:46 iter 960: loss 1.3742, time 81.85ms
2024-09-10 09:13:46 iter 970: loss 1.3784, time 81.85ms
2024-09-10 09:13:47 iter 980: loss 1.3644, time 80.26ms
2024-09-10 09:13:48 iter 990: loss 1.3513, time 83.27ms
2024-09-10 09:13:59 step 1000: train loss 1.2729, val loss 1.5253
2024-09-10 09:13:59 iter 1000: loss 1.3278, time 10598.67ms
2024-09-10 09:14:00 iter 1010: loss 1.3516, time 82.38ms
2024-09-10 09:14:01 iter 1020: loss 1.3224, time 81.35ms
2024-09-10 09:14:02 iter 1030: loss 1.3501, time 82.17ms
2024-09-10 09:14:03 iter 1040: loss 1.3235, time 81.74ms
2024-09-10 09:14:03 iter 1050: loss 1.3327, time 84.00ms
2024-09-10 09:14:04 iter 1060: loss 1.3460, time 82.15ms
2024-09-10 09:14:05 iter 1070: loss 1.3504, time 81.79ms
2024-09-10 09:14:06 iter 1080: loss 1.3173, time 83.04ms
2024-09-10 09:14:07 iter 1090: loss 1.2674, time 83.63ms
2024-09-10 09:14:07 iter 1100: loss 1.3180, time 81.79ms
2024-09-10 09:14:08 iter 1110: loss 1.3165, time 81.88ms
2024-09-10 09:14:09 iter 1120: loss 1.3021, time 81.64ms
2024-09-10 09:14:10 iter 1130: loss 1.2952, time 82.71ms
2024-09-10 09:14:11 iter 1140: loss 1.3331, time 83.84ms
2024-09-10 09:14:12 iter 1150: loss 1.2709, time 81.65ms
2024-09-10 09:14:12 iter 1160: loss 1.3126, time 83.06ms
2024-09-10 09:14:13 iter 1170: loss 1.2868, time 81.44ms
2024-09-10 09:14:14 iter 1180: loss 1.2697, time 83.27ms
2024-09-10 09:14:15 iter 1190: loss 1.3175, time 82.48ms
2024-09-10 09:14:16 iter 1200: loss 1.2975, time 83.29ms
2024-09-10 09:14:17 iter 1210: loss 1.2860, time 82.23ms
2024-09-10 09:14:17 iter 1220: loss 1.3024, time 81.88ms
2024-09-10 09:14:18 iter 1230: loss 1.2967, time 82.84ms
2024-09-10 09:14:19 iter 1240: loss 1.3083, time 81.10ms
2024-09-10 09:14:30 step 1250: train loss 1.2015, val loss 1.4828
2024-09-10 09:14:30 iter 1250: loss 1.3005, time 10742.28ms
2024-09-10 09:14:31 iter 1260: loss 1.2922, time 83.29ms
2024-09-10 09:14:32 iter 1270: loss 1.2951, time 86.36ms
2024-09-10 09:14:33 iter 1280: loss 1.2755, time 82.38ms
2024-09-10 09:14:34 iter 1290: loss 1.2692, time 81.45ms
2024-09-10 09:14:34 iter 1300: loss 1.3028, time 82.07ms
2024-09-10 09:14:35 iter 1310: loss 1.2686, time 87.32ms
2024-09-10 09:14:36 iter 1320: loss 1.2635, time 85.70ms
2024-09-10 09:14:37 iter 1330: loss 1.2599, time 82.43ms
2024-09-10 09:14:38 iter 1340: loss 1.2717, time 81.32ms
2024-09-10 09:14:39 iter 1350: loss 1.2459, time 82.45ms
2024-09-10 09:14:39 iter 1360: loss 1.2404, time 82.39ms
2024-09-10 09:14:40 iter 1370: loss 1.2771, time 81.56ms
2024-09-10 09:14:41 iter 1380: loss 1.2365, time 81.92ms
2024-09-10 09:14:42 iter 1390: loss 1.2522, time 82.97ms
2024-09-10 09:14:43 iter 1400: loss 1.2305, time 83.60ms
2024-09-10 09:14:44 iter 1410: loss 1.2123, time 82.17ms
2024-09-10 09:14:44 iter 1420: loss 1.2641, time 82.05ms
2024-09-10 09:14:45 iter 1430: loss 1.2667, time 82.42ms
2024-09-10 09:14:46 iter 1440: loss 1.2152, time 87.57ms
2024-09-10 09:14:47 iter 1450: loss 1.2689, time 82.79ms
2024-09-10 09:14:48 iter 1460: loss 1.2592, time 81.46ms
2024-09-10 09:14:49 iter 1470: loss 1.2252, time 82.08ms
2024-09-10 09:14:49 iter 1480: loss 1.2141, time 80.53ms
2024-09-10 09:14:50 iter 1490: loss 1.2422, time 81.36ms
2024-09-10 09:15:01 step 1500: train loss 1.1485, val loss 1.4629
2024-09-10 09:15:01 iter 1500: loss 1.2040, time 10725.02ms
2024-09-10 09:15:02 iter 1510: loss 1.2327, time 83.38ms
2024-09-10 09:15:03 iter 1520: loss 1.2134, time 82.58ms
2024-09-10 09:15:04 iter 1530: loss 1.2441, time 80.88ms
2024-09-10 09:15:05 iter 1540: loss 1.2192, time 80.42ms
2024-09-10 09:15:06 iter 1550: loss 1.1706, time 81.63ms
2024-09-10 09:15:06 iter 1560: loss 1.2194, time 82.59ms
2024-09-10 09:15:07 iter 1570: loss 1.1968, time 81.89ms
2024-09-10 09:15:08 iter 1580: loss 1.2070, time 83.45ms
2024-09-10 09:15:09 iter 1590: loss 1.2447, time 83.03ms
2024-09-10 09:15:10 iter 1600: loss 1.2070, time 83.07ms
2024-09-10 09:15:11 iter 1610: loss 1.2036, time 82.34ms
2024-09-10 09:15:11 iter 1620: loss 1.2099, time 82.73ms
2024-09-10 09:15:12 iter 1630: loss 1.1986, time 82.93ms
2024-09-10 09:15:13 iter 1640: loss 1.2157, time 82.52ms
2024-09-10 09:15:14 iter 1650: loss 1.1893, time 82.67ms
2024-09-10 09:15:15 iter 1660: loss 1.2226, time 81.27ms
2024-09-10 09:15:16 iter 1670: loss 1.2216, time 82.37ms
2024-09-10 09:15:16 iter 1680: loss 1.2269, time 82.99ms
2024-09-10 09:15:17 iter 1690: loss 1.2230, time 82.36ms
2024-09-10 09:15:18 iter 1700: loss 1.2222, time 83.16ms
2024-09-10 09:15:19 iter 1710: loss 1.2014, time 82.61ms
2024-09-10 09:15:20 iter 1720: loss 1.1901, time 83.63ms
2024-09-10 09:15:21 iter 1730: loss 1.1985, time 81.27ms
2024-09-10 09:15:21 iter 1740: loss 1.1900, time 82.40ms
2024-09-10 09:15:32 step 1750: train loss 1.1009, val loss 1.4706
2024-09-10 09:15:33 iter 1750: loss 1.1972, time 10722.69ms
2024-09-10 09:15:33 iter 1760: loss 1.1691, time 83.83ms
2024-09-10 09:15:34 iter 1770: loss 1.1632, time 82.86ms
2024-09-10 09:15:35 iter 1780: loss 1.1591, time 82.72ms
2024-09-10 09:15:36 iter 1790: loss 1.1853, time 82.35ms
2024-09-10 09:15:37 iter 1800: loss 1.1653, time 82.37ms
2024-09-10 09:15:38 iter 1810: loss 1.1797, time 82.75ms
2024-09-10 09:15:38 iter 1820: loss 1.1609, time 82.07ms
2024-09-10 09:15:39 iter 1830: loss 1.1281, time 82.50ms
2024-09-10 09:15:40 iter 1840: loss 1.1805, time 85.79ms
2024-09-10 09:15:41 iter 1850: loss 1.1852, time 81.40ms
2024-09-10 09:15:42 iter 1860: loss 1.1931, time 82.55ms
2024-09-10 09:15:43 iter 1870: loss 1.1491, time 82.69ms
2024-09-10 09:15:43 iter 1880: loss 1.2027, time 82.33ms
2024-09-10 09:15:44 iter 1890: loss 1.1449, time 83.45ms
2024-09-10 09:15:45 iter 1900: loss 1.1856, time 90.00ms
2024-09-10 09:15:46 iter 1910: loss 1.1347, time 82.31ms
2024-09-10 09:15:47 iter 1920: loss 1.1415, time 83.11ms
2024-09-10 09:15:48 iter 1930: loss 1.1381, time 82.88ms
2024-09-10 09:15:48 iter 1940: loss 1.1545, time 83.06ms
2024-09-10 09:15:49 iter 1950: loss 1.1630, time 82.30ms
2024-09-10 09:15:50 iter 1960: loss 1.1427, time 82.82ms
2024-09-10 09:15:51 iter 1970: loss 1.1373, time 82.56ms
2024-09-10 09:15:52 iter 1980: loss 1.1557, time 83.75ms
2024-09-10 09:15:53 iter 1990: loss 1.1429, time 87.82ms
2024-09-10 09:16:04 step 2000: train loss 1.0523, val loss 1.4714
2024-09-10 09:16:04 iter 2000: loss 1.1524, time 10649.51ms
2024-09-10 09:16:05 iter 2010: loss 1.1364, time 82.33ms
2024-09-10 09:16:05 iter 2020: loss 1.1503, time 82.45ms
2024-09-10 09:16:06 iter 2030: loss 1.1297, time 82.63ms
2024-09-10 09:16:07 iter 2040: loss 1.1351, time 82.62ms
2024-09-10 09:16:08 iter 2050: loss 1.1390, time 83.16ms
2024-09-10 09:16:09 iter 2060: loss 1.1634, time 82.52ms
2024-09-10 09:16:09 iter 2070: loss 1.1559, time 82.52ms
2024-09-10 09:16:10 iter 2080: loss 1.1379, time 82.79ms
2024-09-10 09:16:11 iter 2090: loss 1.1153, time 82.57ms
2024-09-10 09:16:12 iter 2100: loss 1.1172, time 82.39ms
2024-09-10 09:16:13 iter 2110: loss 1.1115, time 82.37ms
2024-09-10 09:16:14 iter 2120: loss 1.0852, time 82.97ms
2024-09-10 09:16:14 iter 2130: loss 1.1188, time 82.52ms
2024-09-10 09:16:15 iter 2140: loss 1.1228, time 82.54ms
2024-09-10 09:16:16 iter 2150: loss 1.1321, time 83.00ms
2024-09-10 09:16:17 iter 2160: loss 1.1387, time 82.75ms
2024-09-10 09:16:18 iter 2170: loss 1.1413, time 82.68ms
2024-09-10 09:16:19 iter 2180: loss 1.1400, time 83.10ms
2024-09-10 09:16:19 iter 2190: loss 1.1289, time 82.61ms
2024-09-10 09:16:20 iter 2200: loss 1.1105, time 82.62ms
2024-09-10 09:16:21 iter 2210: loss 1.1136, time 82.19ms
2024-09-10 09:16:22 iter 2220: loss 1.1022, time 82.20ms
2024-09-10 09:16:23 iter 2230: loss 1.1102, time 82.20ms
2024-09-10 09:16:24 iter 2240: loss 1.1148, time 82.26ms
2024-09-10 09:16:35 step 2250: train loss 1.0068, val loss 1.4721
2024-09-10 09:16:35 iter 2250: loss 1.1008, time 10587.56ms
2024-09-10 09:16:35 iter 2260: loss 1.1074, time 82.84ms
2024-09-10 09:16:36 iter 2270: loss 1.1249, time 82.81ms
2024-09-10 09:16:37 iter 2280: loss 1.1051, time 82.90ms
2024-09-10 09:16:38 iter 2290: loss 1.0767, time 82.20ms
2024-09-10 09:16:39 iter 2300: loss 1.0608, time 82.63ms
2024-09-10 09:16:40 iter 2310: loss 1.0991, time 82.15ms
2024-09-10 09:16:40 iter 2320: loss 1.0896, time 82.06ms
2024-09-10 09:16:41 iter 2330: loss 1.1208, time 82.27ms
2024-09-10 09:16:42 iter 2340: loss 1.0841, time 83.61ms
2024-09-10 09:16:43 iter 2350: loss 1.1147, time 83.43ms
2024-09-10 09:16:44 iter 2360: loss 1.0837, time 82.80ms
2024-09-10 09:16:45 iter 2370: loss 1.0753, time 82.24ms
2024-09-10 09:16:45 iter 2380: loss 1.0913, time 82.18ms
2024-09-10 09:16:46 iter 2390: loss 1.0810, time 81.25ms
2024-09-10 09:16:47 iter 2400: loss 1.1006, time 82.65ms
2024-09-10 09:16:48 iter 2410: loss 1.0715, time 84.25ms
2024-09-10 09:16:49 iter 2420: loss 1.0547, time 82.78ms
2024-09-10 09:16:50 iter 2430: loss 1.0930, time 82.60ms
2024-09-10 09:16:50 iter 2440: loss 1.0951, time 82.67ms
2024-09-10 09:16:51 iter 2450: loss 1.0703, time 82.16ms
2024-09-10 09:16:52 iter 2460: loss 1.0424, time 82.63ms
2024-09-10 09:16:53 iter 2470: loss 1.0930, time 83.63ms
2024-09-10 09:16:54 iter 2480: loss 1.0626, time 85.36ms
2024-09-10 09:16:55 iter 2490: loss 1.0823, time 85.60ms
2024-09-10 09:17:06 step 2500: train loss 0.9617, val loss 1.4983
2024-09-10 09:17:06 iter 2500: loss 1.0563, time 10750.01ms
2024-09-10 09:17:07 iter 2510: loss 1.0710, time 87.84ms
2024-09-10 09:17:07 iter 2520: loss 1.0651, time 83.89ms
2024-09-10 09:17:08 iter 2530: loss 1.0664, time 81.99ms
2024-09-10 09:17:09 iter 2540: loss 1.0646, time 87.54ms
2024-09-10 09:17:10 iter 2550: loss 1.0628, time 83.08ms
2024-09-10 09:17:11 iter 2560: loss 1.0518, time 81.52ms
2024-09-10 09:17:12 iter 2570: loss 1.0743, time 81.70ms
2024-09-10 09:17:12 iter 2580: loss 1.0481, time 83.36ms
2024-09-10 09:17:13 iter 2590: loss 1.0769, time 82.42ms
2024-09-10 09:17:14 iter 2600: loss 1.0861, time 82.65ms
2024-09-10 09:17:15 iter 2610: loss 1.0758, time 82.75ms
2024-09-10 09:17:16 iter 2620: loss 1.0548, time 82.45ms
2024-09-10 09:17:17 iter 2630: loss 1.0599, time 81.83ms
2024-09-10 09:17:17 iter 2640: loss 1.0487, time 82.85ms
2024-09-10 09:17:18 iter 2650: loss 1.0531, time 82.88ms
2024-09-10 09:17:19 iter 2660: loss 1.0456, time 85.20ms
2024-09-10 09:17:20 iter 2670: loss 1.0262, time 85.24ms
2024-09-10 09:17:21 iter 2680: loss 1.0393, time 82.38ms
2024-09-10 09:17:22 iter 2690: loss 1.0645, time 82.17ms
2024-09-10 09:17:22 iter 2700: loss 1.0826, time 82.53ms
2024-09-10 09:17:23 iter 2710: loss 1.0479, time 82.64ms
2024-09-10 09:17:24 iter 2720: loss 1.0339, time 83.16ms
2024-09-10 09:17:25 iter 2730: loss 1.0375, time 83.25ms
2024-09-10 09:17:26 iter 2740: loss 1.0373, time 82.67ms
2024-09-10 09:17:37 step 2750: train loss 0.9098, val loss 1.5023
2024-09-10 09:17:37 iter 2750: loss 1.0416, time 10775.54ms
2024-09-10 09:17:38 iter 2760: loss 1.0105, time 85.17ms
2024-09-10 09:17:39 iter 2770: loss 1.0051, time 84.82ms
2024-09-10 09:17:40 iter 2780: loss 1.0149, time 84.39ms
2024-09-10 09:17:40 iter 2790: loss 1.0344, time 84.07ms
2024-09-10 09:17:41 iter 2800: loss 1.0144, time 85.40ms
2024-09-10 09:17:42 iter 2810: loss 0.9850, time 85.06ms
2024-09-10 09:17:43 iter 2820: loss 1.0197, time 85.46ms
2024-09-10 09:17:44 iter 2830: loss 0.9722, time 85.51ms
2024-09-10 09:17:45 iter 2840: loss 1.0369, time 85.97ms
2024-09-10 09:17:46 iter 2850: loss 1.0203, time 82.56ms
2024-09-10 09:17:46 iter 2860: loss 1.0306, time 87.36ms
2024-09-10 09:17:47 iter 2870: loss 1.0322, time 82.35ms
2024-09-10 09:17:48 iter 2880: loss 1.0062, time 86.84ms
2024-09-10 09:17:49 iter 2890: loss 0.9859, time 82.25ms
2024-09-10 09:17:50 iter 2900: loss 1.0036, time 81.79ms
2024-09-10 09:17:51 iter 2910: loss 1.0267, time 82.03ms
2024-09-10 09:17:51 iter 2920: loss 1.0254, time 82.57ms
2024-09-10 09:17:52 iter 2930: loss 0.9876, time 82.96ms
2024-09-10 09:17:53 iter 2940: loss 0.9947, time 82.68ms
2024-09-10 09:17:54 iter 2950: loss 1.0018, time 81.73ms
2024-09-10 09:17:55 iter 2960: loss 1.0238, time 85.59ms
2024-09-10 09:17:56 iter 2970: loss 0.9847, time 82.40ms
2024-09-10 09:17:56 iter 2980: loss 0.9858, time 81.88ms
2024-09-10 09:17:57 iter 2990: loss 0.9878, time 82.57ms
2024-09-10 09:18:08 step 3000: train loss 0.8665, val loss 1.5225
2024-09-10 09:18:08 iter 3000: loss 0.9881, time 10622.51ms
2024-09-10 09:18:09 iter 3010: loss 0.9812, time 82.73ms
2024-09-10 09:18:10 iter 3020: loss 1.0102, time 81.82ms
2024-09-10 09:18:11 iter 3030: loss 0.9887, time 81.88ms
2024-09-10 09:18:12 iter 3040: loss 0.9653, time 82.19ms
2024-09-10 09:18:13 iter 3050: loss 0.9884, time 82.72ms
2024-09-10 09:18:13 iter 3060: loss 1.0107, time 85.28ms
2024-09-10 09:18:14 iter 3070: loss 0.9917, time 84.02ms
2024-09-10 09:18:15 iter 3080: loss 0.9738, time 82.15ms
2024-09-10 09:18:16 iter 3090: loss 1.0063, time 82.48ms
2024-09-10 09:18:17 iter 3100: loss 0.9901, time 83.56ms
2024-09-10 09:18:18 iter 3110: loss 1.0154, time 82.19ms
2024-09-10 09:18:18 iter 3120: loss 0.9745, time 82.84ms
2024-09-10 09:18:19 iter 3130: loss 0.9636, time 83.39ms
2024-09-10 09:18:20 iter 3140: loss 0.9847, time 81.20ms
2024-09-10 09:18:21 iter 3150: loss 0.9899, time 82.43ms
2024-09-10 09:18:22 iter 3160: loss 0.9664, time 82.02ms
2024-09-10 09:18:23 iter 3170: loss 0.9616, time 82.73ms
2024-09-10 09:18:23 iter 3180: loss 0.9819, time 83.08ms
2024-09-10 09:18:24 iter 3190: loss 0.9562, time 82.47ms
2024-09-10 09:18:25 iter 3200: loss 0.9540, time 81.26ms
2024-09-10 09:18:26 iter 3210: loss 0.9528, time 82.25ms
2024-09-10 09:18:27 iter 3220: loss 0.9362, time 82.63ms
2024-09-10 09:18:27 iter 3230: loss 0.9588, time 81.84ms
2024-09-10 09:18:28 iter 3240: loss 0.9640, time 81.79ms
2024-09-10 09:18:39 step 3250: train loss 0.8220, val loss 1.5387
2024-09-10 09:18:40 iter 3250: loss 0.9319, time 10724.66ms
2024-09-10 09:18:40 iter 3260: loss 0.9439, time 82.48ms
2024-09-10 09:18:41 iter 3270: loss 0.9444, time 82.71ms
2024-09-10 09:18:42 iter 3280: loss 0.9341, time 82.14ms
2024-09-10 09:18:43 iter 3290: loss 0.9686, time 82.98ms
2024-09-10 09:18:44 iter 3300: loss 0.9545, time 82.53ms
2024-09-10 09:18:45 iter 3310: loss 0.9737, time 81.51ms
2024-09-10 09:18:45 iter 3320: loss 0.9176, time 82.61ms
2024-09-10 09:18:46 iter 3330: loss 0.9600, time 82.15ms
2024-09-10 09:18:47 iter 3340: loss 0.9773, time 82.61ms
2024-09-10 09:18:48 iter 3350: loss 0.9502, time 82.83ms
2024-09-10 09:18:49 iter 3360: loss 0.9627, time 82.09ms
2024-09-10 09:18:49 iter 3370: loss 0.9475, time 82.02ms
2024-09-10 09:18:50 iter 3380: loss 0.9341, time 83.37ms
2024-09-10 09:18:51 iter 3390: loss 0.9247, time 82.83ms
2024-09-10 09:18:52 iter 3400: loss 0.9762, time 82.18ms
2024-09-10 09:18:53 iter 3410: loss 0.9683, time 82.08ms
2024-09-10 09:18:54 iter 3420: loss 0.9346, time 82.20ms
2024-09-10 09:18:54 iter 3430: loss 0.9259, time 82.48ms
2024-09-10 09:18:55 iter 3440: loss 0.9542, time 82.33ms
2024-09-10 09:18:56 iter 3450: loss 0.9540, time 82.35ms
2024-09-10 09:18:57 iter 3460: loss 0.9330, time 82.43ms
2024-09-10 09:18:58 iter 3470: loss 0.9204, time 83.31ms
2024-09-10 09:18:59 iter 3480: loss 0.9086, time 82.80ms
2024-09-10 09:18:59 iter 3490: loss 0.9461, time 82.62ms
2024-09-10 09:19:10 step 3500: train loss 0.7796, val loss 1.5654
2024-09-10 09:19:10 iter 3500: loss 0.9108, time 10626.82ms
2024-09-10 09:19:11 iter 3510: loss 0.9152, time 83.92ms
2024-09-10 09:19:12 iter 3520: loss 0.9369, time 82.44ms
2024-09-10 09:19:13 iter 3530: loss 0.9390, time 84.02ms
2024-09-10 09:19:14 iter 3540: loss 0.9287, time 82.54ms
2024-09-10 09:19:15 iter 3550: loss 0.9333, time 82.35ms
2024-09-10 09:19:16 iter 3560: loss 0.9426, time 83.08ms
2024-09-10 09:19:16 iter 3570: loss 0.9306, time 82.49ms
2024-09-10 09:19:17 iter 3580: loss 0.9341, time 82.59ms
2024-09-10 09:19:18 iter 3590: loss 0.9117, time 82.43ms
2024-09-10 09:19:19 iter 3600: loss 0.9302, time 82.22ms
2024-09-10 09:19:20 iter 3610: loss 0.9177, time 82.17ms
2024-09-10 09:19:21 iter 3620: loss 0.9123, time 83.00ms
2024-09-10 09:19:21 iter 3630: loss 0.9146, time 82.53ms
2024-09-10 09:19:22 iter 3640: loss 0.9321, time 81.54ms
2024-09-10 09:19:23 iter 3650: loss 0.8989, time 82.92ms
2024-09-10 09:19:24 iter 3660: loss 0.9182, time 86.09ms
2024-09-10 09:19:25 iter 3670: loss 0.9192, time 82.27ms
2024-09-10 09:19:26 iter 3680: loss 0.8981, time 82.75ms
2024-09-10 09:19:26 iter 3690: loss 0.9265, time 81.75ms
2024-09-10 09:19:27 iter 3700: loss 0.9327, time 82.48ms
2024-09-10 09:19:28 iter 3710: loss 0.9205, time 82.75ms
2024-09-10 09:19:29 iter 3720: loss 0.8926, time 83.79ms
2024-09-10 09:19:30 iter 3730: loss 0.9152, time 84.49ms
2024-09-10 09:19:31 iter 3740: loss 0.8968, time 82.80ms
2024-09-10 09:19:42 step 3750: train loss 0.7381, val loss 1.6063
2024-09-10 09:19:42 iter 3750: loss 0.9437, time 10736.17ms
2024-09-10 09:19:43 iter 3760: loss 0.8993, time 82.60ms
2024-09-10 09:19:43 iter 3770: loss 0.9082, time 81.55ms
2024-09-10 09:19:44 iter 3780: loss 0.9040, time 82.12ms
2024-09-10 09:19:45 iter 3790: loss 0.9112, time 83.85ms
2024-09-10 09:19:46 iter 3800: loss 0.9169, time 84.57ms
2024-09-10 09:19:47 iter 3810: loss 0.8978, time 83.37ms
2024-09-10 09:19:48 iter 3820: loss 0.8695, time 82.68ms
2024-09-10 09:19:49 iter 3830: loss 0.8740, time 82.21ms
2024-09-10 09:19:49 iter 3840: loss 0.9104, time 85.08ms
2024-09-10 09:19:50 iter 3850: loss 0.8690, time 82.06ms
2024-09-10 09:19:51 iter 3860: loss 0.9008, time 82.57ms
2024-09-10 09:19:52 iter 3870: loss 0.8808, time 82.58ms
2024-09-10 09:19:53 iter 3880: loss 0.8809, time 82.15ms
2024-09-10 09:19:54 iter 3890: loss 0.9043, time 82.26ms
2024-09-10 09:19:54 iter 3900: loss 0.8722, time 82.34ms
2024-09-10 09:19:55 iter 3910: loss 0.8932, time 82.39ms
2024-09-10 09:19:56 iter 3920: loss 0.8880, time 84.74ms
2024-09-10 09:19:57 iter 3930: loss 0.8639, time 82.61ms
2024-09-10 09:19:58 iter 3940: loss 0.8791, time 82.46ms
2024-09-10 09:19:59 iter 3950: loss 0.9181, time 84.04ms
2024-09-10 09:19:59 iter 3960: loss 0.8842, time 82.62ms
2024-09-10 09:20:00 iter 3970: loss 0.8892, time 85.08ms
2024-09-10 09:20:01 iter 3980: loss 0.8805, time 82.46ms
2024-09-10 09:20:02 iter 3990: loss 0.8812, time 82.36ms
2024-09-10 09:20:13 step 4000: train loss 0.7057, val loss 1.6271
2024-09-10 09:20:13 iter 4000: loss 0.8671, time 10697.34ms
2024-09-10 09:20:14 iter 4010: loss 0.8552, time 84.92ms
2024-09-10 09:20:15 iter 4020: loss 0.8742, time 83.53ms
2024-09-10 09:20:16 iter 4030: loss 0.9053, time 87.25ms
2024-09-10 09:20:16 iter 4040: loss 0.8938, time 82.39ms
2024-09-10 09:20:17 iter 4050: loss 0.8807, time 84.45ms
2024-09-10 09:20:18 iter 4060: loss 0.8643, time 84.71ms
2024-09-10 09:20:19 iter 4070: loss 0.8630, time 82.74ms
2024-09-10 09:20:20 iter 4080: loss 0.8573, time 85.78ms
2024-09-10 09:20:21 iter 4090: loss 0.8709, time 82.52ms
2024-09-10 09:20:21 iter 4100: loss 0.8411, time 82.19ms
2024-09-10 09:20:22 iter 4110: loss 0.8782, time 82.28ms
2024-09-10 09:20:23 iter 4120: loss 0.8781, time 82.05ms
2024-09-10 09:20:24 iter 4130: loss 0.8614, time 82.05ms
2024-09-10 09:20:25 iter 4140: loss 0.8649, time 82.73ms
2024-09-10 09:20:25 iter 4150: loss 0.8761, time 83.56ms
2024-09-10 09:20:26 iter 4160: loss 0.8429, time 85.01ms
2024-09-10 09:20:27 iter 4170: loss 0.8715, time 86.76ms
2024-09-10 09:20:28 iter 4180: loss 0.8633, time 81.89ms
2024-09-10 09:20:29 iter 4190: loss 0.8520, time 81.75ms
2024-09-10 09:20:30 iter 4200: loss 0.8389, time 82.21ms
2024-09-10 09:20:31 iter 4210: loss 0.8462, time 84.40ms
2024-09-10 09:20:31 iter 4220: loss 0.8732, time 80.64ms
2024-09-10 09:20:32 iter 4230: loss 0.8685, time 89.18ms
2024-09-10 09:20:33 iter 4240: loss 0.8592, time 88.15ms
2024-09-10 09:20:44 step 4250: train loss 0.6756, val loss 1.6448
2024-09-10 09:20:44 iter 4250: loss 0.8612, time 10793.53ms
2024-09-10 09:20:45 iter 4260: loss 0.8829, time 81.68ms
2024-09-10 09:20:46 iter 4270: loss 0.8533, time 81.39ms
2024-09-10 09:20:47 iter 4280: loss 0.8666, time 81.40ms
2024-09-10 09:20:48 iter 4290: loss 0.8292, time 85.94ms
2024-09-10 09:20:49 iter 4300: loss 0.8620, time 81.36ms
2024-09-10 09:20:49 iter 4310: loss 0.8367, time 80.57ms
2024-09-10 09:20:50 iter 4320: loss 0.8775, time 80.39ms
2024-09-10 09:20:51 iter 4330: loss 0.8531, time 82.55ms
2024-09-10 09:20:52 iter 4340: loss 0.8281, time 82.43ms
2024-09-10 09:20:53 iter 4350: loss 0.8462, time 82.75ms
2024-09-10 09:20:53 iter 4360: loss 0.8380, time 85.00ms
2024-09-10 09:20:54 iter 4370: loss 0.8348, time 83.04ms
2024-09-10 09:20:55 iter 4380: loss 0.8409, time 84.89ms
2024-09-10 09:20:56 iter 4390: loss 0.8579, time 82.94ms
2024-09-10 09:20:57 iter 4400: loss 0.8459, time 84.37ms
2024-09-10 09:20:58 iter 4410: loss 0.8492, time 82.42ms
2024-09-10 09:20:58 iter 4420: loss 0.8577, time 82.64ms
2024-09-10 09:20:59 iter 4430: loss 0.8413, time 85.76ms
2024-09-10 09:21:00 iter 4440: loss 0.8667, time 82.70ms
2024-09-10 09:21:01 iter 4450: loss 0.8492, time 82.22ms
2024-09-10 09:21:02 iter 4460: loss 0.8334, time 81.16ms
2024-09-10 09:21:03 iter 4470: loss 0.8696, time 85.26ms
2024-09-10 09:21:03 iter 4480: loss 0.8223, time 82.64ms
2024-09-10 09:21:04 iter 4490: loss 0.8583, time 82.34ms
2024-09-10 09:21:15 step 4500: train loss 0.6495, val loss 1.6716
2024-09-10 09:21:15 iter 4500: loss 0.8342, time 10657.18ms
2024-09-10 09:21:16 iter 4510: loss 0.8571, time 85.42ms
2024-09-10 09:21:17 iter 4520: loss 0.8217, time 82.32ms
2024-09-10 09:21:18 iter 4530: loss 0.8662, time 83.57ms
2024-09-10 09:21:19 iter 4540: loss 0.8324, time 83.16ms
2024-09-10 09:21:20 iter 4550: loss 0.8632, time 82.36ms
2024-09-10 09:21:20 iter 4560: loss 0.8578, time 82.05ms
2024-09-10 09:21:21 iter 4570: loss 0.8513, time 86.05ms
2024-09-10 09:21:22 iter 4580: loss 0.8170, time 83.54ms
2024-09-10 09:21:23 iter 4590: loss 0.8405, time 81.39ms
2024-09-10 09:21:24 iter 4600: loss 0.8354, time 83.01ms
2024-09-10 09:21:25 iter 4610: loss 0.8258, time 82.28ms
2024-09-10 09:21:25 iter 4620: loss 0.8265, time 82.71ms
2024-09-10 09:21:26 iter 4630: loss 0.8329, time 82.53ms
2024-09-10 09:21:27 iter 4640: loss 0.8599, time 85.78ms
2024-09-10 09:21:28 iter 4650: loss 0.8195, time 82.03ms
2024-09-10 09:23:07 iter 4660: loss 0.8331, time 85.78ms
2024-09-10 09:23:08 iter 4670: loss 0.8332, time 75.94ms
2024-09-10 09:23:09 iter 4680: loss 0.8455, time 77.58ms
2024-09-10 09:23:10 iter 4690: loss 0.8271, time 76.70ms
2024-09-10 09:23:11 iter 4700: loss 0.8224, time 77.96ms
2024-09-10 09:23:12 iter 4710: loss 0.8401, time 76.54ms
2024-09-10 09:23:12 iter 4720: loss 0.8365, time 77.06ms
2024-09-10 09:23:13 iter 4730: loss 0.8347, time 76.72ms
2024-09-10 09:23:14 iter 4740: loss 0.8197, time 77.16ms
2024-09-10 09:23:24 step 4750: train loss 0.6327, val loss 1.6776
2024-09-10 09:23:24 iter 4750: loss 0.8294, time 10120.55ms
2024-09-10 09:23:25 iter 4760: loss 0.8192, time 77.33ms
2024-09-10 09:23:26 iter 4770: loss 0.8209, time 77.49ms
2024-09-10 09:23:27 iter 4780: loss 0.8431, time 77.69ms
2024-09-10 09:23:28 iter 4790: loss 0.8259, time 77.68ms
2024-09-10 09:23:28 iter 4800: loss 0.8459, time 77.38ms
2024-09-10 09:23:29 iter 4810: loss 0.8252, time 77.63ms
2024-09-10 09:23:30 iter 4820: loss 0.8178, time 78.07ms
2024-09-10 09:23:31 iter 4830: loss 0.8282, time 78.68ms
2024-09-10 09:23:31 iter 4840: loss 0.8217, time 78.41ms
2024-09-10 09:23:32 iter 4850: loss 0.8394, time 77.96ms
2024-09-10 09:23:33 iter 4860: loss 0.8217, time 77.74ms
2024-09-10 09:23:34 iter 4870: loss 0.8156, time 78.50ms
2024-09-10 09:23:35 iter 4880: loss 0.8114, time 78.34ms
2024-09-10 09:23:35 iter 4890: loss 0.8049, time 78.28ms
2024-09-10 09:23:36 iter 4900: loss 0.8270, time 78.91ms
2024-09-10 09:23:37 iter 4910: loss 0.8323, time 78.28ms
2024-09-10 09:23:38 iter 4920: loss 0.8296, time 79.20ms
2024-09-10 09:23:39 iter 4930: loss 0.8162, time 78.91ms
2024-09-10 09:23:39 iter 4940: loss 0.8179, time 79.54ms
2024-09-10 09:23:40 iter 4950: loss 0.8247, time 79.06ms
2024-09-10 09:23:41 iter 4960: loss 0.8172, time 79.98ms
2024-09-10 09:23:42 iter 4970: loss 0.8340, time 79.36ms
2024-09-10 09:23:42 iter 4980: loss 0.8328, time 78.48ms
2024-09-10 09:23:43 iter 4990: loss 0.7944, time 79.41ms
2024-09-10 09:23:54 step 5000: train loss 0.6162, val loss 1.6962
2024-09-10 09:23:54 iter 5000: loss 0.8095, time 10303.44ms
2024-09-10 09:23:54 training done
2024-09-10 09:23:54 Best validation loss: 1.4629472494125366
2024-09-10 09:23:54 Total train time: 12.35 mins
2024-09-10 09:23:54 Loading meta from ../../data/shakespeare_char/meta.pkl...
2024-09-10 09:23:56 Sample 1:
2024-09-10 09:23:56  the contrary.
2024-09-10 09:23:56 
2024-09-10 09:23:56 BENVOLIO:
2024-09-10 09:23:56 I do beseech you, sir, test me my rage
2024-09-10 09:23:56 And not my word 'fore the prince's doom.
2024-09-10 09:23:56 
2024-09-10 09:23:56 MERCUTIO:
2024-09-10 09:23:56 I thank you, good fellow, sir; now are you all'?
2024-09-10 09:23:56 
2024-09-10 09:23:56 ROMEO:
2024-09-10 09:23:56 Nay, sir, nay, you say, sir.
2024-09-10 09:23:56 
2024-09-10 09:23:56 MERCUTIO:
2024-09-10 09:23:56 Why, 'tis not out of a man for a man coward,
2024-09-10 09:23:56 sir.
2024-09-10 09:23:56 
2024-09-10 09:23:56 ROMEO:
2024-09-10 09:23:56 I pray you, sir, let him go serve; and there is no man
2024-09-10 09:23:56 shall stay the reek; and I he shall stand for love a
2024-09-10 09:23:56 hundred's skull: so besides.
2024-09-10 09:23:56 
2024-09-10 09:23:56 MERCUTIO:
2024-09-10 09:23:56 Sir, it is the wisest soldier he was a man--
2024-09-10 09:23:56 
2024-09-10 09:23:56 ROMEO:
2024-09-10 09:23:56 A kind of brave fellow! ah, 
2024-09-10 09:23:56 Inference time: 2.00 seconds
2024-09-10 09:23:56 Tokens per second: 250.38
2024-09-10 09:23:56 ---------------
2024-09-10 09:23:58 Sample 2:
2024-09-10 09:23:58  away with me.
2024-09-10 09:23:58 
2024-09-10 09:23:58 POMPEY:
2024-09-10 09:23:58 Why, no; I spoke with this character, my brother and
2024-09-10 09:23:58 the sister of his just proverb Christian straight: and I
2024-09-10 09:23:58 have fought it, and think it is best a man may straight alone.
2024-09-10 09:23:58 
2024-09-10 09:23:58 POMPEY:
2024-09-10 09:23:58 I am the greater for the time that I love the present.
2024-09-10 09:23:58 
2024-09-10 09:23:58 ESCALUS:
2024-09-10 09:23:58 I thought you had to been a subject down,
2024-09-10 09:23:58 And look'd on a harvest-man to my wife,
2024-09-10 09:23:58 Whose hand is spoke, the prince my life's son:
2024-09-10 09:23:58 Who should seem to heaven, if I then should have rain
2024-09-10 09:23:58 And never will end the hate utter for the field
2024-09-10 09:23:58 Inference time: 1.93 seconds
2024-09-10 09:23:58 Tokens per second: 259.33
2024-09-10 09:23:58 ---------------
2024-09-10 09:24:00 Sample 3:
2024-09-10 09:24:00  of late,
2024-09-10 09:24:00 And I heard the drunkard blood upon thy loyalty.
2024-09-10 09:24:00 
2024-09-10 09:24:00 BUCKINGHAM:
2024-09-10 09:24:00 How now, my lord, that thy lord?
2024-09-10 09:24:00 
2024-09-10 09:24:00 BUCKINGHAM:
2024-09-10 09:24:00 My lord, this promise the Lord Northumberland,
2024-09-10 09:24:00 Here cousin Richard; and send thy brother York
2024-09-10 09:24:00 Hath pass'd a slaughter-oather with the crown,
2024-09-10 09:24:00 And fall'd the stones hour owes on the back of York.
2024-09-10 09:24:00 I tell thee, Clarence, tell me, for of Clarence,
2024-09-10 09:24:00 What doth he so much is of those wrongs
2024-09-10 09:24:00 That they gave me from me to do them good?
2024-09-10 09:24:00 
2024-09-10 09:24:00 SOMERSET:
2024-09-10 09:24:00 I do beseech your highness to add myself;
2024-09-10 09:24:00 And 
2024-09-10 09:24:00 Inference time: 1.96 seconds
2024-09-10 09:24:00 Tokens per second: 255.72
2024-09-10 09:24:00 ---------------
2024-09-10 09:24:02 Sample 4:
2024-09-10 09:24:02  not be so.
2024-09-10 09:24:02 
2024-09-10 09:24:02 CLAUDIO:
2024-09-10 09:24:02 He's a limb of time: he had, been a foot
2024-09-10 09:24:02 To lie his function: he had been a man of sleep,
2024-09-10 09:24:02 And kneel'd afterwards.
2024-09-10 09:24:02 
2024-09-10 09:24:02 ISABELLA:
2024-09-10 09:24:02 Assay the prince's most unwillingness,
2024-09-10 09:24:02 How it more since it is a south of state,
2024-09-10 09:24:02 Than doth appear like with time.
2024-09-10 09:24:02 
2024-09-10 09:24:02 ISABELLA:
2024-09-10 09:24:02 And thou by my daughter,
2024-09-10 09:24:02 I do but let me be seen the senate.
2024-09-10 09:24:02 
2024-09-10 09:24:02 DUKE VINCENTIO:
2024-09-10 09:24:02 Sir, you pardon him.
2024-09-10 09:24:02 
2024-09-10 09:24:02 LUCIO:
2024-09-10 09:24:02 Marry, I will; and will not only be here too
2024-09-10 09:24:02 complainted with him. And if so I can read a
2024-09-10 09:24:02 man to expect him this house, I 
2024-09-10 09:24:02 Inference time: 2.10 seconds
2024-09-10 09:24:02 Tokens per second: 238.61
2024-09-10 09:24:02 ---------------
2024-09-10 09:24:04 Sample 5:
2024-09-10 09:24:04  this, the poor steep, and see,
2024-09-10 09:24:04 Play'd in safety of morning stones,
2024-09-10 09:24:04 And by having bound to this world woeful deeds!
2024-09-10 09:24:04 O child, my happy accursed man!
2024-09-10 09:24:04 O time, lie now! now day, that dreadful Edward's throne,
2024-09-10 09:24:04 Which he was the most breath to the heaven,
2024-09-10 09:24:04 And hath a mother,
2024-09-10 09:24:04 Which made me with rescue. But what first I do think?
2024-09-10 09:24:04 
2024-09-10 09:24:04 ESCALUS:
2024-09-10 09:24:04 And be a son to find and a widow's neck,
2024-09-10 09:24:04 And a son of the slavish choice: why, now the wrong
2024-09-10 09:24:04 Which shall be brief when the sun shines and so side
2024-09-10 09:24:04 As to my kinsmen and th
2024-09-10 09:24:04 Inference time: 2.07 seconds
2024-09-10 09:24:04 Tokens per second: 241.83
2024-09-10 09:24:04 ---------------
2024-09-10 09:24:06 Sample 6:
2024-09-10 09:24:06  you, consider Hastings,
2024-09-10 09:24:06 Whom you shall hear from Rutland no more;
2024-09-10 09:24:06 The shall be so long as yours early and you.
2024-09-10 09:24:06 
2024-09-10 09:24:06 HASTINGS:
2024-09-10 09:24:06 Good morrow, Catesby, with my letters to the crown,
2024-09-10 09:24:06 And thou shalt not hear themselves of mine
2024-09-10 09:24:06 Say, nor I will kill her ready.
2024-09-10 09:24:06 
2024-09-10 09:24:06 CATESBY:
2024-09-10 09:24:06 My lord, my lord, my lord.
2024-09-10 09:24:06 
2024-09-10 09:24:06 HASTINGS:
2024-09-10 09:24:06 With all my heart.
2024-09-10 09:24:06 
2024-09-10 09:24:06 CATESBY:
2024-09-10 09:24:06 I did consent to them; for thence mean to go with
2024-09-10 09:24:06 The rest spur; I repent mine own life,--
2024-09-10 09:24:06 
2024-09-10 09:24:06 HASTINGS:
2024-09-10 09:24:06 I know so.
2024-09-10 09:24:06 
2024-09-10 09:24:06 GLOUCESTER:
2024-09-10 09:24:06 It is your grace looks on me and fearless my gui
2024-09-10 09:24:06 Inference time: 2.03 seconds
2024-09-10 09:24:06 Tokens per second: 245.77
2024-09-10 09:24:06 ---------------
2024-09-10 09:24:08 Sample 7:
2024-09-10 09:24:08  the good provost,
2024-09-10 09:24:08 We may show me to his presence of care
2024-09-10 09:24:08 That were here delivered to cross him,
2024-09-10 09:24:08 And he did him claim his new ears.
2024-09-10 09:24:08 
2024-09-10 09:24:08 DUKE OF AUMERLE:
2024-09-10 09:24:08 My liege, I hear my liege, see the near
2024-09-10 09:24:08 To nothing but stay, and to come with my request
2024-09-10 09:24:08 Unto my person all the oracle's hand
2024-09-10 09:24:08 For what is nothing: if it be, thou dost confess,
2024-09-10 09:24:08 And thou shalt stir out the bishop's wretch,
2024-09-10 09:24:08 To set the murderous purpose. Nay, go back again,
2024-09-10 09:24:08 Where is thy brain, statue but not the storm;
2024-09-10 09:24:08 And see, then to be so bold in war
2024-09-10 09:24:08 Inference time: 2.13 seconds
2024-09-10 09:24:08 Tokens per second: 234.76
2024-09-10 09:24:08 ---------------
2024-09-10 09:24:10 Sample 8:
2024-09-10 09:24:10  and cover'd all the story of my sorrow;
2024-09-10 09:24:10 And therefore the desire of the gates do presently
2024-09-10 09:24:10 Came to the plain people, cover'd with them,
2024-09-10 09:24:10 And stand too much murder for our virtues,
2024-09-10 09:24:10 And therefore an empty of the people,
2024-09-10 09:24:10 Let them approve each on their souls: all's hand is their
2024-09-10 09:24:10 That they must bear their brothern themselves; but I
2024-09-10 09:24:10 Will perchase them for the king.
2024-09-10 09:24:10 
2024-09-10 09:24:10 LEONTES:
2024-09-10 09:24:10 What, will they go?
2024-09-10 09:24:10 
2024-09-10 09:24:10 PERDITA:
2024-09-10 09:24:10 You shall ha' if some wit feel to the world,
2024-09-10 09:24:10 Learn out mine own fine and the battles of my speech?
2024-09-10 09:24:10 
2024-09-10 09:24:10 Inference time: 2.03 seconds
2024-09-10 09:24:10 Tokens per second: 246.66
2024-09-10 09:24:10 ---------------
2024-09-10 09:24:12 Sample 9:
2024-09-10 09:24:12  and from thy surprise;
2024-09-10 09:24:12 The bloody prince, closed her heart, and even now,
2024-09-10 09:24:12 Drawn death, and thou wert born to be mad!
2024-09-10 09:24:12 
2024-09-10 09:24:12 QUEEN MARGARET:
2024-09-10 09:24:12 Well, and so I tender now to speak.
2024-09-10 09:24:12 
2024-09-10 09:24:12 KING HENRY VI:
2024-09-10 09:24:12 I tell thee, ere thou hadst leave in thy tongue
2024-09-10 09:24:12 And tell that first and swear what thou hast sounded:
2024-09-10 09:24:12 Then be it executed, and the rest soldiers of thee.
2024-09-10 09:24:12 
2024-09-10 09:24:12 QUEEN MARGARET:
2024-09-10 09:24:12 I will not hear that bears may entail your way,
2024-09-10 09:24:12 To strike the father with that thou art all his.
2024-09-10 09:24:12 
2024-09-10 09:24:12 KING RICHARD III:
2024-09-10 09:24:12 A horse! you have minded a 
2024-09-10 09:24:12 Inference time: 2.15 seconds
2024-09-10 09:24:12 Tokens per second: 232.24
2024-09-10 09:24:12 ---------------
2024-09-10 09:24:15 Sample 10:
2024-09-10 09:24:15  nor grace.
2024-09-10 09:24:15 
2024-09-10 09:24:15 BUCKINGHAM:
2024-09-10 09:24:15 My lord, I have done some supposed
2024-09-10 09:24:15 That you may change this present country's earth,
2024-09-10 09:24:15 In your image seems shall be or no,
2024-09-10 09:24:15 Lest you ancient to buy and biddings me seem
2024-09-10 09:24:15 Better to pass what is bound with our country's were
2024-09-10 09:24:15 As we are, and then were charged on the moon,
2024-09-10 09:24:15 The warlike is full of fame,--
2024-09-10 09:24:15 For the noble moon lord,--as the last world,
2024-09-10 09:24:15 That more breath the house of Lancaster husband,
2024-09-10 09:24:15 That the great Bona that leaves us all hand;
2024-09-10 09:24:15 And then let us enter tell him have left 
2024-09-10 09:24:15 Inference time: 2.05 seconds
2024-09-10 09:24:15 Tokens per second: 243.90
2024-09-10 09:24:15 ---------------
2024-09-10 09:24:15 Average tokens per second: 244.92
2024-09-10 09:24:15 tokens per iteration will be: 8,192
2024-09-10 09:24:15 found vocab_size = 205 (inside ../../data/enwik8/meta.pkl)
2024-09-10 09:24:15 Initializing a new model from scratch
2024-09-10 09:24:15 number of parameters: 10.70M
2024-09-10 09:24:15 num decayed parameter tensors: 26, with 10,793,856 parameters
2024-09-10 09:24:15 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 09:24:15 using fused AdamW: True
2024-09-10 09:24:15 compiling the model... (takes a ~minute)
2024-09-10 09:24:31 step 0: train loss 5.3185, val loss 5.3184
2024-09-10 09:24:52 iter 0: loss 5.3229, time 37455.07ms
2024-09-10 09:24:56 iter 100: loss 2.8816, time 43.44ms
2024-09-10 09:25:01 iter 200: loss 2.6489, time 43.52ms
2024-09-10 09:25:05 iter 300: loss 2.5466, time 43.71ms
2024-09-10 09:25:10 iter 400: loss 2.4117, time 43.94ms
2024-09-10 09:25:14 iter 500: loss 2.3453, time 43.60ms
2024-09-10 09:25:18 iter 600: loss 2.1668, time 44.09ms
2024-09-10 09:25:23 iter 700: loss 2.1499, time 43.89ms
2024-09-10 09:25:27 iter 800: loss 2.0716, time 44.41ms
2024-09-10 09:25:32 iter 900: loss 1.9538, time 44.38ms
2024-09-10 09:25:42 step 1000: train loss 1.8121, val loss 1.8179
2024-09-10 09:25:42 iter 1000: loss 1.9602, time 5790.28ms
2024-09-10 09:25:46 iter 1100: loss 1.8829, time 44.90ms
2024-09-10 09:25:50 iter 1200: loss 1.8408, time 44.70ms
2024-09-10 09:25:55 iter 1300: loss 1.8053, time 46.10ms
2024-09-10 09:25:59 iter 1400: loss 1.6560, time 45.78ms
2024-09-10 09:26:04 iter 1500: loss 1.7775, time 44.53ms
2024-09-10 09:26:08 iter 1600: loss 1.6240, time 44.66ms
2024-09-10 09:26:13 iter 1700: loss 1.6862, time 45.24ms
2024-09-10 09:26:17 iter 1800: loss 1.5785, time 47.11ms
2024-09-10 09:26:22 iter 1900: loss 1.5479, time 44.66ms
2024-09-10 09:26:32 step 2000: train loss 1.4638, val loss 1.4696
2024-09-10 09:26:32 iter 2000: loss 1.5454, time 5814.95ms
2024-09-10 09:26:37 iter 2100: loss 1.5797, time 45.16ms
2024-09-10 09:26:41 iter 2200: loss 1.4516, time 45.08ms
2024-09-10 09:26:46 iter 2300: loss 1.3869, time 45.17ms
2024-09-10 09:26:50 iter 2400: loss 1.5146, time 45.19ms
2024-09-10 09:26:55 iter 2500: loss 1.4812, time 44.99ms
2024-09-10 09:26:59 iter 2600: loss 1.5118, time 44.75ms
2024-09-10 09:27:04 iter 2700: loss 1.4566, time 44.85ms
2024-09-10 09:27:08 iter 2800: loss 1.5016, time 45.35ms
2024-09-10 09:27:13 iter 2900: loss 1.4900, time 44.95ms
2024-09-10 09:27:23 step 3000: train loss 1.3425, val loss 1.3584
2024-09-10 09:27:23 iter 3000: loss 1.4480, time 5824.30ms
2024-09-10 09:27:27 iter 3100: loss 1.4237, time 45.05ms
2024-09-10 09:27:32 iter 3200: loss 1.3096, time 45.06ms
2024-09-10 09:27:36 iter 3300: loss 1.4076, time 45.36ms
2024-09-10 09:27:41 iter 3400: loss 1.4014, time 45.58ms
2024-09-10 09:27:45 iter 3500: loss 1.3795, time 45.22ms
2024-09-10 09:27:50 iter 3600: loss 1.3431, time 45.19ms
2024-09-10 09:27:55 iter 3700: loss 1.4211, time 45.66ms
2024-09-10 09:27:59 iter 3800: loss 1.4061, time 45.23ms
2024-09-10 09:28:04 iter 3900: loss 1.3895, time 45.54ms
2024-09-10 09:28:14 step 4000: train loss 1.2902, val loss 1.2982
2024-09-10 09:28:14 iter 4000: loss 1.3330, time 5834.87ms
2024-09-10 09:28:18 iter 4100: loss 1.3610, time 45.34ms
2024-09-10 09:28:23 iter 4200: loss 1.2642, time 45.30ms
2024-09-10 09:28:27 iter 4300: loss 1.3468, time 46.72ms
2024-09-10 09:28:32 iter 4400: loss 1.4277, time 45.14ms
2024-09-10 09:28:36 iter 4500: loss 1.3028, time 45.35ms
2024-09-10 09:28:41 iter 4600: loss 1.3807, time 44.05ms
2024-09-10 09:28:46 iter 4700: loss 1.3745, time 45.35ms
2024-09-10 09:28:50 iter 4800: loss 1.3637, time 45.29ms
2024-09-10 09:28:55 iter 4900: loss 1.2274, time 46.30ms
2024-09-10 09:29:05 step 5000: train loss 1.2648, val loss 1.2673
2024-09-10 09:29:05 iter 5000: loss 1.4075, time 5845.54ms
2024-09-10 09:29:09 iter 5100: loss 1.3327, time 45.35ms
2024-09-10 09:29:14 iter 5200: loss 1.2743, time 45.41ms
2024-09-10 09:29:18 iter 5300: loss 1.3202, time 46.11ms
2024-09-10 09:29:23 iter 5400: loss 1.3156, time 45.31ms
2024-09-10 09:29:27 iter 5500: loss 1.3834, time 45.21ms
2024-09-10 09:29:32 iter 5600: loss 1.2627, time 45.23ms
2024-09-10 09:29:37 iter 5700: loss 1.3086, time 45.16ms
2024-09-10 09:29:41 iter 5800: loss 1.2887, time 45.20ms
2024-09-10 09:29:46 iter 5900: loss 1.3549, time 45.06ms
2024-09-10 09:29:56 step 6000: train loss 1.2282, val loss 1.2381
2024-09-10 09:29:56 iter 6000: loss 1.2722, time 5856.49ms
2024-09-10 09:30:00 iter 6100: loss 1.3425, time 44.97ms
2024-09-10 09:30:05 iter 6200: loss 1.3649, time 45.03ms
2024-09-10 09:30:09 iter 6300: loss 1.2047, time 45.98ms
2024-09-10 09:30:14 iter 6400: loss 1.2936, time 45.14ms
2024-09-10 09:30:19 iter 6500: loss 1.3239, time 45.39ms
2024-09-10 09:30:23 iter 6600: loss 1.2763, time 45.13ms
2024-09-10 09:30:28 iter 6700: loss 1.3563, time 45.29ms
2024-09-10 09:30:32 iter 6800: loss 1.2979, time 45.41ms
2024-09-10 09:30:37 iter 6900: loss 1.3050, time 45.45ms
2024-09-10 09:30:47 step 7000: train loss 1.2141, val loss 1.2293
2024-09-10 09:30:47 iter 7000: loss 1.2686, time 5834.48ms
2024-09-10 09:30:51 iter 7100: loss 1.2501, time 45.86ms
2024-09-10 09:30:56 iter 7200: loss 1.1863, time 45.70ms
2024-09-10 09:31:01 iter 7300: loss 1.3340, time 45.25ms
2024-09-10 09:31:05 iter 7400: loss 1.2612, time 45.20ms
2024-09-10 09:31:10 iter 7500: loss 1.3526, time 45.75ms
2024-09-10 09:31:14 iter 7600: loss 1.2525, time 45.75ms
2024-09-10 09:31:19 iter 7700: loss 1.2308, time 45.73ms
2024-09-10 09:31:23 iter 7800: loss 1.3181, time 45.31ms
2024-09-10 09:31:28 iter 7900: loss 1.2685, time 45.25ms
2024-09-10 09:31:38 step 8000: train loss 1.1992, val loss 1.2083
2024-09-10 09:31:38 iter 8000: loss 1.3171, time 5826.91ms
2024-09-10 09:31:43 iter 8100: loss 1.2437, time 45.31ms
2024-09-10 09:31:47 iter 8200: loss 1.2323, time 45.26ms
2024-09-10 09:31:52 iter 8300: loss 1.2716, time 45.48ms
2024-09-10 09:31:56 iter 8400: loss 1.1973, time 45.79ms
2024-09-10 09:32:01 iter 8500: loss 1.3202, time 45.62ms
2024-09-10 09:32:05 iter 8600: loss 1.2110, time 45.19ms
2024-09-10 09:32:10 iter 8700: loss 1.2259, time 45.30ms
2024-09-10 09:32:14 iter 8800: loss 1.2833, time 45.35ms
2024-09-10 09:32:19 iter 8900: loss 1.1898, time 45.39ms
2024-09-10 09:32:29 step 9000: train loss 1.1832, val loss 1.1950
2024-09-10 09:32:29 iter 9000: loss 1.2152, time 5836.12ms
2024-09-10 09:32:34 iter 9100: loss 1.2073, time 45.13ms
2024-09-10 09:32:38 iter 9200: loss 1.1869, time 45.37ms
2024-09-10 09:32:43 iter 9300: loss 1.2123, time 45.00ms
2024-09-10 09:32:47 iter 9400: loss 1.2096, time 45.30ms
2024-09-10 09:32:52 iter 9500: loss 1.2274, time 45.35ms
2024-09-10 09:32:56 iter 9600: loss 1.3061, time 45.74ms
2024-09-10 09:33:01 iter 9700: loss 1.2667, time 45.31ms
2024-09-10 09:33:05 iter 9800: loss 1.2522, time 45.21ms
2024-09-10 09:33:10 iter 9900: loss 1.2313, time 45.30ms
2024-09-10 09:33:20 step 10000: train loss 1.1706, val loss 1.1893
2024-09-10 09:33:20 iter 10000: loss 1.2501, time 5850.16ms
2024-09-10 09:33:25 iter 10100: loss 1.2889, time 45.38ms
2024-09-10 09:33:29 iter 10200: loss 1.2187, time 45.34ms
2024-09-10 09:33:34 iter 10300: loss 1.2511, time 45.06ms
2024-09-10 09:33:38 iter 10400: loss 1.2384, time 45.22ms
2024-09-10 09:33:43 iter 10500: loss 1.2626, time 45.40ms
2024-09-10 09:33:47 iter 10600: loss 1.2132, time 45.42ms
2024-09-10 09:33:52 iter 10700: loss 1.1916, time 45.40ms
2024-09-10 09:33:56 iter 10800: loss 1.2607, time 45.22ms
2024-09-10 09:34:01 iter 10900: loss 1.2587, time 45.36ms
2024-09-10 09:34:11 step 11000: train loss 1.1682, val loss 1.1801
2024-09-10 09:34:11 iter 11000: loss 1.2876, time 5837.57ms
2024-09-10 09:34:16 iter 11100: loss 1.1619, time 45.88ms
2024-09-10 09:34:20 iter 11200: loss 1.1788, time 45.52ms
2024-09-10 09:34:25 iter 11300: loss 1.2927, time 45.34ms
2024-09-10 09:34:29 iter 11400: loss 1.2156, time 44.40ms
2024-09-10 09:34:34 iter 11500: loss 1.2593, time 45.76ms
2024-09-10 09:34:38 iter 11600: loss 1.2208, time 45.51ms
2024-09-10 09:34:43 iter 11700: loss 1.2320, time 45.46ms
2024-09-10 09:34:47 iter 11800: loss 1.2912, time 44.93ms
2024-09-10 09:34:52 iter 11900: loss 1.2424, time 45.04ms
2024-09-10 09:35:02 step 12000: train loss 1.1597, val loss 1.1777
2024-09-10 09:35:02 iter 12000: loss 1.2839, time 5841.27ms
2024-09-10 09:35:07 iter 12100: loss 1.2016, time 45.19ms
2024-09-10 09:35:11 iter 12200: loss 1.2636, time 45.61ms
2024-09-10 09:35:16 iter 12300: loss 1.1756, time 45.52ms
2024-09-10 09:35:20 iter 12400: loss 1.2131, time 45.24ms
2024-09-10 09:35:25 iter 12500: loss 1.1424, time 45.23ms
2024-09-10 09:35:29 iter 12600: loss 1.2196, time 45.27ms
2024-09-10 09:35:34 iter 12700: loss 1.1193, time 45.28ms
2024-09-10 09:35:38 iter 12800: loss 1.2194, time 45.27ms
2024-09-10 09:35:43 iter 12900: loss 1.2252, time 45.25ms
2024-09-10 09:35:53 step 13000: train loss 1.1503, val loss 1.1650
2024-09-10 09:35:53 iter 13000: loss 1.1737, time 5834.89ms
2024-09-10 09:35:58 iter 13100: loss 1.2025, time 45.19ms
2024-09-10 09:36:02 iter 13200: loss 1.1613, time 45.26ms
2024-09-10 09:36:07 iter 13300: loss 1.2488, time 45.08ms
2024-09-10 09:36:11 iter 13400: loss 1.2350, time 45.58ms
2024-09-10 09:36:16 iter 13500: loss 1.1753, time 45.32ms
2024-09-10 09:36:20 iter 13600: loss 1.1676, time 45.59ms
2024-09-10 09:36:25 iter 13700: loss 1.2315, time 45.16ms
2024-09-10 09:36:30 iter 13800: loss 1.1551, time 45.41ms
2024-09-10 09:36:34 iter 13900: loss 1.2126, time 45.43ms
2024-09-10 09:36:44 step 14000: train loss 1.1401, val loss 1.1610
2024-09-10 09:36:44 iter 14000: loss 1.1745, time 5826.49ms
2024-09-10 09:36:49 iter 14100: loss 1.2144, time 44.98ms
2024-09-10 09:36:53 iter 14200: loss 1.1582, time 45.31ms
2024-09-10 09:36:58 iter 14300: loss 1.2533, time 45.35ms
2024-09-10 09:37:02 iter 14400: loss 1.1924, time 45.25ms
2024-09-10 09:37:07 iter 14500: loss 1.2370, time 45.33ms
2024-09-10 09:37:12 iter 14600: loss 1.2305, time 45.13ms
2024-09-10 09:37:16 iter 14700: loss 1.1575, time 43.73ms
2024-09-10 09:37:21 iter 14800: loss 1.1546, time 45.59ms
2024-09-10 09:37:25 iter 14900: loss 1.2009, time 45.12ms
2024-09-10 09:37:35 step 15000: train loss 1.1388, val loss 1.1504
2024-09-10 09:37:35 iter 15000: loss 1.2699, time 5837.55ms
2024-09-10 09:37:40 iter 15100: loss 1.2173, time 45.16ms
2024-09-10 09:37:44 iter 15200: loss 1.2226, time 45.63ms
2024-09-10 09:37:49 iter 15300: loss 1.2107, time 45.09ms
2024-09-10 09:37:53 iter 15400: loss 1.2290, time 45.28ms
2024-09-10 09:37:58 iter 15500: loss 1.1872, time 45.23ms
2024-09-10 09:38:03 iter 15600: loss 1.1772, time 45.22ms
2024-09-10 09:38:07 iter 15700: loss 1.1536, time 45.20ms
2024-09-10 09:38:12 iter 15800: loss 1.1926, time 45.36ms
2024-09-10 09:38:16 iter 15900: loss 1.1170, time 45.30ms
2024-09-10 09:38:26 step 16000: train loss 1.1319, val loss 1.1612
2024-09-10 09:38:26 iter 16000: loss 1.1982, time 5829.08ms
2024-09-10 09:38:31 iter 16100: loss 1.1991, time 45.19ms
2024-09-10 09:38:35 iter 16200: loss 1.1546, time 45.33ms
2024-09-10 09:38:40 iter 16300: loss 1.1753, time 45.57ms
2024-09-10 09:38:45 iter 16400: loss 1.1542, time 45.58ms
2024-09-10 09:38:49 iter 16500: loss 1.2456, time 45.81ms
2024-09-10 09:38:54 iter 16600: loss 1.2048, time 45.69ms
2024-09-10 09:38:58 iter 16700: loss 1.1571, time 45.38ms
2024-09-10 09:39:03 iter 16800: loss 1.2332, time 45.62ms
2024-09-10 09:39:07 iter 16900: loss 1.1921, time 46.11ms
2024-09-10 09:39:17 step 17000: train loss 1.1218, val loss 1.1452
2024-09-10 09:39:18 iter 17000: loss 1.1735, time 5830.91ms
2024-09-10 09:39:22 iter 17100: loss 1.1526, time 45.27ms
2024-09-10 09:39:27 iter 17200: loss 1.1694, time 45.22ms
2024-09-10 09:39:31 iter 17300: loss 1.2773, time 44.96ms
2024-09-10 09:39:36 iter 17400: loss 1.1787, time 45.00ms
2024-09-10 09:39:40 iter 17500: loss 1.2024, time 45.18ms
2024-09-10 09:39:45 iter 17600: loss 1.2480, time 45.52ms
2024-09-10 09:39:49 iter 17700: loss 1.2175, time 45.10ms
2024-09-10 09:39:54 iter 17800: loss 1.2312, time 45.18ms
2024-09-10 09:39:58 iter 17900: loss 1.1810, time 45.35ms
2024-09-10 09:40:09 step 18000: train loss 1.1215, val loss 1.1468
2024-09-10 09:40:09 iter 18000: loss 1.2129, time 5833.95ms
2024-09-10 09:40:13 iter 18100: loss 1.2145, time 45.39ms
2024-09-10 09:40:18 iter 18200: loss 1.2156, time 45.63ms
2024-09-10 09:40:22 iter 18300: loss 1.1838, time 45.66ms
2024-09-10 09:40:27 iter 18400: loss 1.2609, time 45.68ms
2024-09-10 09:40:31 iter 18500: loss 1.2586, time 45.22ms
2024-09-10 09:40:36 iter 18600: loss 1.2636, time 45.22ms
2024-09-10 09:40:40 iter 18700: loss 1.2294, time 45.21ms
2024-09-10 09:40:45 iter 18800: loss 1.1454, time 45.30ms
2024-09-10 09:40:49 iter 18900: loss 1.2660, time 45.19ms
2024-09-10 09:41:00 step 19000: train loss 1.1181, val loss 1.1365
2024-09-10 09:41:00 iter 19000: loss 1.1847, time 5813.15ms
2024-09-10 09:41:04 iter 19100: loss 1.1145, time 45.25ms
2024-09-10 09:41:09 iter 19200: loss 1.1431, time 45.48ms
2024-09-10 09:41:13 iter 19300: loss 1.2232, time 45.52ms
2024-09-10 09:41:18 iter 19400: loss 1.1618, time 45.38ms
2024-09-10 09:41:22 iter 19500: loss 1.2446, time 44.97ms
2024-09-10 09:41:27 iter 19600: loss 1.1598, time 45.54ms
2024-09-10 09:41:31 iter 19700: loss 1.1771, time 45.59ms
2024-09-10 09:41:36 iter 19800: loss 1.2016, time 45.13ms
2024-09-10 09:41:40 iter 19900: loss 1.1158, time 45.45ms
2024-09-10 09:41:51 step 20000: train loss 1.1130, val loss 1.1319
2024-09-10 09:41:51 iter 20000: loss 1.1661, time 5838.77ms
2024-09-10 09:41:55 iter 20100: loss 1.1777, time 45.22ms
2024-09-10 09:42:00 iter 20200: loss 1.2021, time 45.19ms
2024-09-10 09:42:04 iter 20300: loss 1.1351, time 45.21ms
2024-09-10 09:42:09 iter 20400: loss 1.2119, time 45.44ms
2024-09-10 09:42:13 iter 20500: loss 1.1598, time 45.63ms
2024-09-10 09:42:18 iter 20600: loss 1.0978, time 45.49ms
2024-09-10 09:42:22 iter 20700: loss 1.2006, time 45.96ms
2024-09-10 09:42:27 iter 20800: loss 1.1787, time 45.57ms
2024-09-10 09:42:32 iter 20900: loss 1.2115, time 45.33ms
2024-09-10 09:42:42 step 21000: train loss 1.1142, val loss 1.1345
2024-09-10 09:42:42 iter 21000: loss 1.1992, time 5828.44ms
2024-09-10 09:42:46 iter 21100: loss 1.1963, time 45.59ms
2024-09-10 09:42:51 iter 21200: loss 1.2511, time 45.27ms
2024-09-10 09:42:55 iter 21300: loss 1.2222, time 45.80ms
2024-09-10 09:43:00 iter 21400: loss 1.1150, time 45.32ms
2024-09-10 09:43:04 iter 21500: loss 1.1375, time 45.12ms
2024-09-10 09:43:09 iter 21600: loss 1.1448, time 45.24ms
2024-09-10 09:43:14 iter 21700: loss 1.1751, time 45.70ms
2024-09-10 09:43:18 iter 21800: loss 1.0640, time 45.63ms
2024-09-10 09:43:23 iter 21900: loss 1.2477, time 45.24ms
2024-09-10 09:43:33 step 22000: train loss 1.1085, val loss 1.1243
2024-09-10 09:43:33 iter 22000: loss 1.1774, time 5839.01ms
2024-09-10 09:43:37 iter 22100: loss 1.1680, time 45.58ms
2024-09-10 09:43:42 iter 22200: loss 1.2215, time 45.42ms
2024-09-10 09:43:47 iter 22300: loss 1.1542, time 45.25ms
2024-09-10 09:43:51 iter 22400: loss 1.1672, time 45.50ms
2024-09-10 09:43:56 iter 22500: loss 1.1455, time 45.17ms
2024-09-10 09:44:00 iter 22600: loss 1.1637, time 45.37ms
2024-09-10 09:44:05 iter 22700: loss 1.1230, time 45.71ms
2024-09-10 09:44:09 iter 22800: loss 1.1621, time 45.36ms
2024-09-10 09:44:14 iter 22900: loss 1.1855, time 45.67ms
2024-09-10 09:44:24 step 23000: train loss 1.1065, val loss 1.1258
2024-09-10 09:44:24 iter 23000: loss 1.2276, time 5880.98ms
2024-09-10 09:44:29 iter 23100: loss 1.1249, time 45.29ms
2024-09-10 09:44:33 iter 23200: loss 1.0932, time 45.40ms
2024-09-10 09:44:38 iter 23300: loss 1.0631, time 45.69ms
2024-09-10 09:44:42 iter 23400: loss 1.1817, time 45.34ms
2024-09-10 09:44:47 iter 23500: loss 1.2020, time 45.22ms
2024-09-10 09:44:51 iter 23600: loss 1.0722, time 45.67ms
2024-09-10 09:44:56 iter 23700: loss 1.1728, time 45.71ms
2024-09-10 09:45:00 iter 23800: loss 1.1387, time 45.71ms
2024-09-10 09:45:05 iter 23900: loss 1.1243, time 45.03ms
2024-09-10 09:45:15 step 24000: train loss 1.1015, val loss 1.1208
2024-09-10 09:45:15 iter 24000: loss 1.0995, time 5835.98ms
2024-09-10 09:45:20 iter 24100: loss 1.1416, time 45.58ms
2024-09-10 09:45:24 iter 24200: loss 1.1796, time 45.72ms
2024-09-10 09:45:29 iter 24300: loss 1.1859, time 45.25ms
2024-09-10 09:45:33 iter 24400: loss 1.2622, time 45.70ms
2024-09-10 09:45:38 iter 24500: loss 1.1261, time 45.11ms
2024-09-10 09:45:43 iter 24600: loss 1.1248, time 45.61ms
2024-09-10 09:45:47 iter 24700: loss 1.1755, time 45.10ms
2024-09-10 09:45:52 iter 24800: loss 1.2042, time 45.38ms
2024-09-10 09:45:56 iter 24900: loss 1.1918, time 45.55ms
2024-09-10 09:46:06 step 25000: train loss 1.0974, val loss 1.1190
2024-09-10 09:46:06 iter 25000: loss 1.1489, time 5831.83ms
2024-09-10 09:46:11 iter 25100: loss 1.1815, time 45.50ms
2024-09-10 09:46:16 iter 25200: loss 1.2125, time 45.23ms
2024-09-10 09:46:20 iter 25300: loss 1.0556, time 45.30ms
2024-09-10 09:46:25 iter 25400: loss 1.2105, time 45.71ms
2024-09-10 09:46:29 iter 25500: loss 1.1659, time 45.26ms
2024-09-10 09:46:34 iter 25600: loss 1.2206, time 45.66ms
2024-09-10 09:46:38 iter 25700: loss 1.1868, time 45.24ms
2024-09-10 09:46:43 iter 25800: loss 1.1122, time 45.32ms
2024-09-10 09:46:47 iter 25900: loss 1.1199, time 45.15ms
2024-09-10 09:46:58 step 26000: train loss 1.0917, val loss 1.1190
2024-09-10 09:46:58 iter 26000: loss 1.2382, time 5840.80ms
2024-09-10 09:47:02 iter 26100: loss 1.1253, time 45.42ms
2024-09-10 09:47:07 iter 26200: loss 1.1421, time 45.49ms
2024-09-10 09:47:11 iter 26300: loss 1.1648, time 45.65ms
2024-09-10 09:47:16 iter 26400: loss 1.1325, time 45.28ms
2024-09-10 09:47:20 iter 26500: loss 1.1593, time 45.26ms
2024-09-10 09:47:25 iter 26600: loss 1.2054, time 45.47ms
2024-09-10 09:47:29 iter 26700: loss 1.0528, time 45.02ms
2024-09-10 09:47:34 iter 26800: loss 1.0776, time 45.57ms
2024-09-10 09:47:38 iter 26900: loss 1.1755, time 45.28ms
2024-09-10 09:47:49 step 27000: train loss 1.0980, val loss 1.1149
2024-09-10 09:47:49 iter 27000: loss 1.1985, time 5840.32ms
2024-09-10 09:47:53 iter 27100: loss 1.1417, time 45.47ms
2024-09-10 09:47:58 iter 27200: loss 1.1780, time 45.28ms
2024-09-10 09:48:02 iter 27300: loss 1.1127, time 45.17ms
2024-09-10 09:48:07 iter 27400: loss 1.1236, time 45.49ms
2024-09-10 09:48:11 iter 27500: loss 1.1662, time 45.61ms
2024-09-10 09:48:16 iter 27600: loss 1.1672, time 45.26ms
2024-09-10 09:48:21 iter 27700: loss 1.1522, time 45.04ms
2024-09-10 09:48:25 iter 27800: loss 1.1177, time 45.33ms
2024-09-10 09:48:30 iter 27900: loss 1.1874, time 44.27ms
2024-09-10 09:48:40 step 28000: train loss 1.0861, val loss 1.1067
2024-09-10 09:48:40 iter 28000: loss 1.1562, time 5842.62ms
2024-09-10 09:48:44 iter 28100: loss 1.0671, time 44.61ms
2024-09-10 09:48:49 iter 28200: loss 1.0975, time 45.18ms
2024-09-10 09:48:53 iter 28300: loss 1.1501, time 45.65ms
2024-09-10 09:48:58 iter 28400: loss 1.2320, time 45.71ms
2024-09-10 09:49:03 iter 28500: loss 1.0702, time 45.52ms
2024-09-10 09:49:07 iter 28600: loss 1.1471, time 44.12ms
2024-09-10 09:49:12 iter 28700: loss 1.1348, time 46.73ms
2024-09-10 09:49:16 iter 28800: loss 1.1443, time 45.36ms
2024-09-10 09:49:21 iter 28900: loss 1.0623, time 44.57ms
2024-09-10 09:49:31 step 29000: train loss 1.0807, val loss 1.1060
2024-09-10 09:49:31 iter 29000: loss 1.1318, time 5846.86ms
2024-09-10 09:49:36 iter 29100: loss 1.1423, time 46.63ms
2024-09-10 09:49:40 iter 29200: loss 1.0763, time 45.60ms
2024-09-10 09:49:45 iter 29300: loss 1.1467, time 45.01ms
2024-09-10 09:49:49 iter 29400: loss 1.1431, time 45.65ms
2024-09-10 09:49:54 iter 29500: loss 1.1406, time 45.19ms
2024-09-10 09:49:58 iter 29600: loss 1.0698, time 45.07ms
2024-09-10 09:50:03 iter 29700: loss 1.1400, time 45.73ms
2024-09-10 09:50:08 iter 29800: loss 1.1720, time 45.78ms
2024-09-10 09:50:12 iter 29900: loss 1.1865, time 45.18ms
2024-09-10 09:50:22 step 30000: train loss 1.0819, val loss 1.1028
2024-09-10 09:50:22 iter 30000: loss 1.1005, time 5836.03ms
2024-09-10 09:50:27 iter 30100: loss 1.0701, time 45.11ms
2024-09-10 09:50:31 iter 30200: loss 1.1135, time 45.68ms
2024-09-10 09:50:36 iter 30300: loss 1.1682, time 45.69ms
2024-09-10 09:50:40 iter 30400: loss 1.1602, time 45.74ms
2024-09-10 09:50:45 iter 30500: loss 1.1790, time 45.19ms
2024-09-10 09:50:50 iter 30600: loss 1.1678, time 45.33ms
2024-09-10 09:50:54 iter 30700: loss 1.1798, time 45.16ms
2024-09-10 09:50:59 iter 30800: loss 1.2226, time 45.59ms
2024-09-10 09:51:03 iter 30900: loss 1.0729, time 45.34ms
2024-09-10 09:51:13 step 31000: train loss 1.0818, val loss 1.1029
2024-09-10 09:51:13 iter 31000: loss 1.1155, time 5819.90ms
2024-09-10 09:51:18 iter 31100: loss 1.1266, time 45.70ms
2024-09-10 09:51:22 iter 31200: loss 1.1526, time 45.64ms
2024-09-10 09:51:27 iter 31300: loss 1.1386, time 45.87ms
2024-09-10 09:51:32 iter 31400: loss 1.2174, time 45.28ms
2024-09-10 09:51:36 iter 31500: loss 1.1745, time 45.13ms
2024-09-10 09:51:41 iter 31600: loss 1.1126, time 45.47ms
2024-09-10 09:51:45 iter 31700: loss 1.1055, time 45.12ms
2024-09-10 09:51:50 iter 31800: loss 1.1737, time 45.33ms
2024-09-10 09:51:54 iter 31900: loss 1.1339, time 45.34ms
2024-09-10 09:52:05 step 32000: train loss 1.0771, val loss 1.0997
2024-09-10 09:52:05 iter 32000: loss 1.0873, time 5969.20ms
2024-09-10 09:52:09 iter 32100: loss 1.1143, time 45.74ms
2024-09-10 09:52:14 iter 32200: loss 1.1169, time 45.05ms
2024-09-10 09:52:19 iter 32300: loss 1.1083, time 45.40ms
2024-09-10 09:52:23 iter 32400: loss 1.1179, time 46.83ms
2024-09-10 09:52:28 iter 32500: loss 1.1656, time 45.34ms
2024-09-10 09:52:32 iter 32600: loss 1.1023, time 47.56ms
2024-09-10 09:52:37 iter 32700: loss 1.1334, time 46.19ms
2024-09-10 09:52:42 iter 32800: loss 1.1485, time 47.15ms
2024-09-10 09:52:46 iter 32900: loss 1.2479, time 46.98ms
2024-09-10 09:52:57 step 33000: train loss 1.0745, val loss 1.1024
2024-09-10 09:52:57 iter 33000: loss 1.0526, time 5993.67ms
2024-09-10 09:53:01 iter 33100: loss 1.0311, time 45.18ms
2024-09-10 09:53:06 iter 33200: loss 1.1329, time 46.43ms
2024-09-10 09:53:11 iter 33300: loss 1.1420, time 45.63ms
2024-09-10 09:53:15 iter 33400: loss 1.1178, time 45.53ms
2024-09-10 09:53:20 iter 33500: loss 1.1214, time 45.28ms
2024-09-10 09:53:24 iter 33600: loss 1.1191, time 45.17ms
2024-09-10 09:53:29 iter 33700: loss 1.1114, time 43.94ms
2024-09-10 09:53:33 iter 33800: loss 1.1516, time 45.74ms
2024-09-10 09:53:38 iter 33900: loss 1.1259, time 43.79ms
2024-09-10 09:53:49 step 34000: train loss 1.0726, val loss 1.0920
2024-09-10 09:53:49 iter 34000: loss 1.1257, time 6548.29ms
2024-09-10 09:53:53 iter 34100: loss 1.0995, time 45.81ms
2024-09-10 09:53:58 iter 34200: loss 1.1353, time 45.94ms
2024-09-10 09:54:03 iter 34300: loss 1.1515, time 45.46ms
2024-09-10 09:54:07 iter 34400: loss 1.0929, time 45.07ms
2024-09-10 09:54:12 iter 34500: loss 1.1479, time 46.02ms
2024-09-10 09:54:16 iter 34600: loss 1.1592, time 44.71ms
2024-09-10 09:54:21 iter 34700: loss 1.0928, time 45.56ms
2024-09-10 09:54:25 iter 34800: loss 1.1231, time 46.11ms
2024-09-10 09:54:30 iter 34900: loss 1.0891, time 46.36ms
2024-09-10 09:54:41 step 35000: train loss 1.0696, val loss 1.0951
2024-09-10 09:54:41 iter 35000: loss 1.1720, time 6544.84ms
2024-09-10 09:54:46 iter 35100: loss 1.1168, time 45.27ms
2024-09-10 09:54:50 iter 35200: loss 1.1598, time 46.21ms
2024-09-10 09:54:55 iter 35300: loss 1.1490, time 45.61ms
2024-09-10 09:54:59 iter 35400: loss 1.1513, time 45.93ms
2024-09-10 09:55:04 iter 35500: loss 1.1414, time 45.61ms
2024-09-10 09:55:08 iter 35600: loss 1.1264, time 45.73ms
2024-09-10 09:55:13 iter 35700: loss 1.1052, time 43.82ms
2024-09-10 09:55:18 iter 35800: loss 1.1096, time 45.84ms
2024-09-10 09:55:22 iter 35900: loss 1.0787, time 44.40ms
2024-09-10 09:55:33 step 36000: train loss 1.0639, val loss 1.0911
2024-09-10 09:55:33 iter 36000: loss 1.0810, time 6548.30ms
2024-09-10 09:55:38 iter 36100: loss 1.1134, time 45.93ms
2024-09-10 09:55:42 iter 36200: loss 1.1974, time 47.57ms
2024-09-10 09:55:47 iter 36300: loss 1.0796, time 42.27ms
2024-09-10 09:55:51 iter 36400: loss 1.1872, time 45.53ms
2024-09-10 09:55:56 iter 36500: loss 1.0592, time 46.47ms
2024-09-10 09:56:01 iter 36600: loss 1.1341, time 46.11ms
2024-09-10 09:56:05 iter 36700: loss 1.1330, time 45.79ms
2024-09-10 09:56:10 iter 36800: loss 1.2055, time 45.73ms
2024-09-10 09:56:14 iter 36900: loss 1.0612, time 45.73ms
2024-09-10 09:56:25 step 37000: train loss 1.0682, val loss 1.0956
2024-09-10 09:56:25 iter 37000: loss 1.1475, time 5894.42ms
2024-09-10 09:56:29 iter 37100: loss 1.1502, time 45.93ms
2024-09-10 09:56:34 iter 37200: loss 1.2030, time 46.05ms
2024-09-10 09:56:39 iter 37300: loss 1.0874, time 45.71ms
2024-09-10 09:56:43 iter 37400: loss 1.1142, time 46.07ms
2024-09-10 09:56:48 iter 37500: loss 1.1456, time 46.08ms
2024-09-10 09:56:52 iter 37600: loss 1.0824, time 45.81ms
2024-09-10 09:56:57 iter 37700: loss 1.1299, time 45.53ms
2024-09-10 09:57:01 iter 37800: loss 1.1018, time 45.52ms
2024-09-10 09:57:06 iter 37900: loss 1.0949, time 45.95ms
2024-09-10 09:57:16 step 38000: train loss 1.0647, val loss 1.0914
2024-09-10 09:57:16 iter 38000: loss 1.1027, time 5901.80ms
2024-09-10 09:57:21 iter 38100: loss 1.1203, time 45.39ms
2024-09-10 09:57:26 iter 38200: loss 1.1816, time 45.28ms
2024-09-10 09:57:30 iter 38300: loss 1.1263, time 46.14ms
2024-09-10 09:57:35 iter 38400: loss 1.1625, time 45.86ms
2024-09-10 09:57:39 iter 38500: loss 1.1332, time 45.61ms
2024-09-10 09:57:44 iter 38600: loss 1.1560, time 45.78ms
2024-09-10 09:57:48 iter 38700: loss 1.1316, time 45.83ms
2024-09-10 09:57:53 iter 38800: loss 1.1584, time 45.98ms
2024-09-10 09:57:58 iter 38900: loss 1.1292, time 45.65ms
2024-09-10 09:58:08 step 39000: train loss 1.0660, val loss 1.0893
2024-09-10 09:58:08 iter 39000: loss 1.0972, time 5894.53ms
2024-09-10 09:58:13 iter 39100: loss 1.0982, time 46.15ms
2024-09-10 09:58:17 iter 39200: loss 1.0690, time 45.66ms
2024-09-10 09:58:22 iter 39300: loss 1.0887, time 45.44ms
2024-09-10 09:58:26 iter 39400: loss 1.0347, time 45.19ms
2024-09-10 09:58:31 iter 39500: loss 1.0178, time 45.85ms
2024-09-10 09:58:35 iter 39600: loss 1.1941, time 45.74ms
2024-09-10 09:58:40 iter 39700: loss 1.1068, time 46.14ms
2024-09-10 09:58:45 iter 39800: loss 1.0579, time 45.68ms
2024-09-10 09:58:49 iter 39900: loss 1.0448, time 46.09ms
2024-09-10 09:58:59 step 40000: train loss 1.0605, val loss 1.0842
2024-09-10 09:58:59 iter 40000: loss 1.0865, time 5888.26ms
2024-09-10 09:59:04 iter 40100: loss 1.1767, time 46.26ms
2024-09-10 09:59:09 iter 40200: loss 1.0683, time 45.82ms
2024-09-10 09:59:13 iter 40300: loss 1.1895, time 45.77ms
2024-09-10 09:59:18 iter 40400: loss 1.1419, time 45.71ms
2024-09-10 09:59:22 iter 40500: loss 1.1138, time 45.53ms
2024-09-10 09:59:27 iter 40600: loss 1.0539, time 45.96ms
2024-09-10 09:59:31 iter 40700: loss 1.0955, time 46.03ms
2024-09-10 09:59:36 iter 40800: loss 1.1275, time 46.15ms
2024-09-10 09:59:41 iter 40900: loss 1.1268, time 45.72ms
2024-09-10 09:59:51 step 41000: train loss 1.0540, val loss 1.0786
2024-09-10 09:59:51 iter 41000: loss 1.1545, time 5897.31ms
2024-09-10 09:59:56 iter 41100: loss 1.1946, time 45.89ms
2024-09-10 10:00:00 iter 41200: loss 1.0816, time 46.09ms
2024-09-10 10:00:05 iter 41300: loss 1.1352, time 46.17ms
2024-09-10 10:00:09 iter 41400: loss 1.2075, time 45.69ms
2024-09-10 10:00:14 iter 41500: loss 1.1784, time 46.15ms
2024-09-10 10:00:18 iter 41600: loss 1.1058, time 46.12ms
2024-09-10 10:00:23 iter 41700: loss 1.0787, time 45.37ms
2024-09-10 10:00:28 iter 41800: loss 1.1161, time 45.75ms
2024-09-10 10:00:32 iter 41900: loss 1.2042, time 45.63ms
2024-09-10 10:00:42 step 42000: train loss 1.0521, val loss 1.0800
2024-09-10 10:00:42 iter 42000: loss 1.1938, time 5877.04ms
2024-09-10 10:00:47 iter 42100: loss 1.1742, time 45.79ms
2024-09-10 10:00:52 iter 42200: loss 1.1476, time 45.51ms
2024-09-10 10:00:56 iter 42300: loss 1.1177, time 45.47ms
2024-09-10 10:01:01 iter 42400: loss 1.0985, time 45.50ms
2024-09-10 10:01:05 iter 42500: loss 1.1119, time 45.67ms
2024-09-10 10:01:10 iter 42600: loss 1.1230, time 46.04ms
2024-09-10 10:01:14 iter 42700: loss 1.1614, time 45.84ms
2024-09-10 10:01:19 iter 42800: loss 1.2180, time 45.53ms
2024-09-10 10:01:24 iter 42900: loss 1.0584, time 45.47ms
2024-09-10 10:01:34 step 43000: train loss 1.0565, val loss 1.0798
2024-09-10 10:01:34 iter 43000: loss 1.1124, time 5885.98ms
2024-09-10 10:01:38 iter 43100: loss 1.1475, time 45.75ms
2024-09-10 10:01:43 iter 43200: loss 1.1431, time 45.74ms
2024-09-10 10:01:48 iter 43300: loss 1.1751, time 45.78ms
2024-09-10 10:01:52 iter 43400: loss 1.1430, time 45.63ms
2024-09-10 10:01:57 iter 43500: loss 1.0991, time 45.23ms
2024-09-10 10:02:01 iter 43600: loss 1.0851, time 45.35ms
2024-09-10 10:02:06 iter 43700: loss 1.0720, time 45.59ms
2024-09-10 10:02:10 iter 43800: loss 1.1656, time 45.53ms
2024-09-10 10:02:15 iter 43900: loss 0.9802, time 45.69ms
2024-09-10 10:02:25 step 44000: train loss 1.0521, val loss 1.0798
2024-09-10 10:02:25 iter 44000: loss 1.0477, time 5887.31ms
2024-09-10 10:02:30 iter 44100: loss 1.1650, time 45.52ms
2024-09-10 10:02:34 iter 44200: loss 1.2565, time 45.59ms
2024-09-10 10:02:39 iter 44300: loss 1.1155, time 45.74ms
2024-09-10 10:02:44 iter 44400: loss 1.0694, time 45.70ms
2024-09-10 10:02:48 iter 44500: loss 1.1287, time 45.52ms
2024-09-10 10:02:53 iter 44600: loss 1.0627, time 45.86ms
2024-09-10 10:02:57 iter 44700: loss 1.1087, time 45.73ms
2024-09-10 10:03:02 iter 44800: loss 1.0798, time 45.72ms
2024-09-10 10:03:06 iter 44900: loss 1.1049, time 45.94ms
2024-09-10 10:03:17 step 45000: train loss 1.0500, val loss 1.0738
2024-09-10 10:03:17 iter 45000: loss 1.1986, time 5885.96ms
2024-09-10 10:03:21 iter 45100: loss 1.1362, time 45.78ms
2024-09-10 10:03:26 iter 45200: loss 1.1634, time 46.01ms
2024-09-10 10:03:30 iter 45300: loss 1.1302, time 45.53ms
2024-09-10 10:03:35 iter 45400: loss 1.0871, time 45.61ms
2024-09-10 10:03:40 iter 45500: loss 1.1036, time 45.91ms
2024-09-10 10:03:44 iter 45600: loss 1.0861, time 45.47ms
2024-09-10 10:03:49 iter 45700: loss 1.0495, time 45.62ms
2024-09-10 10:03:53 iter 45800: loss 1.0953, time 45.54ms
2024-09-10 10:03:58 iter 45900: loss 1.0552, time 46.14ms
2024-09-10 10:04:08 step 46000: train loss 1.0465, val loss 1.0753
2024-09-10 10:04:08 iter 46000: loss 1.0079, time 5884.65ms
2024-09-10 10:04:13 iter 46100: loss 1.1081, time 46.00ms
2024-09-10 10:04:17 iter 46200: loss 1.1845, time 46.08ms
2024-09-10 10:04:22 iter 46300: loss 1.1402, time 46.08ms
2024-09-10 10:04:26 iter 46400: loss 1.1014, time 45.27ms
2024-09-10 10:04:31 iter 46500: loss 1.1075, time 46.01ms
2024-09-10 10:04:36 iter 46600: loss 1.1505, time 45.65ms
2024-09-10 10:04:40 iter 46700: loss 1.1449, time 45.38ms
2024-09-10 10:04:45 iter 46800: loss 1.1368, time 45.57ms
2024-09-10 10:04:49 iter 46900: loss 1.1205, time 45.85ms
2024-09-10 10:04:59 step 47000: train loss 1.0431, val loss 1.0723
2024-09-10 10:05:00 iter 47000: loss 1.0520, time 5879.89ms
2024-09-10 10:05:04 iter 47100: loss 1.1263, time 46.06ms
2024-09-10 10:05:09 iter 47200: loss 1.1319, time 45.12ms
2024-09-10 10:05:13 iter 47300: loss 1.1142, time 45.60ms
2024-09-10 10:05:18 iter 47400: loss 1.0640, time 45.64ms
2024-09-10 10:05:22 iter 47500: loss 1.0813, time 45.73ms
2024-09-10 10:05:27 iter 47600: loss 1.0848, time 45.91ms
2024-09-10 10:05:32 iter 47700: loss 1.1381, time 44.79ms
2024-09-10 10:05:36 iter 47800: loss 1.0879, time 46.49ms
2024-09-10 10:05:41 iter 47900: loss 1.0915, time 45.41ms
2024-09-10 10:05:51 step 48000: train loss 1.0458, val loss 1.0717
2024-09-10 10:05:51 iter 48000: loss 1.0471, time 5891.95ms
2024-09-10 10:05:56 iter 48100: loss 1.0809, time 45.39ms
2024-09-10 10:06:00 iter 48200: loss 1.0582, time 45.70ms
2024-09-10 10:06:05 iter 48300: loss 1.0909, time 45.61ms
2024-09-10 10:06:09 iter 48400: loss 1.0191, time 45.53ms
2024-09-10 10:06:14 iter 48500: loss 1.1100, time 45.34ms
2024-09-10 10:06:18 iter 48600: loss 1.0957, time 45.53ms
2024-09-10 10:06:23 iter 48700: loss 1.0784, time 45.91ms
2024-09-10 10:06:27 iter 48800: loss 1.1321, time 45.45ms
2024-09-10 10:06:32 iter 48900: loss 1.1554, time 45.72ms
2024-09-10 10:06:42 step 49000: train loss 1.0408, val loss 1.0722
2024-09-10 10:06:42 iter 49000: loss 1.0903, time 5879.10ms
2024-09-10 10:06:47 iter 49100: loss 1.1127, time 45.62ms
2024-09-10 10:06:51 iter 49200: loss 1.0973, time 45.66ms
2024-09-10 10:06:56 iter 49300: loss 1.0789, time 45.39ms
2024-09-10 10:07:01 iter 49400: loss 1.1009, time 45.66ms
2024-09-10 10:07:05 iter 49500: loss 1.1058, time 46.06ms
2024-09-10 10:07:10 iter 49600: loss 1.1802, time 44.26ms
2024-09-10 10:07:14 iter 49700: loss 1.1561, time 45.50ms
2024-09-10 10:07:19 iter 49800: loss 1.0418, time 45.33ms
2024-09-10 10:07:23 iter 49900: loss 1.0855, time 45.09ms
2024-09-10 10:07:34 step 50000: train loss 1.0347, val loss 1.0667
2024-09-10 10:07:34 iter 50000: loss 1.1519, time 5884.68ms
2024-09-10 10:07:38 iter 50100: loss 1.0854, time 45.34ms
2024-09-10 10:07:43 iter 50200: loss 1.1016, time 44.57ms
2024-09-10 10:07:47 iter 50300: loss 1.1397, time 45.90ms
2024-09-10 10:07:52 iter 50400: loss 1.0787, time 44.40ms
2024-09-10 10:07:56 iter 50500: loss 1.1050, time 46.09ms
2024-09-10 10:08:01 iter 50600: loss 1.0940, time 45.78ms
2024-09-10 10:08:06 iter 50700: loss 1.1735, time 45.80ms
2024-09-10 10:08:10 iter 50800: loss 1.1169, time 45.61ms
2024-09-10 10:08:15 iter 50900: loss 1.1574, time 46.30ms
2024-09-10 10:08:25 step 51000: train loss 1.0368, val loss 1.0610
2024-09-10 10:08:25 iter 51000: loss 1.1095, time 5878.28ms
2024-09-10 10:08:30 iter 51100: loss 1.1102, time 45.64ms
2024-09-10 10:08:34 iter 51200: loss 1.1299, time 45.40ms
2024-09-10 10:08:39 iter 51300: loss 0.9943, time 45.57ms
2024-09-10 10:08:43 iter 51400: loss 1.0869, time 45.19ms
2024-09-10 10:08:48 iter 51500: loss 1.0487, time 45.61ms
2024-09-10 10:08:52 iter 51600: loss 1.0807, time 46.05ms
2024-09-10 10:08:57 iter 51700: loss 0.9719, time 45.31ms
2024-09-10 10:09:01 iter 51800: loss 1.1417, time 45.73ms
2024-09-10 10:09:06 iter 51900: loss 1.0558, time 45.48ms
2024-09-10 10:09:16 step 52000: train loss 1.0387, val loss 1.0640
2024-09-10 10:09:16 iter 52000: loss 1.0958, time 5891.37ms
2024-09-10 10:09:21 iter 52100: loss 1.1023, time 45.29ms
2024-09-10 10:09:25 iter 52200: loss 1.1457, time 44.05ms
2024-09-10 10:09:30 iter 52300: loss 1.2037, time 45.10ms
2024-09-10 10:09:35 iter 52400: loss 1.0818, time 45.74ms
2024-09-10 10:09:39 iter 52500: loss 1.1129, time 45.73ms
2024-09-10 10:09:44 iter 52600: loss 1.1167, time 45.93ms
2024-09-10 10:09:48 iter 52700: loss 1.0589, time 45.50ms
2024-09-10 10:09:53 iter 52800: loss 1.1171, time 45.80ms
2024-09-10 10:09:57 iter 52900: loss 1.1123, time 44.45ms
2024-09-10 10:10:08 step 53000: train loss 1.0310, val loss 1.0614
2024-09-10 10:10:08 iter 53000: loss 1.1439, time 5878.45ms
2024-09-10 10:10:12 iter 53100: loss 1.0493, time 45.04ms
2024-09-10 10:10:17 iter 53200: loss 1.1407, time 45.31ms
2024-09-10 10:10:21 iter 53300: loss 1.1275, time 45.85ms
2024-09-10 10:10:26 iter 53400: loss 1.1062, time 45.59ms
2024-09-10 10:10:30 iter 53500: loss 1.1132, time 46.15ms
2024-09-10 10:10:35 iter 53600: loss 1.0987, time 45.61ms
2024-09-10 10:10:40 iter 53700: loss 1.0816, time 46.02ms
2024-09-10 10:10:44 iter 53800: loss 1.0201, time 45.48ms
2024-09-10 10:10:49 iter 53900: loss 1.1303, time 45.38ms
2024-09-10 10:10:59 step 54000: train loss 1.0311, val loss 1.0560
2024-09-10 10:10:59 iter 54000: loss 1.0419, time 5867.11ms
2024-09-10 10:11:03 iter 54100: loss 1.0559, time 45.56ms
2024-09-10 10:11:08 iter 54200: loss 1.0891, time 45.58ms
2024-09-10 10:11:13 iter 54300: loss 1.0660, time 45.23ms
2024-09-10 10:11:17 iter 54400: loss 0.9794, time 45.43ms
2024-09-10 10:11:22 iter 54500: loss 1.0375, time 45.55ms
2024-09-10 10:11:26 iter 54600: loss 1.1093, time 45.84ms
2024-09-10 10:11:31 iter 54700: loss 1.1449, time 46.01ms
2024-09-10 10:11:35 iter 54800: loss 1.1386, time 45.35ms
2024-09-10 10:11:40 iter 54900: loss 1.0578, time 45.34ms
2024-09-10 10:11:50 step 55000: train loss 1.0310, val loss 1.0608
2024-09-10 10:11:50 iter 55000: loss 1.1417, time 5874.31ms
2024-09-10 10:11:55 iter 55100: loss 1.1257, time 45.94ms
2024-09-10 10:11:59 iter 55200: loss 1.0992, time 45.46ms
2024-09-10 10:12:04 iter 55300: loss 1.0941, time 45.16ms
2024-09-10 10:12:08 iter 55400: loss 1.1013, time 45.19ms
2024-09-10 10:12:13 iter 55500: loss 1.0741, time 45.67ms
2024-09-10 10:12:18 iter 55600: loss 1.0540, time 45.94ms
2024-09-10 10:12:22 iter 55700: loss 1.0247, time 45.76ms
2024-09-10 10:12:27 iter 55800: loss 1.0721, time 45.92ms
2024-09-10 10:12:31 iter 55900: loss 1.1666, time 46.49ms
2024-09-10 10:12:41 step 56000: train loss 1.0250, val loss 1.0512
2024-09-10 10:12:41 iter 56000: loss 1.0782, time 5886.36ms
2024-09-10 10:12:46 iter 56100: loss 1.1384, time 45.12ms
2024-09-10 10:12:51 iter 56200: loss 1.0442, time 45.83ms
2024-09-10 10:12:55 iter 56300: loss 1.1124, time 45.35ms
2024-09-10 10:13:00 iter 56400: loss 1.0411, time 45.73ms
2024-09-10 10:13:04 iter 56500: loss 1.1474, time 46.44ms
2024-09-10 10:13:09 iter 56600: loss 1.1482, time 45.39ms
2024-09-10 10:13:13 iter 56700: loss 0.9990, time 46.05ms
2024-09-10 10:13:18 iter 56800: loss 0.9999, time 45.59ms
2024-09-10 10:13:23 iter 56900: loss 0.9789, time 44.78ms
2024-09-10 10:13:33 step 57000: train loss 1.0273, val loss 1.0530
2024-09-10 10:13:33 iter 57000: loss 1.1624, time 5887.79ms
2024-09-10 10:13:37 iter 57100: loss 1.0472, time 46.17ms
2024-09-10 10:13:42 iter 57200: loss 1.0741, time 45.17ms
2024-09-10 10:13:47 iter 57300: loss 1.0721, time 45.64ms
2024-09-10 10:13:51 iter 57400: loss 1.0468, time 45.21ms
2024-09-10 10:13:56 iter 57500: loss 1.1266, time 45.40ms
2024-09-10 10:14:00 iter 57600: loss 0.9840, time 46.85ms
2024-09-10 10:14:05 iter 57700: loss 1.1748, time 45.68ms
2024-09-10 10:14:09 iter 57800: loss 1.0344, time 45.64ms
2024-09-10 10:14:14 iter 57900: loss 1.0353, time 45.60ms
2024-09-10 10:14:24 step 58000: train loss 1.0187, val loss 1.0532
2024-09-10 10:14:24 iter 58000: loss 1.1053, time 5884.83ms
2024-09-10 10:14:29 iter 58100: loss 1.0097, time 45.59ms
2024-09-10 10:14:33 iter 58200: loss 1.0408, time 45.70ms
2024-09-10 10:14:38 iter 58300: loss 1.0379, time 45.65ms
2024-09-10 10:14:42 iter 58400: loss 1.0751, time 46.03ms
2024-09-10 10:14:47 iter 58500: loss 1.0646, time 46.05ms
2024-09-10 10:14:52 iter 58600: loss 1.0939, time 45.20ms
2024-09-10 10:14:56 iter 58700: loss 1.0952, time 45.66ms
2024-09-10 10:15:01 iter 58800: loss 1.0556, time 45.60ms
2024-09-10 10:15:05 iter 58900: loss 1.1319, time 45.54ms
2024-09-10 10:15:15 step 59000: train loss 1.0178, val loss 1.0493
2024-09-10 10:15:15 iter 59000: loss 1.0686, time 5880.16ms
2024-09-10 10:15:20 iter 59100: loss 1.0234, time 45.40ms
2024-09-10 10:15:25 iter 59200: loss 1.0944, time 45.74ms
2024-09-10 10:15:29 iter 59300: loss 1.1363, time 45.22ms
2024-09-10 10:15:34 iter 59400: loss 1.1396, time 46.33ms
2024-09-10 10:15:38 iter 59500: loss 1.0873, time 45.66ms
2024-09-10 10:15:43 iter 59600: loss 1.0970, time 45.40ms
2024-09-10 10:15:47 iter 59700: loss 1.0688, time 45.74ms
2024-09-10 10:15:52 iter 59800: loss 1.1570, time 45.59ms
2024-09-10 10:15:57 iter 59900: loss 1.1400, time 45.74ms
2024-09-10 10:16:07 step 60000: train loss 1.0222, val loss 1.0559
2024-09-10 10:16:07 iter 60000: loss 1.1027, time 5856.16ms
2024-09-10 10:16:11 iter 60100: loss 1.0580, time 46.00ms
2024-09-10 10:16:16 iter 60200: loss 1.0086, time 45.03ms
2024-09-10 10:16:20 iter 60300: loss 1.1241, time 45.95ms
2024-09-10 10:16:25 iter 60400: loss 1.0474, time 46.08ms
2024-09-10 10:16:30 iter 60500: loss 1.1048, time 46.32ms
2024-09-10 10:16:34 iter 60600: loss 1.0318, time 45.43ms
2024-09-10 10:16:39 iter 60700: loss 1.1845, time 45.46ms
2024-09-10 10:16:43 iter 60800: loss 1.1125, time 44.88ms
2024-09-10 10:16:48 iter 60900: loss 1.1376, time 45.75ms
2024-09-10 10:16:58 step 61000: train loss 1.0194, val loss 1.0434
2024-09-10 10:16:58 iter 61000: loss 1.0571, time 5874.30ms
2024-09-10 10:17:03 iter 61100: loss 1.1101, time 45.66ms
2024-09-10 10:17:07 iter 61200: loss 1.0758, time 47.33ms
2024-09-10 10:17:12 iter 61300: loss 1.0762, time 46.02ms
2024-09-10 10:17:16 iter 61400: loss 1.1043, time 45.75ms
2024-09-10 10:17:21 iter 61500: loss 1.1000, time 45.52ms
2024-09-10 10:17:25 iter 61600: loss 1.1059, time 45.28ms
2024-09-10 10:17:30 iter 61700: loss 1.1343, time 45.61ms
2024-09-10 10:17:35 iter 61800: loss 0.9734, time 45.43ms
2024-09-10 10:17:39 iter 61900: loss 1.0666, time 45.51ms
2024-09-10 10:17:49 step 62000: train loss 1.0194, val loss 1.0449
2024-09-10 10:17:49 iter 62000: loss 1.0819, time 5875.81ms
2024-09-10 10:17:54 iter 62100: loss 1.0555, time 45.81ms
2024-09-10 10:17:58 iter 62200: loss 1.0560, time 45.48ms
2024-09-10 10:18:03 iter 62300: loss 1.1797, time 45.21ms
2024-09-10 10:18:08 iter 62400: loss 1.0237, time 45.23ms
2024-09-10 10:18:12 iter 62500: loss 1.0624, time 45.21ms
2024-09-10 10:18:17 iter 62600: loss 1.0839, time 45.88ms
2024-09-10 10:18:21 iter 62700: loss 1.1296, time 45.34ms
2024-09-10 10:18:26 iter 62800: loss 1.0720, time 45.80ms
2024-09-10 10:18:30 iter 62900: loss 1.0163, time 45.40ms
2024-09-10 10:18:40 step 63000: train loss 1.0172, val loss 1.0418
2024-09-10 10:18:41 iter 63000: loss 1.0542, time 5866.24ms
2024-09-10 10:18:45 iter 63100: loss 1.0773, time 45.81ms
2024-09-10 10:18:50 iter 63200: loss 1.0513, time 45.47ms
2024-09-10 10:18:54 iter 63300: loss 1.1366, time 45.21ms
2024-09-10 10:18:59 iter 63400: loss 1.0937, time 45.14ms
2024-09-10 10:19:03 iter 63500: loss 1.0545, time 45.26ms
2024-09-10 10:19:08 iter 63600: loss 1.1217, time 45.14ms
2024-09-10 10:19:12 iter 63700: loss 1.1000, time 46.05ms
2024-09-10 10:19:17 iter 63800: loss 1.1098, time 45.12ms
2024-09-10 10:19:22 iter 63900: loss 0.9846, time 46.28ms
2024-09-10 10:19:32 step 64000: train loss 1.0084, val loss 1.0397
2024-09-10 10:19:32 iter 64000: loss 1.0849, time 5881.20ms
2024-09-10 10:19:36 iter 64100: loss 1.0466, time 45.65ms
2024-09-10 10:19:41 iter 64200: loss 1.0237, time 45.30ms
2024-09-10 10:19:45 iter 64300: loss 1.1288, time 46.05ms
2024-09-10 10:19:50 iter 64400: loss 1.0399, time 45.19ms
2024-09-10 10:19:55 iter 64500: loss 1.0830, time 45.79ms
2024-09-10 10:19:59 iter 64600: loss 1.0825, time 45.16ms
2024-09-10 10:20:04 iter 64700: loss 1.0463, time 46.27ms
2024-09-10 10:20:08 iter 64800: loss 1.0421, time 45.84ms
2024-09-10 10:20:13 iter 64900: loss 0.9732, time 45.17ms
2024-09-10 10:20:23 step 65000: train loss 1.0131, val loss 1.0443
2024-09-10 10:20:23 iter 65000: loss 1.0390, time 5889.79ms
2024-09-10 10:20:28 iter 65100: loss 1.1056, time 45.28ms
2024-09-10 10:20:32 iter 65200: loss 1.1063, time 45.99ms
2024-09-10 10:20:37 iter 65300: loss 1.0781, time 45.27ms
2024-09-10 10:20:41 iter 65400: loss 1.1683, time 46.15ms
2024-09-10 10:20:46 iter 65500: loss 1.1259, time 44.93ms
2024-09-10 10:20:50 iter 65600: loss 1.0797, time 45.29ms
2024-09-10 10:20:55 iter 65700: loss 1.0527, time 45.93ms
2024-09-10 10:20:59 iter 65800: loss 1.0688, time 45.62ms
2024-09-10 10:21:04 iter 65900: loss 1.0362, time 45.21ms
2024-09-10 10:21:14 step 66000: train loss 1.0078, val loss 1.0423
2024-09-10 10:21:14 iter 66000: loss 1.0680, time 5862.99ms
2024-09-10 10:21:19 iter 66100: loss 1.0846, time 45.68ms
2024-09-10 10:21:23 iter 66200: loss 1.0510, time 45.91ms
2024-09-10 10:21:28 iter 66300: loss 1.1314, time 45.21ms
2024-09-10 10:21:32 iter 66400: loss 1.0730, time 44.98ms
2024-09-10 10:21:37 iter 66500: loss 1.0281, time 45.34ms
2024-09-10 10:21:42 iter 66600: loss 1.0383, time 45.54ms
2024-09-10 10:21:46 iter 66700: loss 1.0678, time 44.76ms
2024-09-10 10:21:51 iter 66800: loss 0.9025, time 45.29ms
2024-09-10 10:21:55 iter 66900: loss 1.1192, time 45.66ms
2024-09-10 10:22:05 step 67000: train loss 1.0099, val loss 1.0329
2024-09-10 10:22:05 iter 67000: loss 1.1052, time 5874.29ms
2024-09-10 10:22:10 iter 67100: loss 1.0471, time 45.44ms
2024-09-10 10:22:15 iter 67200: loss 0.9976, time 45.61ms
2024-09-10 10:22:19 iter 67300: loss 1.0874, time 45.13ms
2024-09-10 10:22:24 iter 67400: loss 1.0598, time 45.36ms
2024-09-10 10:22:28 iter 67500: loss 1.0571, time 45.30ms
2024-09-10 10:22:33 iter 67600: loss 1.1205, time 45.21ms
2024-09-10 10:22:37 iter 67700: loss 1.0665, time 45.41ms
2024-09-10 10:22:42 iter 67800: loss 1.0130, time 45.74ms
2024-09-10 10:22:46 iter 67900: loss 1.0608, time 45.52ms
2024-09-10 10:22:57 step 68000: train loss 1.0086, val loss 1.0375
2024-09-10 10:22:57 iter 68000: loss 1.0597, time 5871.89ms
2024-09-10 10:23:01 iter 68100: loss 1.0971, time 45.48ms
2024-09-10 10:23:06 iter 68200: loss 1.1017, time 45.26ms
2024-09-10 10:23:10 iter 68300: loss 0.9401, time 45.74ms
2024-09-10 10:23:15 iter 68400: loss 1.0915, time 45.32ms
2024-09-10 10:23:19 iter 68500: loss 1.0442, time 45.46ms
2024-09-10 10:23:24 iter 68600: loss 1.1129, time 45.40ms
2024-09-10 10:23:29 iter 68700: loss 1.0660, time 45.30ms
2024-09-10 10:23:33 iter 68800: loss 1.0089, time 45.85ms
2024-09-10 10:23:38 iter 68900: loss 1.0168, time 45.23ms
2024-09-10 10:23:48 step 69000: train loss 1.0038, val loss 1.0315
2024-09-10 10:23:48 iter 69000: loss 1.0801, time 5875.82ms
2024-09-10 10:23:52 iter 69100: loss 1.0580, time 45.26ms
2024-09-10 10:23:57 iter 69200: loss 1.0418, time 45.62ms
2024-09-10 10:24:02 iter 69300: loss 1.0283, time 45.82ms
2024-09-10 10:24:06 iter 69400: loss 1.0129, time 45.62ms
2024-09-10 10:24:11 iter 69500: loss 1.0145, time 45.64ms
2024-09-10 10:24:15 iter 69600: loss 1.0314, time 46.11ms
2024-09-10 10:24:20 iter 69700: loss 1.0843, time 45.90ms
2024-09-10 10:24:24 iter 69800: loss 0.9496, time 45.54ms
2024-09-10 10:24:29 iter 69900: loss 0.9805, time 45.57ms
2024-09-10 10:24:39 step 70000: train loss 1.0030, val loss 1.0332
2024-09-10 10:24:39 iter 70000: loss 1.0755, time 5874.50ms
2024-09-10 10:24:44 iter 70100: loss 0.9440, time 45.25ms
2024-09-10 10:24:48 iter 70200: loss 1.0495, time 45.68ms
2024-09-10 10:24:53 iter 70300: loss 1.0163, time 45.77ms
2024-09-10 10:24:57 iter 70400: loss 1.0332, time 44.62ms
2024-09-10 10:25:02 iter 70500: loss 0.9815, time 45.31ms
2024-09-10 10:25:06 iter 70600: loss 1.0991, time 46.64ms
2024-09-10 10:25:11 iter 70700: loss 1.0785, time 45.65ms
2024-09-10 10:25:16 iter 70800: loss 1.0695, time 45.84ms
2024-09-10 10:25:20 iter 70900: loss 0.9383, time 44.21ms
2024-09-10 10:25:30 step 71000: train loss 1.0005, val loss 1.0336
2024-09-10 10:25:30 iter 71000: loss 0.9886, time 5870.99ms
2024-09-10 10:25:35 iter 71100: loss 1.0950, time 45.55ms
2024-09-10 10:25:40 iter 71200: loss 1.0089, time 45.54ms
2024-09-10 10:25:44 iter 71300: loss 1.0210, time 45.70ms
2024-09-10 10:25:49 iter 71400: loss 1.0727, time 45.55ms
2024-09-10 10:25:53 iter 71500: loss 1.1334, time 45.34ms
2024-09-10 10:25:58 iter 71600: loss 1.1028, time 46.16ms
2024-09-10 10:26:02 iter 71700: loss 1.0525, time 46.07ms
2024-09-10 10:26:07 iter 71800: loss 0.9940, time 45.73ms
2024-09-10 10:26:11 iter 71900: loss 1.0520, time 45.52ms
2024-09-10 10:26:22 step 72000: train loss 1.0011, val loss 1.0301
2024-09-10 10:26:22 iter 72000: loss 1.0637, time 5871.37ms
2024-09-10 10:26:26 iter 72100: loss 1.0960, time 45.22ms
2024-09-10 10:26:31 iter 72200: loss 1.0374, time 44.96ms
2024-09-10 10:26:35 iter 72300: loss 1.0459, time 45.31ms
2024-09-10 10:26:40 iter 72400: loss 1.0121, time 46.05ms
2024-09-10 10:26:44 iter 72500: loss 1.0298, time 45.28ms
2024-09-10 10:26:49 iter 72600: loss 0.9737, time 45.71ms
2024-09-10 10:26:54 iter 72700: loss 0.9496, time 45.83ms
2024-09-10 10:26:58 iter 72800: loss 0.9784, time 45.18ms
2024-09-10 10:27:03 iter 72900: loss 1.0649, time 45.94ms
2024-09-10 10:27:13 step 73000: train loss 0.9987, val loss 1.0348
2024-09-10 10:27:13 iter 73000: loss 1.0252, time 5873.69ms
2024-09-10 10:27:17 iter 73100: loss 1.0310, time 45.62ms
2024-09-10 10:27:22 iter 73200: loss 0.9784, time 45.66ms
2024-09-10 10:27:27 iter 73300: loss 1.0055, time 45.75ms
2024-09-10 10:27:31 iter 73400: loss 1.1452, time 45.69ms
2024-09-10 10:27:36 iter 73500: loss 1.1075, time 46.18ms
2024-09-10 10:27:40 iter 73600: loss 1.0564, time 45.77ms
2024-09-10 10:27:45 iter 73700: loss 0.9700, time 45.93ms
2024-09-10 10:27:49 iter 73800: loss 0.9935, time 45.23ms
2024-09-10 10:27:54 iter 73900: loss 0.9735, time 45.06ms
2024-09-10 10:28:04 step 74000: train loss 0.9937, val loss 1.0299
2024-09-10 10:28:04 iter 74000: loss 1.0352, time 5873.29ms
2024-09-10 10:28:09 iter 74100: loss 1.0315, time 45.36ms
2024-09-10 10:28:13 iter 74200: loss 1.0566, time 45.49ms
2024-09-10 10:28:18 iter 74300: loss 1.0424, time 45.71ms
2024-09-10 10:28:22 iter 74400: loss 0.9500, time 45.85ms
2024-09-10 10:28:27 iter 74500: loss 1.0889, time 45.63ms
2024-09-10 10:28:32 iter 74600: loss 1.0437, time 45.58ms
2024-09-10 10:28:36 iter 74700: loss 1.0016, time 45.32ms
2024-09-10 10:28:41 iter 74800: loss 1.0209, time 45.75ms
2024-09-10 10:28:45 iter 74900: loss 1.0787, time 45.88ms
2024-09-10 10:28:55 step 75000: train loss 0.9931, val loss 1.0216
2024-09-10 10:28:55 iter 75000: loss 1.0023, time 5873.29ms
2024-09-10 10:29:00 iter 75100: loss 1.0326, time 45.62ms
2024-09-10 10:29:05 iter 75200: loss 1.1048, time 45.78ms
2024-09-10 10:29:09 iter 75300: loss 1.0131, time 45.88ms
2024-09-10 10:29:14 iter 75400: loss 1.0676, time 45.17ms
2024-09-10 10:29:18 iter 75500: loss 1.1246, time 45.82ms
2024-09-10 10:29:23 iter 75600: loss 1.0566, time 45.41ms
2024-09-10 10:29:27 iter 75700: loss 1.0068, time 46.24ms
2024-09-10 10:29:32 iter 75800: loss 1.0090, time 45.75ms
2024-09-10 10:29:37 iter 75900: loss 1.0568, time 46.82ms
2024-09-10 10:29:47 step 76000: train loss 0.9908, val loss 1.0293
2024-09-10 10:29:47 iter 76000: loss 1.0839, time 5871.42ms
2024-09-10 10:29:51 iter 76100: loss 1.0600, time 45.29ms
2024-09-10 10:29:56 iter 76200: loss 1.0529, time 45.33ms
2024-09-10 10:30:00 iter 76300: loss 0.9738, time 45.61ms
2024-09-10 10:30:05 iter 76400: loss 1.0671, time 45.53ms
2024-09-10 10:30:10 iter 76500: loss 1.0738, time 45.71ms
2024-09-10 10:30:14 iter 76600: loss 1.0752, time 45.58ms
2024-09-10 10:30:19 iter 76700: loss 0.9945, time 45.60ms
2024-09-10 10:30:23 iter 76800: loss 1.0412, time 45.30ms
2024-09-10 10:30:28 iter 76900: loss 1.0257, time 45.13ms
2024-09-10 10:30:38 step 77000: train loss 0.9922, val loss 1.0249
2024-09-10 10:30:38 iter 77000: loss 1.0439, time 5879.43ms
2024-09-10 10:30:43 iter 77100: loss 1.0214, time 45.71ms
2024-09-10 10:30:47 iter 77200: loss 1.0078, time 45.22ms
2024-09-10 10:30:52 iter 77300: loss 1.1046, time 45.64ms
2024-09-10 10:30:56 iter 77400: loss 1.0351, time 45.28ms
2024-09-10 10:31:01 iter 77500: loss 1.0297, time 46.31ms
2024-09-10 10:31:05 iter 77600: loss 1.0510, time 45.54ms
2024-09-10 10:31:10 iter 77700: loss 1.1034, time 45.18ms
2024-09-10 10:31:15 iter 77800: loss 1.0151, time 46.18ms
2024-09-10 10:31:19 iter 77900: loss 1.0961, time 45.65ms
2024-09-10 10:31:29 step 78000: train loss 0.9916, val loss 1.0270
2024-09-10 10:31:29 iter 78000: loss 1.0124, time 5882.40ms
2024-09-10 10:31:34 iter 78100: loss 1.0823, time 45.61ms
2024-09-10 10:31:39 iter 78200: loss 0.9355, time 45.60ms
2024-09-10 10:31:43 iter 78300: loss 1.0569, time 45.85ms
2024-09-10 10:31:48 iter 78400: loss 1.0067, time 45.16ms
2024-09-10 10:31:52 iter 78500: loss 1.0442, time 45.33ms
2024-09-10 10:31:57 iter 78600: loss 1.0235, time 45.78ms
2024-09-10 10:32:01 iter 78700: loss 1.1226, time 46.23ms
2024-09-10 10:32:06 iter 78800: loss 1.0440, time 45.74ms
2024-09-10 10:32:11 iter 78900: loss 1.0280, time 45.37ms
2024-09-10 10:32:21 step 79000: train loss 0.9918, val loss 1.0207
2024-09-10 10:32:21 iter 79000: loss 1.1490, time 5877.09ms
2024-09-10 10:32:25 iter 79100: loss 1.0158, time 45.48ms
2024-09-10 10:32:30 iter 79200: loss 1.1128, time 45.61ms
2024-09-10 10:32:34 iter 79300: loss 1.0740, time 44.76ms
2024-09-10 10:32:39 iter 79400: loss 1.0375, time 45.72ms
2024-09-10 10:32:44 iter 79500: loss 1.0075, time 45.36ms
2024-09-10 10:32:48 iter 79600: loss 1.0823, time 46.74ms
2024-09-10 10:32:53 iter 79700: loss 1.0769, time 45.91ms
2024-09-10 10:32:57 iter 79800: loss 1.0531, time 45.12ms
2024-09-10 10:33:02 iter 79900: loss 1.0052, time 45.22ms
2024-09-10 10:33:12 step 80000: train loss 0.9873, val loss 1.0193
2024-09-10 10:33:12 iter 80000: loss 1.0537, time 5884.91ms
2024-09-10 10:33:17 iter 80100: loss 1.0797, time 45.70ms
2024-09-10 10:33:21 iter 80200: loss 1.1096, time 45.95ms
2024-09-10 10:33:26 iter 80300: loss 1.0017, time 45.82ms
2024-09-10 10:33:30 iter 80400: loss 0.9828, time 45.24ms
2024-09-10 10:33:35 iter 80500: loss 1.0975, time 46.17ms
2024-09-10 10:33:40 iter 80600: loss 0.9326, time 45.63ms
2024-09-10 10:33:44 iter 80700: loss 1.0201, time 45.79ms
2024-09-10 10:33:49 iter 80800: loss 1.0722, time 45.96ms
2024-09-10 10:33:53 iter 80900: loss 1.0353, time 45.13ms
2024-09-10 10:34:03 step 81000: train loss 0.9897, val loss 1.0258
2024-09-10 10:34:04 iter 81000: loss 1.0450, time 5871.27ms
2024-09-10 10:34:08 iter 81100: loss 0.9418, time 45.20ms
2024-09-10 10:34:13 iter 81200: loss 1.0214, time 45.66ms
2024-09-10 10:34:17 iter 81300: loss 0.9962, time 45.68ms
2024-09-10 10:34:22 iter 81400: loss 1.0533, time 45.36ms
2024-09-10 10:34:26 iter 81500: loss 1.0436, time 45.40ms
2024-09-10 10:34:31 iter 81600: loss 1.0429, time 45.86ms
2024-09-10 10:34:35 iter 81700: loss 1.0749, time 45.90ms
2024-09-10 10:34:40 iter 81800: loss 1.0577, time 45.35ms
2024-09-10 10:34:45 iter 81900: loss 1.0426, time 46.10ms
2024-09-10 10:34:55 step 82000: train loss 0.9861, val loss 1.0161
2024-09-10 10:34:55 iter 82000: loss 1.0357, time 5873.25ms
2024-09-10 10:34:59 iter 82100: loss 1.0837, time 45.26ms
2024-09-10 10:35:04 iter 82200: loss 0.9920, time 46.11ms
2024-09-10 10:35:09 iter 82300: loss 1.0633, time 45.98ms
2024-09-10 10:35:13 iter 82400: loss 1.1173, time 45.25ms
2024-09-10 10:35:18 iter 82500: loss 1.0446, time 45.74ms
2024-09-10 10:35:22 iter 82600: loss 1.0493, time 45.98ms
2024-09-10 10:35:27 iter 82700: loss 1.0412, time 45.72ms
2024-09-10 10:35:31 iter 82800: loss 1.0268, time 45.27ms
2024-09-10 10:35:36 iter 82900: loss 1.0918, time 45.86ms
2024-09-10 10:35:46 step 83000: train loss 0.9869, val loss 1.0179
2024-09-10 10:35:46 iter 83000: loss 1.0145, time 5869.79ms
2024-09-10 10:35:51 iter 83100: loss 1.1065, time 45.56ms
2024-09-10 10:35:55 iter 83200: loss 1.0660, time 45.84ms
2024-09-10 10:36:00 iter 83300: loss 1.0253, time 46.18ms
2024-09-10 10:36:04 iter 83400: loss 1.0228, time 45.85ms
2024-09-10 10:36:09 iter 83500: loss 1.0447, time 45.41ms
2024-09-10 10:36:14 iter 83600: loss 1.0260, time 45.69ms
2024-09-10 10:36:18 iter 83700: loss 0.9898, time 45.91ms
2024-09-10 10:36:23 iter 83800: loss 1.0613, time 46.17ms
2024-09-10 10:36:27 iter 83900: loss 0.9992, time 45.78ms
2024-09-10 10:36:37 step 84000: train loss 0.9905, val loss 1.0238
2024-09-10 10:36:37 iter 84000: loss 1.0161, time 5879.10ms
2024-09-10 10:36:42 iter 84100: loss 1.0697, time 45.23ms
2024-09-10 10:36:47 iter 84200: loss 1.0064, time 45.40ms
2024-09-10 10:36:51 iter 84300: loss 1.0459, time 45.39ms
2024-09-10 10:36:56 iter 84400: loss 1.0890, time 46.09ms
2024-09-10 10:37:00 iter 84500: loss 1.0308, time 45.26ms
2024-09-10 10:37:05 iter 84600: loss 0.9728, time 45.11ms
2024-09-10 10:37:09 iter 84700: loss 1.0235, time 45.20ms
2024-09-10 10:37:14 iter 84800: loss 1.0304, time 45.90ms
2024-09-10 10:37:19 iter 84900: loss 1.0263, time 46.16ms
2024-09-10 10:37:29 step 85000: train loss 0.9840, val loss 1.0161
2024-09-10 10:37:29 iter 85000: loss 1.0141, time 5878.52ms
2024-09-10 10:37:33 iter 85100: loss 0.9552, time 45.20ms
2024-09-10 10:37:38 iter 85200: loss 1.0302, time 45.37ms
2024-09-10 10:37:42 iter 85300: loss 0.9428, time 45.45ms
2024-09-10 10:37:47 iter 85400: loss 1.0245, time 45.14ms
2024-09-10 10:37:52 iter 85500: loss 0.9851, time 45.96ms
2024-09-10 10:37:56 iter 85600: loss 1.0204, time 45.70ms
2024-09-10 10:38:01 iter 85700: loss 1.0328, time 45.24ms
2024-09-10 10:38:05 iter 85800: loss 0.9959, time 45.73ms
2024-09-10 10:38:10 iter 85900: loss 1.0042, time 45.29ms
2024-09-10 10:38:20 step 86000: train loss 0.9833, val loss 1.0200
2024-09-10 10:38:20 iter 86000: loss 1.0283, time 5874.17ms
2024-09-10 10:38:25 iter 86100: loss 0.9678, time 45.44ms
2024-09-10 10:38:29 iter 86200: loss 1.0292, time 45.71ms
2024-09-10 10:38:34 iter 86300: loss 1.0286, time 45.24ms
2024-09-10 10:38:38 iter 86400: loss 1.0215, time 45.55ms
2024-09-10 10:38:43 iter 86500: loss 1.0044, time 45.26ms
2024-09-10 10:38:47 iter 86600: loss 1.0662, time 45.85ms
2024-09-10 10:38:52 iter 86700: loss 1.0002, time 45.72ms
2024-09-10 10:38:57 iter 86800: loss 1.0620, time 45.62ms
2024-09-10 10:39:01 iter 86900: loss 1.0419, time 45.56ms
2024-09-10 10:39:11 step 87000: train loss 0.9787, val loss 1.0142
2024-09-10 10:39:11 iter 87000: loss 1.0366, time 5875.43ms
2024-09-10 10:39:16 iter 87100: loss 1.0102, time 45.86ms
2024-09-10 10:39:20 iter 87200: loss 0.9919, time 45.59ms
2024-09-10 10:39:25 iter 87300: loss 1.0849, time 45.66ms
2024-09-10 10:39:30 iter 87400: loss 1.0902, time 45.81ms
2024-09-10 10:39:34 iter 87500: loss 0.9814, time 45.15ms
2024-09-10 10:39:39 iter 87600: loss 1.0021, time 45.91ms
2024-09-10 10:39:43 iter 87700: loss 1.0863, time 45.80ms
2024-09-10 10:39:48 iter 87800: loss 1.0888, time 45.60ms
2024-09-10 10:39:52 iter 87900: loss 1.0434, time 46.12ms
2024-09-10 10:40:03 step 88000: train loss 0.9755, val loss 1.0117
2024-09-10 10:40:03 iter 88000: loss 1.0405, time 5878.21ms
2024-09-10 10:40:07 iter 88100: loss 1.0830, time 45.57ms
2024-09-10 10:40:12 iter 88200: loss 0.9824, time 47.23ms
2024-09-10 10:40:16 iter 88300: loss 1.0592, time 45.89ms
2024-09-10 10:40:21 iter 88400: loss 0.9955, time 45.31ms
2024-09-10 10:40:26 iter 88500: loss 1.0059, time 45.77ms
2024-09-10 10:40:30 iter 88600: loss 1.0474, time 45.13ms
2024-09-10 10:40:35 iter 88700: loss 1.0191, time 45.58ms
2024-09-10 10:40:39 iter 88800: loss 1.0380, time 46.95ms
2024-09-10 10:40:44 iter 88900: loss 1.0588, time 45.49ms
2024-09-10 10:40:54 step 89000: train loss 0.9747, val loss 1.0129
2024-09-10 10:40:54 iter 89000: loss 1.0399, time 5883.40ms
2024-09-10 10:40:59 iter 89100: loss 1.0160, time 46.18ms
2024-09-10 10:41:03 iter 89200: loss 1.0093, time 46.16ms
2024-09-10 10:41:08 iter 89300: loss 0.9867, time 45.69ms
2024-09-10 10:41:12 iter 89400: loss 0.9767, time 45.37ms
2024-09-10 10:41:17 iter 89500: loss 1.0077, time 45.84ms
2024-09-10 10:41:21 iter 89600: loss 0.9028, time 45.71ms
2024-09-10 10:41:26 iter 89700: loss 1.0598, time 45.65ms
2024-09-10 10:41:30 iter 89800: loss 1.0154, time 45.25ms
2024-09-10 10:41:35 iter 89900: loss 1.0072, time 45.57ms
2024-09-10 10:41:45 step 90000: train loss 0.9778, val loss 1.0085
2024-09-10 10:41:45 iter 90000: loss 1.0060, time 5873.27ms
2024-09-10 10:41:50 iter 90100: loss 1.0550, time 44.23ms
2024-09-10 10:41:54 iter 90200: loss 1.0345, time 46.55ms
2024-09-10 10:41:59 iter 90300: loss 1.1282, time 45.88ms
2024-09-10 10:42:03 iter 90400: loss 0.9890, time 45.24ms
2024-09-10 10:42:08 iter 90500: loss 1.0110, time 45.49ms
2024-09-10 10:42:13 iter 90600: loss 1.0620, time 45.51ms
2024-09-10 10:42:17 iter 90700: loss 1.0537, time 45.30ms
2024-09-10 10:42:22 iter 90800: loss 1.0082, time 45.53ms
2024-09-10 10:42:26 iter 90900: loss 0.9761, time 45.65ms
2024-09-10 10:42:36 step 91000: train loss 0.9750, val loss 1.0109
2024-09-10 10:42:37 iter 91000: loss 1.0219, time 5877.64ms
2024-09-10 10:42:41 iter 91100: loss 1.0534, time 44.90ms
2024-09-10 10:42:46 iter 91200: loss 1.0177, time 45.64ms
2024-09-10 10:42:50 iter 91300: loss 1.0545, time 45.73ms
2024-09-10 10:42:55 iter 91400: loss 1.0392, time 45.24ms
2024-09-10 10:42:59 iter 91500: loss 1.0578, time 46.10ms
2024-09-10 10:43:04 iter 91600: loss 0.9573, time 45.62ms
2024-09-10 10:43:08 iter 91700: loss 1.0492, time 45.83ms
2024-09-10 10:43:13 iter 91800: loss 0.9480, time 45.96ms
2024-09-10 10:43:18 iter 91900: loss 1.0392, time 46.37ms
2024-09-10 10:43:28 step 92000: train loss 0.9768, val loss 1.0082
2024-09-10 10:43:28 iter 92000: loss 1.0164, time 5887.02ms
2024-09-10 10:43:32 iter 92100: loss 1.0954, time 45.30ms
2024-09-10 10:43:37 iter 92200: loss 1.0375, time 47.27ms
2024-09-10 10:43:41 iter 92300: loss 1.0526, time 45.68ms
2024-09-10 10:43:46 iter 92400: loss 1.0397, time 45.24ms
2024-09-10 10:43:51 iter 92500: loss 1.0231, time 45.26ms
2024-09-10 10:43:55 iter 92600: loss 0.9433, time 45.71ms
2024-09-10 10:44:00 iter 92700: loss 1.0581, time 45.58ms
2024-09-10 10:44:04 iter 92800: loss 1.0258, time 45.19ms
2024-09-10 10:44:09 iter 92900: loss 1.0243, time 45.69ms
2024-09-10 10:44:19 step 93000: train loss 0.9764, val loss 1.0081
2024-09-10 10:44:19 iter 93000: loss 1.0557, time 5883.85ms
2024-09-10 10:44:24 iter 93100: loss 1.0194, time 45.58ms
2024-09-10 10:44:28 iter 93200: loss 1.0448, time 45.24ms
2024-09-10 10:44:33 iter 93300: loss 1.0527, time 45.21ms
2024-09-10 10:44:37 iter 93400: loss 0.9974, time 45.36ms
2024-09-10 10:44:42 iter 93500: loss 1.0380, time 45.50ms
2024-09-10 10:44:46 iter 93600: loss 1.0615, time 45.77ms
2024-09-10 10:44:51 iter 93700: loss 1.0309, time 45.20ms
2024-09-10 10:44:56 iter 93800: loss 1.1112, time 46.46ms
2024-09-10 10:45:00 iter 93900: loss 1.0398, time 46.75ms
2024-09-10 10:45:10 step 94000: train loss 0.9703, val loss 1.0155
2024-09-10 10:45:10 iter 94000: loss 1.0312, time 5877.39ms
2024-09-10 10:45:15 iter 94100: loss 1.0383, time 45.22ms
2024-09-10 10:45:19 iter 94200: loss 0.9614, time 45.67ms
2024-09-10 10:45:24 iter 94300: loss 1.0601, time 46.23ms
2024-09-10 10:45:29 iter 94400: loss 0.9965, time 45.66ms
2024-09-10 10:45:33 iter 94500: loss 1.0968, time 46.03ms
2024-09-10 10:45:38 iter 94600: loss 0.9610, time 45.73ms
2024-09-10 10:45:42 iter 94700: loss 1.1057, time 45.60ms
2024-09-10 10:45:47 iter 94800: loss 1.0201, time 45.06ms
2024-09-10 10:45:51 iter 94900: loss 0.9614, time 45.48ms
2024-09-10 10:46:02 step 95000: train loss 0.9672, val loss 1.0070
2024-09-10 10:46:02 iter 95000: loss 1.0738, time 5874.35ms
2024-09-10 10:46:06 iter 95100: loss 1.0076, time 45.23ms
2024-09-10 10:46:11 iter 95200: loss 0.9867, time 46.30ms
2024-09-10 10:46:15 iter 95300: loss 1.1436, time 45.30ms
2024-09-10 10:46:20 iter 95400: loss 1.0269, time 45.73ms
2024-09-10 10:46:24 iter 95500: loss 1.0453, time 45.62ms
2024-09-10 10:46:29 iter 95600: loss 1.0419, time 45.57ms
2024-09-10 10:46:33 iter 95700: loss 1.0909, time 45.47ms
2024-09-10 10:46:38 iter 95800: loss 1.0093, time 45.39ms
2024-09-10 10:46:43 iter 95900: loss 1.0075, time 45.80ms
2024-09-10 10:46:53 step 96000: train loss 0.9741, val loss 1.0050
2024-09-10 10:46:53 iter 96000: loss 1.0115, time 5873.23ms
2024-09-10 10:46:57 iter 96100: loss 1.0661, time 45.75ms
2024-09-10 10:47:02 iter 96200: loss 1.0375, time 45.10ms
2024-09-10 10:47:07 iter 96300: loss 1.0861, time 46.47ms
2024-09-10 10:47:11 iter 96400: loss 1.0870, time 44.92ms
2024-09-10 10:47:16 iter 96500: loss 1.0542, time 45.87ms
2024-09-10 10:47:20 iter 96600: loss 1.0571, time 44.24ms
2024-09-10 10:47:25 iter 96700: loss 1.0539, time 44.30ms
2024-09-10 10:47:29 iter 96800: loss 0.9467, time 45.61ms
2024-09-10 10:47:34 iter 96900: loss 1.0810, time 45.83ms
2024-09-10 10:47:44 step 97000: train loss 0.9684, val loss 1.0103
2024-09-10 10:47:44 iter 97000: loss 0.9345, time 5878.65ms
2024-09-10 10:47:49 iter 97100: loss 0.9900, time 45.75ms
2024-09-10 10:47:53 iter 97200: loss 1.0448, time 46.72ms
2024-09-10 10:47:58 iter 97300: loss 0.9870, time 45.34ms
2024-09-10 10:48:02 iter 97400: loss 1.0790, time 45.42ms
2024-09-10 10:48:07 iter 97500: loss 0.9768, time 45.78ms
2024-09-10 10:48:11 iter 97600: loss 1.0653, time 45.34ms
2024-09-10 10:48:16 iter 97700: loss 0.9904, time 45.23ms
2024-09-10 10:48:21 iter 97800: loss 1.0853, time 45.74ms
2024-09-10 10:48:25 iter 97900: loss 1.0715, time 45.22ms
2024-09-10 10:48:35 step 98000: train loss 0.9701, val loss 1.0079
2024-09-10 10:48:35 iter 98000: loss 0.9640, time 5868.82ms
2024-09-10 10:48:40 iter 98100: loss 1.0712, time 45.26ms
2024-09-10 10:48:44 iter 98200: loss 0.9404, time 45.32ms
2024-09-10 10:48:49 iter 98300: loss 0.9789, time 45.28ms
2024-09-10 10:48:54 iter 98400: loss 1.0422, time 45.64ms
2024-09-10 10:48:58 iter 98500: loss 1.0464, time 45.66ms
2024-09-10 10:49:03 iter 98600: loss 1.0286, time 46.12ms
2024-09-10 10:49:07 iter 98700: loss 1.0498, time 45.59ms
2024-09-10 10:49:12 iter 98800: loss 1.0200, time 45.71ms
2024-09-10 10:49:16 iter 98900: loss 1.0176, time 45.72ms
2024-09-10 10:49:26 step 99000: train loss 0.9714, val loss 1.0055
2024-09-10 10:49:27 iter 99000: loss 1.0136, time 5906.17ms
2024-09-10 10:49:31 iter 99100: loss 1.0597, time 45.25ms
2024-09-10 10:49:36 iter 99200: loss 1.0714, time 45.67ms
2024-09-10 10:49:40 iter 99300: loss 1.0827, time 45.88ms
2024-09-10 10:49:45 iter 99400: loss 1.0933, time 45.71ms
2024-09-10 10:49:49 iter 99500: loss 1.0152, time 45.51ms
2024-09-10 10:49:54 iter 99600: loss 0.9349, time 45.88ms
2024-09-10 10:49:58 iter 99700: loss 1.0478, time 45.52ms
2024-09-10 10:50:03 iter 99800: loss 1.0465, time 45.67ms
2024-09-10 10:50:07 iter 99900: loss 1.0013, time 46.14ms
2024-09-10 10:50:18 step 100000: train loss 0.9734, val loss 1.0071
2024-09-10 10:50:18 iter 100000: loss 0.9391, time 5872.01ms
2024-09-10 10:50:18 training done
2024-09-10 10:50:18 Best validation loss: 1.0050407648086548
2024-09-10 10:50:18 Total train time: 86.05 mins
2024-09-10 10:50:18 Loading meta from ../../data/enwik8/meta.pkl...
2024-09-10 10:50:20 Sample 1:
2024-09-10 10:50:20  [[DA]], [[Internet]], [[Internet]], [[Intel]], [[Intel]], [[Internet]], [[Intelsat]] and [[Internet]].
2024-09-10 10:50:20 
2024-09-10 10:50:20 ==See also==
2024-09-10 10:50:20 *[[List of writers with deliberate descriptions]]
2024-09-10 10:50:20 
2024-09-10 10:50:20 ==External links==
2024-09-10 10:50:20 ===Related files===
2024-09-10 10:50:20 *[http://www.internet.org/avl/ ''Internet.org'']
2024-09-10 10:50:20 *[http://www.math.net/math.net/ Math.net]
2024-09-10 10:50:20 *[http://www.perliamentation.org/ Perliamentation: Perliamentary Arts Defense Participation]
2024-09-10 10:50:20 *[http://www.ornithmed.org/ OrnithMedicine]
2024-09-10 10:50:20 *[http://www.theornith Theory of the World]
2024-09-10 10:50:20 *[http://www.particip
2024-09-10 10:50:20 Inference time: 2.15 seconds
2024-09-10 10:50:20 Tokens per second: 232.13
2024-09-10 10:50:20 ---------------
2024-09-10 10:50:22 Sample 2:
2024-09-10 10:50:22  south of the [[Middle Ages]] (see [[Middle Ages]], [[Middle Ages]]). Ten years old ages such as [[pigment sound]] and [[temperature]] have a larger local area. As a temperature of the arm / [[Middle Ages]] than [[Islam]], [[Africa]] and other Middle Ages are still highly separated from the [[Eastern Orthodox Church|Orthodox]] church.
2024-09-10 10:50:22 
2024-09-10 10:50:22 Perhaps the most prominent church in the [[Arabic language]], there are small dialects from those who are not unknown as the [[Eastern Orthodox Church|Eastern Orth
2024-09-10 10:50:22 Inference time: 2.27 seconds
2024-09-10 10:50:22 Tokens per second: 220.45
2024-09-10 10:50:22 ---------------
2024-09-10 10:50:24 Sample 3:
2024-09-10 10:50:24  to other classes but ordinary classes display during the first, thus it may be provided for all artifacts by blues. For example, if the player is displayed, it has a total of 5 classes called the &quot;[[The Man in the House]]&quot; an additional driver with instruments.
2024-09-10 10:50:24 
2024-09-10 10:50:24 ===Pair and resources===
2024-09-10 10:50:24 ''See also: [[Pair and resources]]''
2024-09-10 10:50:24 
2024-09-10 10:50:24 See also:
2024-09-10 10:50:24 *[[Analog computer]]
2024-09-10 10:50:24 *[[Biography]]
2024-09-10 10:50:24 *[[List of programs topics]]
2024-09-10 10:50:24 
2024-09-10 10:50:24 [[Category:Notable computer]]
2024-09-10 10:50:24 
2024-09-10 10:50:24 [[bg:ÐÐ¸ÑÐ±Ð¾Ð³ÑÑÐºÐ¸Ðµ]]
2024-09-10 10:50:24 [[ca:Birbage d'Irondica]]
2024-09-10 10:50:24 [
2024-09-10 10:50:24 Inference time: 2.02 seconds
2024-09-10 10:50:24 Tokens per second: 247.08
2024-09-10 10:50:24 ---------------
2024-09-10 10:50:26 Sample 4:
2024-09-10 10:50:26  later a small barrist. The English female agent accompanied a project by structuring the large [[North American Front]] to achieve the [[Spanish language|Spanish]] flood at [[Kafka]]. By the [[1970s]] the [[Front (moon)|Front]] had been pieced by [[Casuar Barrister]] in [[New York City]] and [[London]] and [[San Diego Bernardo de la Grande]] in the [[United States dollar]] to [[New York City]] on [[August 9]], [[1951]]. The Bernardo Speaker around [[March 17]], [[1951]] the Bernards situated in 
2024-09-10 10:50:26 Inference time: 2.23 seconds
2024-09-10 10:50:26 Tokens per second: 223.88
2024-09-10 10:50:26 ---------------
2024-09-10 10:50:29 Sample 5:
2024-09-10 10:50:29  trainers in a line it was saved by the murder. The state was replaced by [[Colombia]] in [[1968]].  [[Canada]] began to take construction of a state of an attacking during the [[Arctic Ocean]]. The most popular is that of the mainland [[1989]] [[Mariner of Denmark]].
2024-09-10 10:50:29 
2024-09-10 10:50:29 When Colombia started returning to the [[Florence Revolution]], the population was started at [[Westminster Athletic Association|Westminster]] and [[South Athletic Columbia]] in a particular branch of the [[League of Italy|Italians
2024-09-10 10:50:29 Inference time: 2.30 seconds
2024-09-10 10:50:29 Tokens per second: 217.53
2024-09-10 10:50:29 ---------------
2024-09-10 10:50:31 Sample 6:
2024-09-10 10:50:31  to form the most important part of China - some have allowed the Chinese to embedded the common phrases for the Romance Christian, in which the Chinese churches were carved with the Chinese infantry.
2024-09-10 10:50:31 
2024-09-10 10:50:31 Most of the major churches of China - include islands (compared to the south coast of China) and had a great power in the Chinese colonies. In contrast, Kozoo was restricted to the [[Chinese Church]] in the [[Middle East]] of the [[United States]] (the [[Chinese colonies|Chinese colony]]), which se
2024-09-10 10:50:31 Inference time: 2.13 seconds
2024-09-10 10:50:31 Tokens per second: 234.74
2024-09-10 10:50:31 ---------------
2024-09-10 10:50:33 Sample 7:
2024-09-10 10:50:33  language and, as each stands for language.  The term refers to the standard language.  Both East and African languages are used to describe the article &quot;escape&quot;, and particularly for language for their language.  Each standard language consists of the article names being ''etc.''  It has been used to define some of the languages that are [[Escape language|Escape]] with their [[grammatical language]]s that are added to it.  It may be an implementation of a variety of articles because th
2024-09-10 10:50:33 Inference time: 2.22 seconds
2024-09-10 10:50:33 Tokens per second: 225.40
2024-09-10 10:50:33 ---------------
2024-09-10 10:50:35 Sample 8:
2024-09-10 10:50:35  they would be considered a long term that is correct. However, the protocol of study is better used by cancellating a fixed distinction of the outdoor definition of the protocol. The reconciliation was said to have a complex association with reeds of definition.  For example, this definition was the reconciliation of Cannel 4:1-11, but was an attached against the Philippines with a player, Apple is also a reconciliation of cannels of the profit, with the reconciliation of a simple point where ca
2024-09-10 10:50:35 Inference time: 1.94 seconds
2024-09-10 10:50:35 Tokens per second: 257.24
2024-09-10 10:50:35 ---------------
2024-09-10 10:50:37 Sample 9:
2024-09-10 10:50:37  all literary descendants of Jews and Passages also known as the [[Hamas Empire]] in the [[Catholic Church]], but as a determining them alone of the prehistoric power by the [[New Testament]] experienced the US in [[1987]], and the surviving republic was buried by [[Tataran]] called [[Picho bows]]. 
2024-09-10 10:50:37 
2024-09-10 10:50:37 Compared to the construction between Manchus and Europe was reply to the office, the advice and courts of the [[Egyptian Government]], the Anglo-Saxon [[rebels]] such as [[Bulgarian government]] and 
2024-09-10 10:50:37 Inference time: 2.09 seconds
2024-09-10 10:50:37 Tokens per second: 238.75
2024-09-10 10:50:37 ---------------
2024-09-10 10:50:39 Sample 10:
2024-09-10 10:50:39  among the early [[18th century|18th centuries]].
2024-09-10 10:50:39 
2024-09-10 10:50:39 Other also called '''amos''', the two worlds and two letters were still called '''amos''' or '''''Amos Amos''''', sometimes called '''Amos'''. In addition to this view, amos AMos also meant a single '''amos''' in a meaning of an amos, and are usually described as [[amos (circulating amos)|amos]] and as [[cosmic amos|amos]] in a different period. In addition, this is what is the whole amount of amos the cosmic subset is amos, or the emotional amou
2024-09-10 10:50:39 Inference time: 2.12 seconds
2024-09-10 10:50:39 Tokens per second: 235.76
2024-09-10 10:50:39 ---------------
2024-09-10 10:50:39 Average tokens per second: 233.30
2024-09-10 10:50:39 tokens per iteration will be: 8,192
2024-09-10 10:50:39 found vocab_size = 27 (inside ../../data/text8/meta.pkl)
2024-09-10 10:50:39 Initializing a new model from scratch
2024-09-10 10:50:39 number of parameters: 10.63M
2024-09-10 10:50:39 num decayed parameter tensors: 26, with 10,725,504 parameters
2024-09-10 10:50:39 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 10:50:39 using fused AdamW: True
2024-09-10 10:50:39 compiling the model... (takes a ~minute)
2024-09-10 10:50:55 step 0: train loss 3.4364, val loss 3.4373
2024-09-10 10:51:14 iter 0: loss 3.4212, time 34935.87ms
2024-09-10 10:51:19 iter 100: loss 2.3865, time 42.75ms
2024-09-10 10:51:23 iter 200: loss 2.3383, time 43.40ms
2024-09-10 10:51:27 iter 300: loss 2.3483, time 44.02ms
2024-09-10 10:51:32 iter 400: loss 2.2147, time 43.72ms
2024-09-10 10:51:36 iter 500: loss 1.9893, time 43.58ms
2024-09-10 10:51:40 iter 600: loss 1.9379, time 43.64ms
2024-09-10 10:51:45 iter 700: loss 1.8216, time 43.68ms
2024-09-10 10:51:49 iter 800: loss 1.7639, time 43.61ms
2024-09-10 10:51:54 iter 900: loss 1.7117, time 44.10ms
2024-09-10 10:52:04 step 1000: train loss 1.5980, val loss 1.5753
2024-09-10 10:52:04 iter 1000: loss 1.6535, time 5860.22ms
2024-09-10 10:52:08 iter 1100: loss 1.6158, time 44.68ms
2024-09-10 10:52:13 iter 1200: loss 1.5686, time 44.64ms
2024-09-10 10:52:17 iter 1300: loss 1.5276, time 44.29ms
2024-09-10 10:52:22 iter 1400: loss 1.5214, time 43.51ms
2024-09-10 10:52:26 iter 1500: loss 1.5138, time 44.70ms
2024-09-10 10:52:30 iter 1600: loss 1.5704, time 45.25ms
2024-09-10 10:52:35 iter 1700: loss 1.5513, time 43.67ms
2024-09-10 10:52:39 iter 1800: loss 1.3379, time 45.30ms
2024-09-10 10:52:44 iter 1900: loss 1.4750, time 46.05ms
2024-09-10 10:52:54 step 2000: train loss 1.3689, val loss 1.3501
2024-09-10 10:52:54 iter 2000: loss 1.4791, time 5833.79ms
2024-09-10 10:52:58 iter 2100: loss 1.3813, time 45.12ms
2024-09-10 10:53:03 iter 2200: loss 1.4396, time 45.47ms
2024-09-10 10:53:07 iter 2300: loss 1.4485, time 45.43ms
2024-09-10 10:53:12 iter 2400: loss 1.4010, time 45.26ms
2024-09-10 10:53:16 iter 2500: loss 1.3788, time 45.18ms
2024-09-10 10:53:21 iter 2600: loss 1.3599, time 44.55ms
2024-09-10 10:53:25 iter 2700: loss 1.3440, time 44.81ms
2024-09-10 10:53:30 iter 2800: loss 1.4237, time 44.69ms
2024-09-10 10:53:34 iter 2900: loss 1.3944, time 45.60ms
2024-09-10 10:53:45 step 3000: train loss 1.2834, val loss 1.2711
2024-09-10 10:53:45 iter 3000: loss 1.3047, time 5830.33ms
2024-09-10 10:53:49 iter 3100: loss 1.3282, time 44.86ms
2024-09-10 10:53:54 iter 3200: loss 1.2748, time 44.65ms
2024-09-10 10:53:58 iter 3300: loss 1.3761, time 44.46ms
2024-09-10 10:54:03 iter 3400: loss 1.3082, time 43.93ms
2024-09-10 10:54:07 iter 3500: loss 1.3375, time 45.73ms
2024-09-10 10:54:12 iter 3600: loss 1.2854, time 45.56ms
2024-09-10 10:54:16 iter 3700: loss 1.3042, time 43.44ms
2024-09-10 10:54:21 iter 3800: loss 1.3371, time 44.88ms
2024-09-10 10:54:25 iter 3900: loss 1.2533, time 45.45ms
2024-09-10 10:54:35 step 4000: train loss 1.2463, val loss 1.2335
2024-09-10 10:54:35 iter 4000: loss 1.2984, time 5851.97ms
2024-09-10 10:54:40 iter 4100: loss 1.2786, time 45.21ms
2024-09-10 10:54:44 iter 4200: loss 1.2942, time 44.90ms
2024-09-10 10:54:49 iter 4300: loss 1.3371, time 45.20ms
2024-09-10 10:54:53 iter 4400: loss 1.2626, time 44.86ms
2024-09-10 10:54:58 iter 4500: loss 1.2658, time 45.24ms
2024-09-10 10:55:02 iter 4600: loss 1.3019, time 45.41ms
2024-09-10 10:55:07 iter 4700: loss 1.2733, time 45.78ms
2024-09-10 10:55:11 iter 4800: loss 1.2499, time 45.11ms
2024-09-10 10:55:16 iter 4900: loss 1.3260, time 45.28ms
2024-09-10 10:55:26 step 5000: train loss 1.2181, val loss 1.2046
2024-09-10 10:55:26 iter 5000: loss 1.3138, time 5855.64ms
2024-09-10 10:55:31 iter 5100: loss 1.2213, time 45.55ms
2024-09-10 10:55:35 iter 5200: loss 1.2888, time 45.45ms
2024-09-10 10:55:40 iter 5300: loss 1.2538, time 44.88ms
2024-09-10 10:55:44 iter 5400: loss 1.2468, time 44.80ms
2024-09-10 10:55:49 iter 5500: loss 1.2575, time 45.06ms
2024-09-10 10:55:53 iter 5600: loss 1.2447, time 44.89ms
2024-09-10 10:55:58 iter 5700: loss 1.2679, time 44.30ms
2024-09-10 10:56:02 iter 5800: loss 1.2674, time 49.04ms
2024-09-10 10:56:07 iter 5900: loss 1.2850, time 44.97ms
2024-09-10 10:56:17 step 6000: train loss 1.2003, val loss 1.1835
2024-09-10 10:56:17 iter 6000: loss 1.2575, time 6039.25ms
2024-09-10 10:56:22 iter 6100: loss 1.3013, time 44.83ms
2024-09-10 10:56:27 iter 6200: loss 1.2467, time 46.00ms
2024-09-10 10:56:31 iter 6300: loss 1.3255, time 44.77ms
2024-09-10 10:56:36 iter 6400: loss 1.2418, time 45.39ms
2024-09-10 10:56:40 iter 6500: loss 1.2222, time 44.77ms
2024-09-10 10:56:45 iter 6600: loss 1.2486, time 45.20ms
2024-09-10 10:56:49 iter 6700: loss 1.2488, time 44.69ms
2024-09-10 10:56:54 iter 6800: loss 1.2079, time 45.14ms
2024-09-10 10:56:58 iter 6900: loss 1.2921, time 45.95ms
2024-09-10 10:57:09 step 7000: train loss 1.1909, val loss 1.1730
2024-09-10 10:57:09 iter 7000: loss 1.2381, time 5948.80ms
2024-09-10 10:57:13 iter 7100: loss 1.2058, time 45.56ms
2024-09-10 10:57:18 iter 7200: loss 1.2224, time 46.21ms
2024-09-10 10:57:22 iter 7300: loss 1.2908, time 46.21ms
2024-09-10 10:57:27 iter 7400: loss 1.2921, time 44.71ms
2024-09-10 10:57:32 iter 7500: loss 1.2611, time 44.58ms
2024-09-10 10:57:36 iter 7600: loss 1.2795, time 46.09ms
2024-09-10 10:57:41 iter 7700: loss 1.2311, time 47.92ms
2024-09-10 10:57:45 iter 7800: loss 1.1879, time 45.24ms
2024-09-10 10:57:50 iter 7900: loss 1.2016, time 44.84ms
2024-09-10 10:58:00 step 8000: train loss 1.1734, val loss 1.1600
2024-09-10 10:58:00 iter 8000: loss 1.2376, time 6057.80ms
2024-09-10 10:58:05 iter 8100: loss 1.1791, time 46.78ms
2024-09-10 10:58:09 iter 8200: loss 1.1857, time 44.87ms
2024-09-10 10:58:14 iter 8300: loss 1.2911, time 46.24ms
2024-09-10 10:58:19 iter 8400: loss 1.2416, time 44.90ms
2024-09-10 10:58:23 iter 8500: loss 1.1762, time 45.36ms
2024-09-10 10:58:28 iter 8600: loss 1.2195, time 44.62ms
2024-09-10 10:58:32 iter 8700: loss 1.2171, time 45.32ms
2024-09-10 10:58:37 iter 8800: loss 1.2152, time 45.41ms
2024-09-10 10:58:41 iter 8900: loss 1.2549, time 44.77ms
2024-09-10 10:58:51 step 9000: train loss 1.1671, val loss 1.1511
2024-09-10 10:58:51 iter 9000: loss 1.1845, time 5838.80ms
2024-09-10 10:58:56 iter 9100: loss 1.2369, time 44.94ms
2024-09-10 10:59:00 iter 9200: loss 1.2189, time 45.21ms
2024-09-10 10:59:05 iter 9300: loss 1.1920, time 45.34ms
2024-09-10 10:59:09 iter 9400: loss 1.2395, time 45.37ms
2024-09-10 10:59:14 iter 9500: loss 1.2279, time 44.94ms
2024-09-10 10:59:18 iter 9600: loss 1.1761, time 45.11ms
2024-09-10 10:59:23 iter 9700: loss 1.1881, time 45.42ms
2024-09-10 10:59:27 iter 9800: loss 1.2092, time 45.36ms
2024-09-10 10:59:32 iter 9900: loss 1.2167, time 45.24ms
2024-09-10 10:59:42 step 10000: train loss 1.1616, val loss 1.1472
2024-09-10 10:59:42 iter 10000: loss 1.1835, time 5854.22ms
2024-09-10 10:59:47 iter 10100: loss 1.1755, time 45.33ms
2024-09-10 10:59:51 iter 10200: loss 1.1638, time 44.99ms
2024-09-10 10:59:56 iter 10300: loss 1.2304, time 45.49ms
2024-09-10 11:00:00 iter 10400: loss 1.2082, time 45.90ms
2024-09-10 11:00:05 iter 10500: loss 1.2394, time 45.24ms
2024-09-10 11:00:09 iter 10600: loss 1.2018, time 44.89ms
2024-09-10 11:00:14 iter 10700: loss 1.1542, time 45.68ms
2024-09-10 11:00:18 iter 10800: loss 1.1766, time 44.93ms
2024-09-10 11:00:23 iter 10900: loss 1.2493, time 44.85ms
2024-09-10 11:00:33 step 11000: train loss 1.1572, val loss 1.1401
2024-09-10 11:00:33 iter 11000: loss 1.2325, time 5863.30ms
2024-09-10 11:00:37 iter 11100: loss 1.1699, time 45.45ms
2024-09-10 11:00:42 iter 11200: loss 1.2118, time 44.87ms
2024-09-10 11:00:46 iter 11300: loss 1.1588, time 44.92ms
2024-09-10 11:00:51 iter 11400: loss 1.2329, time 44.72ms
2024-09-10 11:00:55 iter 11500: loss 1.1518, time 44.88ms
2024-09-10 11:01:00 iter 11600: loss 1.2045, time 45.01ms
2024-09-10 11:01:05 iter 11700: loss 1.1711, time 44.77ms
2024-09-10 11:01:09 iter 11800: loss 1.2052, time 45.23ms
2024-09-10 11:01:14 iter 11900: loss 1.2384, time 45.71ms
2024-09-10 11:01:24 step 12000: train loss 1.1451, val loss 1.1314
2024-09-10 11:01:24 iter 12000: loss 1.2207, time 5849.92ms
2024-09-10 11:01:28 iter 12100: loss 1.1573, time 44.68ms
2024-09-10 11:01:33 iter 12200: loss 1.2337, time 45.50ms
2024-09-10 11:01:37 iter 12300: loss 1.1465, time 45.35ms
2024-09-10 11:01:42 iter 12400: loss 1.1223, time 45.08ms
2024-09-10 11:01:46 iter 12500: loss 1.1857, time 45.15ms
2024-09-10 11:01:51 iter 12600: loss 1.1788, time 44.81ms
2024-09-10 11:01:55 iter 12700: loss 1.1908, time 44.51ms
2024-09-10 11:02:00 iter 12800: loss 1.2093, time 45.04ms
2024-09-10 11:02:04 iter 12900: loss 1.1992, time 45.38ms
2024-09-10 11:02:15 step 13000: train loss 1.1384, val loss 1.1238
2024-09-10 11:02:15 iter 13000: loss 1.1703, time 5849.48ms
2024-09-10 11:02:19 iter 13100: loss 1.2152, time 45.32ms
2024-09-10 11:02:24 iter 13200: loss 1.1620, time 45.44ms
2024-09-10 11:02:28 iter 13300: loss 1.2379, time 45.27ms
2024-09-10 11:02:33 iter 13400: loss 1.1745, time 45.57ms
2024-09-10 11:02:37 iter 13500: loss 1.2098, time 44.80ms
2024-09-10 11:02:42 iter 13600: loss 1.1722, time 44.96ms
2024-09-10 11:02:46 iter 13700: loss 1.1684, time 45.18ms
2024-09-10 11:02:51 iter 13800: loss 1.2098, time 45.85ms
2024-09-10 11:02:55 iter 13900: loss 1.1168, time 45.10ms
2024-09-10 11:03:05 step 14000: train loss 1.1379, val loss 1.1229
2024-09-10 11:03:05 iter 14000: loss 1.2071, time 5846.32ms
2024-09-10 11:03:10 iter 14100: loss 1.1637, time 44.96ms
2024-09-10 11:03:14 iter 14200: loss 1.0922, time 45.25ms
2024-09-10 11:03:19 iter 14300: loss 1.1886, time 44.66ms
2024-09-10 11:03:24 iter 14400: loss 1.1469, time 44.51ms
2024-09-10 11:03:28 iter 14500: loss 1.1255, time 44.68ms
2024-09-10 11:03:33 iter 14600: loss 1.1704, time 45.37ms
2024-09-10 11:03:37 iter 14700: loss 1.1415, time 45.15ms
2024-09-10 11:03:42 iter 14800: loss 1.2196, time 44.84ms
2024-09-10 11:03:46 iter 14900: loss 1.2265, time 45.21ms
2024-09-10 11:03:56 step 15000: train loss 1.1301, val loss 1.1172
2024-09-10 11:03:56 iter 15000: loss 1.1225, time 5858.78ms
2024-09-10 11:04:01 iter 15100: loss 1.2405, time 45.10ms
2024-09-10 11:04:05 iter 15200: loss 1.1848, time 45.07ms
2024-09-10 11:04:10 iter 15300: loss 1.1661, time 44.91ms
2024-09-10 11:04:14 iter 15400: loss 1.1713, time 45.06ms
2024-09-10 11:04:19 iter 15500: loss 1.1576, time 45.43ms
2024-09-10 11:04:23 iter 15600: loss 1.1841, time 45.48ms
2024-09-10 11:04:28 iter 15700: loss 1.1641, time 44.80ms
2024-09-10 11:04:32 iter 15800: loss 1.1198, time 45.01ms
2024-09-10 11:04:37 iter 15900: loss 1.1942, time 44.98ms
2024-09-10 11:04:47 step 16000: train loss 1.1246, val loss 1.1161
2024-09-10 11:04:47 iter 16000: loss 1.1475, time 5846.02ms
2024-09-10 11:04:52 iter 16100: loss 1.1442, time 44.94ms
2024-09-10 11:04:56 iter 16200: loss 1.1680, time 45.08ms
2024-09-10 11:05:01 iter 16300: loss 1.1689, time 45.08ms
2024-09-10 11:05:05 iter 16400: loss 1.1693, time 45.86ms
2024-09-10 11:05:10 iter 16500: loss 1.1366, time 44.81ms
2024-09-10 11:05:14 iter 16600: loss 1.1441, time 45.82ms
2024-09-10 11:05:19 iter 16700: loss 1.1964, time 45.14ms
2024-09-10 11:05:23 iter 16800: loss 1.2098, time 45.66ms
2024-09-10 11:05:28 iter 16900: loss 1.1340, time 45.33ms
2024-09-10 11:05:38 step 17000: train loss 1.1231, val loss 1.1058
2024-09-10 11:05:38 iter 17000: loss 1.1525, time 5854.41ms
2024-09-10 11:05:43 iter 17100: loss 1.1453, time 45.29ms
2024-09-10 11:05:47 iter 17200: loss 1.1209, time 45.51ms
2024-09-10 11:05:52 iter 17300: loss 1.1623, time 45.10ms
2024-09-10 11:05:56 iter 17400: loss 1.1879, time 45.38ms
2024-09-10 11:06:01 iter 17500: loss 1.2301, time 45.31ms
2024-09-10 11:06:05 iter 17600: loss 1.1647, time 44.68ms
2024-09-10 11:06:10 iter 17700: loss 1.1895, time 45.18ms
2024-09-10 11:06:14 iter 17800: loss 1.1065, time 45.46ms
2024-09-10 11:06:19 iter 17900: loss 1.1339, time 45.07ms
2024-09-10 11:06:29 step 18000: train loss 1.1170, val loss 1.1050
2024-09-10 11:06:29 iter 18000: loss 1.1788, time 5849.29ms
2024-09-10 11:06:33 iter 18100: loss 1.1675, time 45.17ms
2024-09-10 11:06:38 iter 18200: loss 1.1497, time 44.73ms
2024-09-10 11:06:43 iter 18300: loss 1.1646, time 45.01ms
2024-09-10 11:06:47 iter 18400: loss 1.1516, time 45.61ms
2024-09-10 11:06:52 iter 18500: loss 1.2066, time 45.35ms
2024-09-10 11:06:56 iter 18600: loss 1.1656, time 44.99ms
2024-09-10 11:07:01 iter 18700: loss 1.1108, time 45.23ms
2024-09-10 11:07:05 iter 18800: loss 1.1700, time 44.95ms
2024-09-10 11:07:10 iter 18900: loss 1.1158, time 45.23ms
2024-09-10 11:07:20 step 19000: train loss 1.1114, val loss 1.0958
2024-09-10 11:07:20 iter 19000: loss 1.1215, time 5849.14ms
2024-09-10 11:07:24 iter 19100: loss 1.1696, time 45.09ms
2024-09-10 11:07:29 iter 19200: loss 1.1926, time 45.41ms
2024-09-10 11:07:33 iter 19300: loss 1.1607, time 45.10ms
2024-09-10 11:07:38 iter 19400: loss 1.2112, time 44.97ms
2024-09-10 11:07:42 iter 19500: loss 1.0904, time 45.22ms
2024-09-10 11:07:47 iter 19600: loss 1.1492, time 45.39ms
2024-09-10 11:07:51 iter 19700: loss 1.1663, time 45.34ms
2024-09-10 11:07:56 iter 19800: loss 1.1330, time 44.79ms
2024-09-10 11:08:00 iter 19900: loss 1.0920, time 45.90ms
2024-09-10 11:08:11 step 20000: train loss 1.1087, val loss 1.0987
2024-09-10 11:08:11 iter 20000: loss 1.1859, time 5864.49ms
2024-09-10 11:08:15 iter 20100: loss 1.1673, time 44.06ms
2024-09-10 11:08:20 iter 20200: loss 1.1518, time 44.94ms
2024-09-10 11:08:24 iter 20300: loss 1.1071, time 45.30ms
2024-09-10 11:08:29 iter 20400: loss 1.1517, time 45.01ms
2024-09-10 11:08:33 iter 20500: loss 1.1733, time 44.98ms
2024-09-10 11:08:38 iter 20600: loss 1.1240, time 44.98ms
2024-09-10 11:08:42 iter 20700: loss 1.1705, time 45.73ms
2024-09-10 11:08:47 iter 20800: loss 1.1821, time 44.09ms
2024-09-10 11:08:51 iter 20900: loss 1.1368, time 45.02ms
2024-09-10 11:09:01 step 21000: train loss 1.1105, val loss 1.0928
2024-09-10 11:09:02 iter 21000: loss 1.1538, time 5876.99ms
2024-09-10 11:09:06 iter 21100: loss 1.1292, time 45.29ms
2024-09-10 11:09:11 iter 21200: loss 1.1057, time 44.69ms
2024-09-10 11:09:15 iter 21300: loss 1.1578, time 45.92ms
2024-09-10 11:09:20 iter 21400: loss 1.1654, time 44.83ms
2024-09-10 11:09:24 iter 21500: loss 1.1640, time 44.94ms
2024-09-10 11:09:29 iter 21600: loss 1.1976, time 45.84ms
2024-09-10 11:09:33 iter 21700: loss 1.0975, time 45.49ms
2024-09-10 11:09:38 iter 21800: loss 1.1225, time 45.26ms
2024-09-10 11:09:42 iter 21900: loss 1.1761, time 45.27ms
2024-09-10 11:09:52 step 22000: train loss 1.1042, val loss 1.0885
2024-09-10 11:09:52 iter 22000: loss 1.1568, time 5835.40ms
2024-09-10 11:09:57 iter 22100: loss 1.1814, time 45.63ms
2024-09-10 11:10:01 iter 22200: loss 1.1514, time 45.41ms
2024-09-10 11:10:06 iter 22300: loss 1.1131, time 45.62ms
2024-09-10 11:10:10 iter 22400: loss 1.1380, time 45.70ms
2024-09-10 11:10:15 iter 22500: loss 1.1304, time 43.95ms
2024-09-10 11:10:20 iter 22600: loss 1.1202, time 45.07ms
2024-09-10 11:10:24 iter 22700: loss 1.2170, time 43.97ms
2024-09-10 11:10:29 iter 22800: loss 1.1514, time 45.60ms
2024-09-10 11:10:33 iter 22900: loss 1.1329, time 44.82ms
2024-09-10 11:10:43 step 23000: train loss 1.1008, val loss 1.0827
2024-09-10 11:10:43 iter 23000: loss 1.0998, time 5849.62ms
2024-09-10 11:10:48 iter 23100: loss 1.1289, time 45.35ms
2024-09-10 11:10:52 iter 23200: loss 1.1255, time 46.52ms
2024-09-10 11:10:57 iter 23300: loss 1.1277, time 44.11ms
2024-09-10 11:11:01 iter 23400: loss 1.1332, time 44.10ms
2024-09-10 11:11:06 iter 23500: loss 1.1175, time 46.67ms
2024-09-10 11:11:10 iter 23600: loss 1.1444, time 43.50ms
2024-09-10 11:11:15 iter 23700: loss 1.1414, time 45.92ms
2024-09-10 11:11:19 iter 23800: loss 1.1511, time 45.06ms
2024-09-10 11:11:24 iter 23900: loss 1.1124, time 44.80ms
2024-09-10 11:11:34 step 24000: train loss 1.0996, val loss 1.0860
2024-09-10 11:11:34 iter 24000: loss 1.1502, time 5854.43ms
2024-09-10 11:11:39 iter 24100: loss 1.1202, time 44.86ms
2024-09-10 11:11:43 iter 24200: loss 1.1189, time 45.44ms
2024-09-10 11:11:48 iter 24300: loss 1.1936, time 44.87ms
2024-09-10 11:11:52 iter 24400: loss 1.2178, time 44.87ms
2024-09-10 11:11:57 iter 24500: loss 1.1334, time 45.32ms
2024-09-10 11:12:01 iter 24600: loss 1.1078, time 45.26ms
2024-09-10 11:12:06 iter 24700: loss 1.1545, time 45.32ms
2024-09-10 11:12:10 iter 24800: loss 1.1008, time 44.93ms
2024-09-10 11:12:15 iter 24900: loss 1.1651, time 45.32ms
2024-09-10 11:12:25 step 25000: train loss 1.0972, val loss 1.0823
2024-09-10 11:12:25 iter 25000: loss 1.0920, time 5837.21ms
2024-09-10 11:12:30 iter 25100: loss 1.1083, time 45.03ms
2024-09-10 11:12:34 iter 25200: loss 1.1639, time 44.76ms
2024-09-10 11:12:39 iter 25300: loss 1.1214, time 45.39ms
2024-09-10 11:12:43 iter 25400: loss 1.1479, time 44.83ms
2024-09-10 11:12:48 iter 25500: loss 1.1116, time 45.41ms
2024-09-10 11:12:52 iter 25600: loss 1.1785, time 45.53ms
2024-09-10 11:12:57 iter 25700: loss 1.1458, time 44.88ms
2024-09-10 11:13:01 iter 25800: loss 1.1304, time 45.24ms
2024-09-10 11:13:06 iter 25900: loss 1.1375, time 45.63ms
2024-09-10 11:13:16 step 26000: train loss 1.0955, val loss 1.0809
2024-09-10 11:13:16 iter 26000: loss 1.1786, time 5836.19ms
2024-09-10 11:13:20 iter 26100: loss 1.1507, time 44.88ms
2024-09-10 11:13:25 iter 26200: loss 1.1619, time 45.06ms
2024-09-10 11:13:30 iter 26300: loss 1.1104, time 45.75ms
2024-09-10 11:13:34 iter 26400: loss 1.1596, time 44.88ms
2024-09-10 11:13:39 iter 26500: loss 1.1627, time 45.55ms
2024-09-10 11:13:43 iter 26600: loss 1.0721, time 45.68ms
2024-09-10 11:13:48 iter 26700: loss 1.1339, time 45.84ms
2024-09-10 11:13:52 iter 26800: loss 1.1439, time 45.72ms
2024-09-10 11:13:57 iter 26900: loss 1.1367, time 45.71ms
2024-09-10 11:14:07 step 27000: train loss 1.0929, val loss 1.0763
2024-09-10 11:14:07 iter 27000: loss 1.0875, time 5847.05ms
2024-09-10 11:14:11 iter 27100: loss 1.1064, time 45.08ms
2024-09-10 11:14:16 iter 27200: loss 1.1336, time 45.65ms
2024-09-10 11:14:20 iter 27300: loss 1.1251, time 45.40ms
2024-09-10 11:14:25 iter 27400: loss 1.1845, time 45.69ms
2024-09-10 11:14:30 iter 27500: loss 1.1724, time 45.35ms
2024-09-10 11:14:34 iter 27600: loss 1.1411, time 45.03ms
2024-09-10 11:14:39 iter 27700: loss 1.1631, time 45.02ms
2024-09-10 11:14:43 iter 27800: loss 1.1205, time 45.29ms
2024-09-10 11:14:48 iter 27900: loss 1.1532, time 45.00ms
2024-09-10 11:14:58 step 28000: train loss 1.0852, val loss 1.0740
2024-09-10 11:14:58 iter 28000: loss 1.1635, time 5841.58ms
2024-09-10 11:15:02 iter 28100: loss 1.1241, time 44.98ms
2024-09-10 11:15:07 iter 28200: loss 1.1186, time 45.16ms
2024-09-10 11:15:11 iter 28300: loss 1.0605, time 45.24ms
2024-09-10 11:15:16 iter 28400: loss 1.1314, time 45.75ms
2024-09-10 11:15:20 iter 28500: loss 1.1368, time 45.44ms
2024-09-10 11:15:25 iter 28600: loss 1.1695, time 45.30ms
2024-09-10 11:15:30 iter 28700: loss 1.1851, time 46.39ms
2024-09-10 11:15:34 iter 28800: loss 1.1243, time 45.54ms
2024-09-10 11:15:39 iter 28900: loss 1.1025, time 45.59ms
2024-09-10 11:15:49 step 29000: train loss 1.0874, val loss 1.0734
2024-09-10 11:15:49 iter 29000: loss 1.2062, time 5839.85ms
2024-09-10 11:15:53 iter 29100: loss 1.1121, time 44.87ms
2024-09-10 11:15:58 iter 29200: loss 1.0745, time 45.35ms
2024-09-10 11:16:02 iter 29300: loss 1.1015, time 45.55ms
2024-09-10 11:16:07 iter 29400: loss 1.0941, time 45.98ms
2024-09-10 11:16:11 iter 29500: loss 1.1502, time 45.63ms
2024-09-10 11:16:16 iter 29600: loss 1.1435, time 44.44ms
2024-09-10 11:16:20 iter 29700: loss 1.1109, time 45.98ms
2024-09-10 11:16:25 iter 29800: loss 1.0946, time 44.97ms
2024-09-10 11:16:30 iter 29900: loss 1.0707, time 45.41ms
2024-09-10 11:16:40 step 30000: train loss 1.0831, val loss 1.0725
2024-09-10 11:16:40 iter 30000: loss 1.1408, time 5845.97ms
2024-09-10 11:16:44 iter 30100: loss 1.1389, time 46.16ms
2024-09-10 11:16:49 iter 30200: loss 1.1536, time 44.00ms
2024-09-10 11:16:53 iter 30300: loss 1.1275, time 44.00ms
2024-09-10 11:16:58 iter 30400: loss 1.1061, time 44.77ms
2024-09-10 11:17:02 iter 30500: loss 1.1894, time 44.83ms
2024-09-10 11:17:07 iter 30600: loss 1.1321, time 46.17ms
2024-09-10 11:17:11 iter 30700: loss 1.1045, time 45.13ms
2024-09-10 11:17:16 iter 30800: loss 1.1539, time 45.83ms
2024-09-10 11:17:20 iter 30900: loss 1.0907, time 45.36ms
2024-09-10 11:17:31 step 31000: train loss 1.0839, val loss 1.0699
2024-09-10 11:17:31 iter 31000: loss 1.0922, time 5845.03ms
2024-09-10 11:17:35 iter 31100: loss 1.2195, time 46.25ms
2024-09-10 11:17:40 iter 31200: loss 1.1714, time 45.53ms
2024-09-10 11:17:44 iter 31300: loss 1.1696, time 45.26ms
2024-09-10 11:17:49 iter 31400: loss 1.1655, time 45.26ms
2024-09-10 11:17:53 iter 31500: loss 1.1421, time 45.34ms
2024-09-10 11:17:58 iter 31600: loss 1.1073, time 45.34ms
2024-09-10 11:18:02 iter 31700: loss 1.1596, time 45.34ms
2024-09-10 11:18:07 iter 31800: loss 1.1597, time 45.09ms
2024-09-10 11:18:11 iter 31900: loss 1.1515, time 44.96ms
2024-09-10 11:18:21 step 32000: train loss 1.0782, val loss 1.0687
2024-09-10 11:18:22 iter 32000: loss 1.0488, time 5852.93ms
2024-09-10 11:18:26 iter 32100: loss 1.1091, time 44.76ms
2024-09-10 11:18:31 iter 32200: loss 1.1486, time 45.29ms
2024-09-10 11:18:35 iter 32300: loss 1.0971, time 45.25ms
2024-09-10 11:18:40 iter 32400: loss 1.0943, time 45.37ms
2024-09-10 11:18:44 iter 32500: loss 1.0925, time 44.00ms
2024-09-10 11:18:49 iter 32600: loss 1.1100, time 46.56ms
2024-09-10 11:18:53 iter 32700: loss 1.1318, time 44.70ms
2024-09-10 11:18:58 iter 32800: loss 1.1448, time 44.88ms
2024-09-10 11:19:03 iter 32900: loss 1.0762, time 46.26ms
2024-09-10 11:19:13 step 33000: train loss 1.0838, val loss 1.0644
2024-09-10 11:19:13 iter 33000: loss 1.1286, time 6101.21ms
2024-09-10 11:19:18 iter 33100: loss 1.1506, time 44.87ms
2024-09-10 11:19:22 iter 33200: loss 1.1360, time 46.63ms
2024-09-10 11:19:27 iter 33300: loss 1.1645, time 45.97ms
2024-09-10 11:19:31 iter 33400: loss 1.1042, time 45.11ms
2024-09-10 11:19:36 iter 33500: loss 1.0839, time 46.05ms
2024-09-10 11:19:40 iter 33600: loss 1.1368, time 45.29ms
2024-09-10 11:19:45 iter 33700: loss 1.1318, time 47.99ms
2024-09-10 11:19:50 iter 33800: loss 1.0749, time 44.97ms
2024-09-10 11:19:54 iter 33900: loss 1.0780, time 46.23ms
2024-09-10 11:20:04 step 34000: train loss 1.0758, val loss 1.0594
2024-09-10 11:20:04 iter 34000: loss 1.1517, time 5949.60ms
2024-09-10 11:20:09 iter 34100: loss 1.0967, time 46.80ms
2024-09-10 11:20:14 iter 34200: loss 1.1082, time 44.13ms
2024-09-10 11:20:18 iter 34300: loss 1.1785, time 44.74ms
2024-09-10 11:20:23 iter 34400: loss 1.1513, time 44.78ms
2024-09-10 11:20:27 iter 34500: loss 1.1129, time 44.98ms
2024-09-10 11:20:32 iter 34600: loss 1.1325, time 46.72ms
2024-09-10 11:20:37 iter 34700: loss 1.1806, time 47.09ms
2024-09-10 11:20:41 iter 34800: loss 1.1459, time 46.18ms
2024-09-10 11:22:23 iter 34900: loss 1.1152, time 41.74ms
2024-09-10 11:22:33 step 35000: train loss 1.0710, val loss 1.0631
2024-09-10 11:22:33 iter 35000: loss 1.1197, time 5706.08ms
2024-09-10 11:22:37 iter 35100: loss 1.1128, time 42.65ms
2024-09-10 11:22:41 iter 35200: loss 1.0693, time 45.06ms
2024-09-10 11:22:46 iter 35300: loss 1.1378, time 42.42ms
2024-09-10 11:22:50 iter 35400: loss 1.0981, time 43.59ms
2024-09-10 11:22:54 iter 35500: loss 1.1529, time 43.39ms
2024-09-10 11:22:59 iter 35600: loss 1.1581, time 44.71ms
2024-09-10 11:23:03 iter 35700: loss 1.1514, time 43.30ms
2024-09-10 11:23:08 iter 35800: loss 1.1426, time 44.90ms
2024-09-10 11:23:12 iter 35900: loss 1.2005, time 43.70ms
2024-09-10 11:23:22 step 36000: train loss 1.0753, val loss 1.0595
2024-09-10 11:23:22 iter 36000: loss 1.1165, time 5792.67ms
2024-09-10 11:23:26 iter 36100: loss 1.1667, time 45.36ms
2024-09-10 11:23:31 iter 36200: loss 1.1503, time 44.10ms
2024-09-10 11:23:35 iter 36300: loss 1.1167, time 44.72ms
2024-09-10 11:23:40 iter 36400: loss 1.1125, time 44.57ms
2024-09-10 11:23:44 iter 36500: loss 1.1236, time 44.22ms
2024-09-10 11:23:49 iter 36600: loss 1.1195, time 44.17ms
2024-09-10 11:23:53 iter 36700: loss 1.1612, time 45.27ms
2024-09-10 11:23:58 iter 36800: loss 1.0533, time 44.44ms
2024-09-10 11:24:02 iter 36900: loss 1.1537, time 44.82ms
2024-09-10 11:24:12 step 37000: train loss 1.0707, val loss 1.0535
2024-09-10 11:24:12 iter 37000: loss 1.1488, time 5800.25ms
2024-09-10 11:24:17 iter 37100: loss 1.0684, time 44.90ms
2024-09-10 11:24:21 iter 37200: loss 1.1210, time 44.16ms
2024-09-10 11:24:26 iter 37300: loss 1.1104, time 44.49ms
2024-09-10 11:24:30 iter 37400: loss 1.1002, time 44.76ms
2024-09-10 11:24:35 iter 37500: loss 1.1110, time 45.66ms
2024-09-10 11:24:39 iter 37600: loss 1.1542, time 44.54ms
2024-09-10 11:24:44 iter 37700: loss 1.1150, time 45.10ms
2024-09-10 11:24:48 iter 37800: loss 1.1385, time 45.70ms
2024-09-10 11:24:53 iter 37900: loss 1.1059, time 45.34ms
2024-09-10 11:25:03 step 38000: train loss 1.0707, val loss 1.0477
2024-09-10 11:25:03 iter 38000: loss 1.1309, time 5828.53ms
2024-09-10 11:25:07 iter 38100: loss 1.0842, time 45.33ms
2024-09-10 11:25:12 iter 38200: loss 1.1211, time 44.91ms
2024-09-10 11:25:16 iter 38300: loss 1.1329, time 46.27ms
2024-09-10 11:25:21 iter 38400: loss 1.1304, time 45.71ms
2024-09-10 11:25:25 iter 38500: loss 1.1378, time 44.79ms
2024-09-10 11:25:30 iter 38600: loss 1.1399, time 45.29ms
2024-09-10 11:25:35 iter 38700: loss 1.0976, time 44.95ms
2024-09-10 11:25:39 iter 38800: loss 1.1247, time 44.95ms
2024-09-10 11:25:44 iter 38900: loss 1.0894, time 46.09ms
2024-09-10 11:25:54 step 39000: train loss 1.0657, val loss 1.0530
2024-09-10 11:25:54 iter 39000: loss 1.1489, time 5832.55ms
2024-09-10 11:25:58 iter 39100: loss 1.1694, time 46.05ms
2024-09-10 11:26:03 iter 39200: loss 1.1065, time 44.81ms
2024-09-10 11:26:07 iter 39300: loss 1.1520, time 45.35ms
2024-09-10 11:26:12 iter 39400: loss 1.1261, time 45.39ms
2024-09-10 11:26:16 iter 39500: loss 1.1303, time 45.45ms
2024-09-10 11:26:21 iter 39600: loss 1.0901, time 44.93ms
2024-09-10 11:26:26 iter 39700: loss 1.0897, time 44.75ms
2024-09-10 11:26:30 iter 39800: loss 1.1700, time 45.64ms
2024-09-10 11:26:35 iter 39900: loss 1.0692, time 45.37ms
2024-09-10 11:26:45 step 40000: train loss 1.0641, val loss 1.0500
2024-09-10 11:26:45 iter 40000: loss 1.0771, time 5832.57ms
2024-09-10 11:26:49 iter 40100: loss 1.0721, time 45.35ms
2024-09-10 11:26:54 iter 40200: loss 1.0800, time 45.39ms
2024-09-10 11:26:58 iter 40300: loss 1.1273, time 45.69ms
2024-09-10 11:27:03 iter 40400: loss 1.0938, time 45.27ms
2024-09-10 11:27:07 iter 40500: loss 1.0675, time 45.37ms
2024-09-10 11:27:12 iter 40600: loss 1.1151, time 45.91ms
2024-09-10 11:27:17 iter 40700: loss 1.1330, time 45.00ms
2024-09-10 11:27:21 iter 40800: loss 1.1735, time 46.19ms
2024-09-10 11:27:26 iter 40900: loss 1.0756, time 45.33ms
2024-09-10 11:27:36 step 41000: train loss 1.0675, val loss 1.0502
2024-09-10 11:27:36 iter 41000: loss 1.1527, time 5836.28ms
2024-09-10 11:27:40 iter 41100: loss 1.1418, time 45.84ms
2024-09-10 11:27:45 iter 41200: loss 1.0735, time 45.38ms
2024-09-10 11:27:49 iter 41300: loss 1.1209, time 45.08ms
2024-09-10 11:27:54 iter 41400: loss 1.0923, time 44.95ms
2024-09-10 11:27:59 iter 41500: loss 1.1191, time 45.06ms
2024-09-10 11:28:03 iter 41600: loss 1.0780, time 47.14ms
2024-09-10 11:28:08 iter 41700: loss 1.1009, time 45.08ms
2024-09-10 11:28:12 iter 41800: loss 1.1201, time 44.92ms
2024-09-10 11:28:17 iter 41900: loss 1.0966, time 45.91ms
2024-09-10 11:28:27 step 42000: train loss 1.0652, val loss 1.0525
2024-09-10 11:28:27 iter 42000: loss 1.1012, time 5846.74ms
2024-09-10 11:28:31 iter 42100: loss 1.1354, time 44.24ms
2024-09-10 11:28:36 iter 42200: loss 1.0913, time 44.94ms
2024-09-10 11:28:40 iter 42300: loss 1.0884, time 45.25ms
2024-09-10 11:28:45 iter 42400: loss 1.0829, time 45.80ms
2024-09-10 11:28:50 iter 42500: loss 1.0723, time 45.86ms
2024-09-10 11:28:54 iter 42600: loss 1.1084, time 45.22ms
2024-09-10 11:28:59 iter 42700: loss 1.0923, time 45.08ms
2024-09-10 11:29:03 iter 42800: loss 1.1069, time 44.79ms
2024-09-10 11:29:08 iter 42900: loss 1.1253, time 46.19ms
2024-09-10 11:29:18 step 43000: train loss 1.0650, val loss 1.0461
2024-09-10 11:29:18 iter 43000: loss 1.0695, time 5843.91ms
2024-09-10 11:29:22 iter 43100: loss 1.1266, time 45.64ms
2024-09-10 11:29:27 iter 43200: loss 1.1014, time 45.47ms
2024-09-10 11:29:31 iter 43300: loss 1.1264, time 45.38ms
2024-09-10 11:29:36 iter 43400: loss 1.1530, time 45.04ms
2024-09-10 11:29:41 iter 43500: loss 1.0897, time 45.45ms
2024-09-10 11:29:45 iter 43600: loss 1.1411, time 45.72ms
2024-09-10 11:29:50 iter 43700: loss 1.1301, time 46.26ms
2024-09-10 11:29:54 iter 43800: loss 1.1232, time 45.90ms
2024-09-10 11:29:59 iter 43900: loss 1.0959, time 44.81ms
2024-09-10 11:30:09 step 44000: train loss 1.0605, val loss 1.0439
2024-09-10 11:30:09 iter 44000: loss 1.0991, time 5843.05ms
2024-09-10 11:30:14 iter 44100: loss 1.0683, time 45.56ms
2024-09-10 11:30:18 iter 44200: loss 1.0672, time 45.85ms
2024-09-10 11:30:23 iter 44300: loss 1.1026, time 45.81ms
2024-09-10 11:30:27 iter 44400: loss 1.1084, time 44.99ms
2024-09-10 11:30:32 iter 44500: loss 1.0984, time 45.59ms
2024-09-10 11:30:36 iter 44600: loss 1.1099, time 45.79ms
2024-09-10 11:30:41 iter 44700: loss 1.0627, time 45.27ms
2024-09-10 11:30:45 iter 44800: loss 1.1642, time 45.61ms
2024-09-10 11:30:50 iter 44900: loss 1.0889, time 45.29ms
2024-09-10 11:31:00 step 45000: train loss 1.0576, val loss 1.0374
2024-09-10 11:31:00 iter 45000: loss 1.0629, time 5836.67ms
2024-09-10 11:31:04 iter 45100: loss 1.0457, time 45.24ms
2024-09-10 11:31:09 iter 45200: loss 1.0968, time 45.48ms
2024-09-10 11:31:14 iter 45300: loss 1.0825, time 45.14ms
2024-09-10 11:31:18 iter 45400: loss 1.1444, time 45.46ms
2024-09-10 11:31:23 iter 45500: loss 1.0839, time 45.86ms
2024-09-10 11:31:27 iter 45600: loss 1.1349, time 45.04ms
2024-09-10 11:31:32 iter 45700: loss 1.0781, time 44.87ms
2024-09-10 11:31:36 iter 45800: loss 1.0746, time 45.30ms
2024-09-10 11:31:41 iter 45900: loss 1.1093, time 45.32ms
2024-09-10 11:31:51 step 46000: train loss 1.0579, val loss 1.0457
2024-09-10 11:31:51 iter 46000: loss 1.0604, time 5852.76ms
2024-09-10 11:31:56 iter 46100: loss 1.1001, time 53.42ms
2024-09-10 11:32:00 iter 46200: loss 1.0720, time 45.42ms
2024-09-10 11:32:05 iter 46300: loss 1.1096, time 45.90ms
2024-09-10 11:32:09 iter 46400: loss 1.1315, time 45.23ms
2024-09-10 11:32:14 iter 46500: loss 1.1184, time 44.99ms
2024-09-10 11:32:18 iter 46600: loss 1.1024, time 44.99ms
2024-09-10 11:32:23 iter 46700: loss 1.0718, time 45.19ms
2024-09-10 11:32:27 iter 46800: loss 1.0959, time 46.20ms
2024-09-10 11:32:32 iter 46900: loss 1.1119, time 45.27ms
2024-09-10 11:32:42 step 47000: train loss 1.0558, val loss 1.0431
2024-09-10 11:32:42 iter 47000: loss 1.1347, time 5847.29ms
2024-09-10 11:32:47 iter 47100: loss 1.1223, time 44.88ms
2024-09-10 11:32:51 iter 47200: loss 1.0982, time 45.54ms
2024-09-10 11:32:56 iter 47300: loss 1.1157, time 45.20ms
2024-09-10 11:33:00 iter 47400: loss 1.1059, time 45.25ms
2024-09-10 11:33:05 iter 47500: loss 1.1014, time 45.37ms
2024-09-10 11:33:09 iter 47600: loss 1.0445, time 45.52ms
2024-09-10 11:33:14 iter 47700: loss 1.0572, time 44.69ms
2024-09-10 11:33:18 iter 47800: loss 1.1252, time 46.60ms
2024-09-10 11:33:23 iter 47900: loss 1.1139, time 45.49ms
2024-09-10 11:33:33 step 48000: train loss 1.0527, val loss 1.0360
2024-09-10 11:33:33 iter 48000: loss 1.0490, time 5828.43ms
2024-09-10 11:33:37 iter 48100: loss 1.0868, time 45.31ms
2024-09-10 11:33:42 iter 48200: loss 1.0712, time 45.35ms
2024-09-10 11:33:47 iter 48300: loss 1.1030, time 45.39ms
2024-09-10 11:33:51 iter 48400: loss 1.1023, time 45.31ms
2024-09-10 11:33:56 iter 48500: loss 1.0948, time 44.55ms
2024-09-10 11:34:00 iter 48600: loss 1.0778, time 45.69ms
2024-09-10 11:34:05 iter 48700: loss 1.1071, time 45.58ms
2024-09-10 11:34:09 iter 48800: loss 1.1295, time 44.79ms
2024-09-10 11:34:14 iter 48900: loss 1.1091, time 45.79ms
2024-09-10 11:34:24 step 49000: train loss 1.0530, val loss 1.0340
2024-09-10 11:34:24 iter 49000: loss 1.1426, time 5834.53ms
2024-09-10 11:34:28 iter 49100: loss 1.1096, time 45.72ms
2024-09-10 11:34:33 iter 49200: loss 1.1235, time 45.15ms
2024-09-10 11:34:37 iter 49300: loss 1.0339, time 45.43ms
2024-09-10 11:34:42 iter 49400: loss 1.1248, time 45.27ms
2024-09-10 11:34:47 iter 49500: loss 1.0900, time 45.07ms
2024-09-10 11:34:51 iter 49600: loss 1.0986, time 45.73ms
2024-09-10 11:34:56 iter 49700: loss 1.1550, time 44.85ms
2024-09-10 11:35:00 iter 49800: loss 1.1412, time 45.31ms
2024-09-10 11:35:05 iter 49900: loss 1.0646, time 45.69ms
2024-09-10 11:35:15 step 50000: train loss 1.0486, val loss 1.0341
2024-09-10 11:35:15 iter 50000: loss 1.0694, time 5832.90ms
2024-09-10 11:35:19 iter 50100: loss 1.0852, time 45.27ms
2024-09-10 11:35:24 iter 50200: loss 1.1125, time 45.76ms
2024-09-10 11:35:28 iter 50300: loss 1.1189, time 44.96ms
2024-09-10 11:35:33 iter 50400: loss 1.1033, time 45.15ms
2024-09-10 11:35:37 iter 50500: loss 1.1218, time 45.08ms
2024-09-10 11:35:42 iter 50600: loss 0.9949, time 45.19ms
2024-09-10 11:35:46 iter 50700: loss 0.9950, time 45.34ms
2024-09-10 11:35:51 iter 50800: loss 1.0749, time 45.26ms
2024-09-10 11:35:56 iter 50900: loss 1.0913, time 44.88ms
2024-09-10 11:36:06 step 51000: train loss 1.0495, val loss 1.0320
2024-09-10 11:36:06 iter 51000: loss 1.0978, time 5825.84ms
2024-09-10 11:36:10 iter 51100: loss 1.0218, time 45.18ms
2024-09-10 11:36:15 iter 51200: loss 1.1171, time 45.22ms
2024-09-10 11:36:19 iter 51300: loss 1.0736, time 44.97ms
2024-09-10 11:36:24 iter 51400: loss 1.0653, time 45.64ms
2024-09-10 11:36:28 iter 51500: loss 1.1017, time 45.40ms
2024-09-10 11:36:33 iter 51600: loss 1.0759, time 44.94ms
2024-09-10 11:36:37 iter 51700: loss 1.0668, time 45.27ms
2024-09-10 11:36:42 iter 51800: loss 1.0870, time 45.01ms
2024-09-10 11:36:46 iter 51900: loss 1.0467, time 44.96ms
2024-09-10 11:36:57 step 52000: train loss 1.0454, val loss 1.0311
2024-09-10 11:36:57 iter 52000: loss 1.0641, time 5830.52ms
2024-09-10 11:37:01 iter 52100: loss 1.1072, time 45.45ms
2024-09-10 11:37:06 iter 52200: loss 1.0565, time 44.94ms
2024-09-10 11:37:10 iter 52300: loss 1.1200, time 45.27ms
2024-09-10 11:37:15 iter 52400: loss 1.1297, time 45.36ms
2024-09-10 11:37:19 iter 52500: loss 1.0899, time 44.88ms
2024-09-10 11:37:24 iter 52600: loss 1.0441, time 44.92ms
2024-09-10 11:37:28 iter 52700: loss 1.0989, time 45.82ms
2024-09-10 11:37:33 iter 52800: loss 1.0727, time 45.11ms
2024-09-10 11:37:37 iter 52900: loss 1.1104, time 44.97ms
2024-09-10 11:37:47 step 53000: train loss 1.0459, val loss 1.0321
2024-09-10 11:37:47 iter 53000: loss 1.0914, time 5824.47ms
2024-09-10 11:37:52 iter 53100: loss 1.0751, time 45.69ms
2024-09-10 11:37:56 iter 53200: loss 1.0532, time 44.91ms
2024-09-10 11:38:01 iter 53300: loss 1.1176, time 45.30ms
2024-09-10 11:38:06 iter 53400: loss 1.1041, time 45.45ms
2024-09-10 11:38:10 iter 53500: loss 1.0846, time 45.19ms
2024-09-10 11:38:15 iter 53600: loss 1.0356, time 45.15ms
2024-09-10 11:38:19 iter 53700: loss 1.0604, time 45.24ms
2024-09-10 11:38:24 iter 53800: loss 1.1167, time 44.82ms
2024-09-10 11:38:28 iter 53900: loss 1.0657, time 43.99ms
2024-09-10 11:38:38 step 54000: train loss 1.0479, val loss 1.0303
2024-09-10 11:38:38 iter 54000: loss 1.0067, time 5829.53ms
2024-09-10 11:38:43 iter 54100: loss 1.1423, time 45.08ms
2024-09-10 11:38:47 iter 54200: loss 1.0390, time 45.90ms
2024-09-10 11:38:52 iter 54300: loss 1.0477, time 45.33ms
2024-09-10 11:38:56 iter 54400: loss 1.0982, time 44.91ms
2024-09-10 11:39:01 iter 54500: loss 1.0946, time 45.77ms
2024-09-10 11:39:05 iter 54600: loss 1.1111, time 45.31ms
2024-09-10 11:39:10 iter 54700: loss 1.0532, time 44.72ms
2024-09-10 11:39:15 iter 54800: loss 1.0986, time 45.03ms
2024-09-10 11:39:19 iter 54900: loss 1.0735, time 44.79ms
2024-09-10 11:39:29 step 55000: train loss 1.0397, val loss 1.0316
2024-09-10 11:39:29 iter 55000: loss 1.0830, time 5837.22ms
2024-09-10 11:39:34 iter 55100: loss 1.0348, time 45.38ms
2024-09-10 11:39:38 iter 55200: loss 1.0949, time 45.60ms
2024-09-10 11:39:43 iter 55300: loss 1.0749, time 45.54ms
2024-09-10 11:39:47 iter 55400: loss 1.0934, time 45.49ms
2024-09-10 11:39:52 iter 55500: loss 1.1583, time 44.86ms
2024-09-10 11:39:56 iter 55600: loss 1.0584, time 45.21ms
2024-09-10 11:40:01 iter 55700: loss 1.0691, time 46.20ms
2024-09-10 11:40:05 iter 55800: loss 1.0880, time 44.65ms
2024-09-10 11:40:10 iter 55900: loss 1.0865, time 45.89ms
2024-09-10 11:40:20 step 56000: train loss 1.0436, val loss 1.0279
2024-09-10 11:40:20 iter 56000: loss 1.1017, time 5829.45ms
2024-09-10 11:40:25 iter 56100: loss 1.1234, time 45.22ms
2024-09-10 11:40:29 iter 56200: loss 1.0949, time 45.30ms
2024-09-10 11:40:34 iter 56300: loss 1.0750, time 45.30ms
2024-09-10 11:40:38 iter 56400: loss 1.0975, time 45.36ms
2024-09-10 11:40:43 iter 56500: loss 1.0893, time 44.96ms
2024-09-10 11:40:47 iter 56600: loss 1.0935, time 45.59ms
2024-09-10 11:40:52 iter 56700: loss 1.0339, time 44.92ms
2024-09-10 11:40:56 iter 56800: loss 1.0646, time 44.81ms
2024-09-10 11:41:01 iter 56900: loss 1.0812, time 45.02ms
2024-09-10 11:41:11 step 57000: train loss 1.0389, val loss 1.0253
2024-09-10 11:41:11 iter 57000: loss 1.0655, time 5821.00ms
2024-09-10 11:41:16 iter 57100: loss 1.0948, time 45.93ms
2024-09-10 11:41:20 iter 57200: loss 1.0678, time 44.92ms
2024-09-10 11:41:25 iter 57300: loss 1.1069, time 43.90ms
2024-09-10 11:41:29 iter 57400: loss 1.1316, time 45.02ms
2024-09-10 11:41:34 iter 57500: loss 1.1024, time 45.72ms
2024-09-10 11:41:38 iter 57600: loss 1.1122, time 43.91ms
2024-09-10 11:41:43 iter 57700: loss 1.0930, time 44.15ms
2024-09-10 11:41:47 iter 57800: loss 1.1142, time 45.02ms
2024-09-10 11:41:52 iter 57900: loss 1.0837, time 44.08ms
2024-09-10 11:42:02 step 58000: train loss 1.0370, val loss 1.0246
2024-09-10 11:42:02 iter 58000: loss 1.0183, time 5843.17ms
2024-09-10 11:42:06 iter 58100: loss 1.0640, time 45.54ms
2024-09-10 11:42:11 iter 58200: loss 1.0649, time 45.67ms
2024-09-10 11:42:15 iter 58300: loss 1.0333, time 44.92ms
2024-09-10 11:42:20 iter 58400: loss 1.1319, time 45.05ms
2024-09-10 11:42:24 iter 58500: loss 1.0828, time 44.90ms
2024-09-10 11:42:29 iter 58600: loss 1.0237, time 44.48ms
2024-09-10 11:42:33 iter 58700: loss 1.0923, time 45.50ms
2024-09-10 11:42:38 iter 58800: loss 1.0701, time 45.08ms
2024-09-10 11:42:43 iter 58900: loss 1.0666, time 44.51ms
2024-09-10 11:42:53 step 59000: train loss 1.0362, val loss 1.0224
2024-09-10 11:42:53 iter 59000: loss 1.0927, time 5847.80ms
2024-09-10 11:42:57 iter 59100: loss 1.0459, time 45.19ms
2024-09-10 11:43:02 iter 59200: loss 1.0679, time 44.71ms
2024-09-10 11:43:06 iter 59300: loss 1.1607, time 46.53ms
2024-09-10 11:43:11 iter 59400: loss 1.0370, time 45.52ms
2024-09-10 11:43:16 iter 59500: loss 1.0918, time 45.99ms
2024-09-10 11:43:20 iter 59600: loss 1.0840, time 46.54ms
2024-09-10 11:43:25 iter 59700: loss 1.1036, time 43.93ms
2024-09-10 11:43:29 iter 59800: loss 1.0749, time 46.04ms
2024-09-10 11:43:34 iter 59900: loss 1.0632, time 48.43ms
2024-09-10 11:43:44 step 60000: train loss 1.0334, val loss 1.0227
2024-09-10 11:43:45 iter 60000: loss 1.0787, time 6136.93ms
2024-09-10 11:43:49 iter 60100: loss 1.1463, time 47.80ms
2024-09-10 11:43:54 iter 60200: loss 1.1019, time 47.95ms
2024-09-10 11:43:58 iter 60300: loss 1.0341, time 45.85ms
2024-09-10 11:44:03 iter 60400: loss 1.0372, time 44.51ms
2024-09-10 11:44:08 iter 60500: loss 1.0204, time 46.95ms
2024-09-10 11:44:12 iter 60600: loss 1.0477, time 43.79ms
2024-09-10 11:44:17 iter 60700: loss 1.0829, time 44.96ms
2024-09-10 11:44:21 iter 60800: loss 1.0843, time 44.67ms
2024-09-10 11:44:26 iter 60900: loss 1.0353, time 44.78ms
2024-09-10 11:44:36 step 61000: train loss 1.0342, val loss 1.0206
2024-09-10 11:44:36 iter 61000: loss 1.0681, time 6071.54ms
2024-09-10 11:44:41 iter 61100: loss 1.0942, time 46.75ms
2024-09-10 11:44:46 iter 61200: loss 1.0635, time 44.45ms
2024-09-10 11:44:50 iter 61300: loss 1.0854, time 44.79ms
2024-09-10 11:44:55 iter 61400: loss 1.1132, time 46.90ms
2024-09-10 11:44:59 iter 61500: loss 1.0691, time 45.32ms
2024-09-10 11:45:04 iter 61600: loss 1.0655, time 46.39ms
2024-09-10 11:45:08 iter 61700: loss 1.0849, time 44.90ms
2024-09-10 11:45:13 iter 61800: loss 1.0707, time 45.08ms
2024-09-10 11:45:17 iter 61900: loss 1.1218, time 47.42ms
2024-09-10 11:45:28 step 62000: train loss 1.0298, val loss 1.0221
2024-09-10 11:45:28 iter 62000: loss 1.0232, time 5947.22ms
2024-09-10 11:45:32 iter 62100: loss 1.0392, time 44.30ms
2024-09-10 11:45:37 iter 62200: loss 1.0688, time 44.71ms
2024-09-10 11:45:41 iter 62300: loss 1.0401, time 45.04ms
2024-09-10 11:45:46 iter 62400: loss 1.0758, time 46.23ms
2024-09-10 11:45:50 iter 62500: loss 1.0877, time 44.93ms
2024-09-10 11:45:55 iter 62600: loss 0.9777, time 45.97ms
2024-09-10 11:46:00 iter 62700: loss 1.0760, time 45.29ms
2024-09-10 11:46:04 iter 62800: loss 1.0120, time 44.66ms
2024-09-10 11:46:09 iter 62900: loss 1.0604, time 46.93ms
2024-09-10 11:46:19 step 63000: train loss 1.0331, val loss 1.0135
2024-09-10 11:46:19 iter 63000: loss 1.0429, time 5939.33ms
2024-09-10 11:46:24 iter 63100: loss 1.0911, time 48.01ms
2024-09-10 11:46:28 iter 63200: loss 1.0748, time 46.81ms
2024-09-10 11:46:33 iter 63300: loss 1.0388, time 45.85ms
2024-09-10 11:46:37 iter 63400: loss 1.0245, time 46.65ms
2024-09-10 11:46:42 iter 63500: loss 1.1133, time 44.79ms
2024-09-10 11:46:46 iter 63600: loss 1.0407, time 44.74ms
2024-09-10 11:46:51 iter 63700: loss 1.0800, time 44.75ms
2024-09-10 11:46:55 iter 63800: loss 1.0664, time 46.45ms
2024-09-10 11:47:00 iter 63900: loss 1.0568, time 44.95ms
2024-09-10 11:47:10 step 64000: train loss 1.0269, val loss 1.0106
2024-09-10 11:47:10 iter 64000: loss 1.0704, time 5880.17ms
2024-09-10 11:47:15 iter 64100: loss 1.0651, time 44.47ms
2024-09-10 11:47:19 iter 64200: loss 1.0739, time 45.80ms
2024-09-10 11:47:24 iter 64300: loss 1.0760, time 45.78ms
2024-09-10 11:47:28 iter 64400: loss 0.9723, time 47.37ms
2024-09-10 11:47:33 iter 64500: loss 1.0755, time 44.73ms
2024-09-10 11:47:37 iter 64600: loss 1.0807, time 44.88ms
2024-09-10 11:47:42 iter 64700: loss 1.0645, time 44.73ms
2024-09-10 11:47:47 iter 64800: loss 1.1093, time 46.14ms
2024-09-10 11:47:51 iter 64900: loss 1.0735, time 44.73ms
2024-09-10 11:48:01 step 65000: train loss 1.0281, val loss 1.0143
2024-09-10 11:48:01 iter 65000: loss 1.0738, time 5900.26ms
2024-09-10 11:48:06 iter 65100: loss 1.0612, time 45.83ms
2024-09-10 11:48:10 iter 65200: loss 1.0507, time 45.81ms
2024-09-10 11:48:15 iter 65300: loss 1.0285, time 45.04ms
2024-09-10 11:48:20 iter 65400: loss 1.0535, time 44.61ms
2024-09-10 11:48:24 iter 65500: loss 1.0398, time 44.74ms
2024-09-10 11:48:29 iter 65600: loss 1.0705, time 44.96ms
2024-09-10 11:48:33 iter 65700: loss 1.0155, time 46.33ms
2024-09-10 11:48:38 iter 65800: loss 1.0488, time 46.18ms
2024-09-10 11:48:42 iter 65900: loss 1.1050, time 46.51ms
2024-09-10 11:48:53 step 66000: train loss 1.0231, val loss 1.0148
2024-09-10 11:48:53 iter 66000: loss 1.0540, time 6074.08ms
2024-09-10 11:48:57 iter 66100: loss 1.0507, time 44.60ms
2024-09-10 11:49:02 iter 66200: loss 1.1015, time 46.05ms
2024-09-10 11:49:06 iter 66300: loss 1.0349, time 46.06ms
2024-09-10 11:49:11 iter 66400: loss 1.0646, time 44.89ms
2024-09-10 11:49:16 iter 66500: loss 0.9815, time 44.81ms
2024-09-10 11:49:20 iter 66600: loss 1.0624, time 46.27ms
2024-09-10 11:49:25 iter 66700: loss 1.0274, time 45.32ms
2024-09-10 11:49:29 iter 66800: loss 1.1298, time 45.45ms
2024-09-10 11:49:34 iter 66900: loss 1.0772, time 47.10ms
2024-09-10 11:49:44 step 67000: train loss 1.0253, val loss 1.0037
2024-09-10 11:49:44 iter 67000: loss 1.0535, time 5951.76ms
2024-09-10 11:49:49 iter 67100: loss 1.0816, time 44.35ms
2024-09-10 11:49:53 iter 67200: loss 1.0972, time 45.72ms
2024-09-10 11:49:58 iter 67300: loss 1.0631, time 45.36ms
2024-09-10 11:50:03 iter 67400: loss 1.1365, time 45.38ms
2024-09-10 11:50:07 iter 67500: loss 1.0235, time 46.04ms
2024-09-10 11:50:12 iter 67600: loss 1.0171, time 45.00ms
2024-09-10 11:50:16 iter 67700: loss 1.1148, time 44.81ms
2024-09-10 11:50:21 iter 67800: loss 1.0750, time 45.72ms
2024-09-10 11:50:26 iter 67900: loss 1.0992, time 46.17ms
2024-09-10 11:50:36 step 68000: train loss 1.0226, val loss 1.0069
2024-09-10 11:50:36 iter 68000: loss 1.0358, time 6072.13ms
2024-09-10 11:50:41 iter 68100: loss 1.0777, time 47.25ms
2024-09-10 11:50:45 iter 68200: loss 1.0516, time 45.02ms
2024-09-10 11:50:50 iter 68300: loss 1.1543, time 47.48ms
2024-09-10 11:50:54 iter 68400: loss 1.1249, time 46.15ms
2024-09-10 11:50:59 iter 68500: loss 1.0730, time 46.31ms
2024-09-10 11:51:04 iter 68600: loss 1.0079, time 46.28ms
2024-09-10 11:51:08 iter 68700: loss 1.0312, time 44.89ms
2024-09-10 11:51:13 iter 68800: loss 1.0952, time 45.20ms
2024-09-10 11:51:17 iter 68900: loss 1.1175, time 45.90ms
2024-09-10 11:51:27 step 69000: train loss 1.0228, val loss 1.0085
2024-09-10 11:51:27 iter 69000: loss 1.0349, time 5825.31ms
2024-09-10 11:51:32 iter 69100: loss 1.0742, time 45.25ms
2024-09-10 11:51:36 iter 69200: loss 1.0976, time 46.23ms
2024-09-10 11:51:41 iter 69300: loss 1.0844, time 44.86ms
2024-09-10 11:51:45 iter 69400: loss 1.0490, time 44.58ms
2024-09-10 11:51:50 iter 69500: loss 1.0448, time 45.52ms
2024-09-10 11:51:54 iter 69600: loss 0.9576, time 45.11ms
2024-09-10 11:51:59 iter 69700: loss 1.0437, time 45.29ms
2024-09-10 11:52:04 iter 69800: loss 1.0643, time 44.88ms
2024-09-10 11:52:08 iter 69900: loss 1.0448, time 45.49ms
2024-09-10 11:52:18 step 70000: train loss 1.0186, val loss 1.0092
2024-09-10 11:52:18 iter 70000: loss 1.0521, time 5820.06ms
2024-09-10 11:52:23 iter 70100: loss 1.0541, time 45.36ms
2024-09-10 11:52:27 iter 70200: loss 1.0489, time 45.00ms
2024-09-10 11:52:32 iter 70300: loss 1.0289, time 44.79ms
2024-09-10 11:52:36 iter 70400: loss 1.0232, time 44.93ms
2024-09-10 11:52:41 iter 70500: loss 1.0528, time 44.71ms
2024-09-10 11:52:45 iter 70600: loss 1.0579, time 44.76ms
2024-09-10 11:52:50 iter 70700: loss 1.0183, time 44.86ms
2024-09-10 11:52:54 iter 70800: loss 1.1051, time 44.96ms
2024-09-10 11:52:59 iter 70900: loss 1.0982, time 45.26ms
2024-09-10 11:53:09 step 71000: train loss 1.0177, val loss 1.0070
2024-09-10 11:53:09 iter 71000: loss 1.0465, time 5828.80ms
2024-09-10 11:53:14 iter 71100: loss 1.0697, time 45.48ms
2024-09-10 11:53:18 iter 71200: loss 1.0568, time 44.91ms
2024-09-10 11:53:23 iter 71300: loss 1.1100, time 45.34ms
2024-09-10 11:53:27 iter 71400: loss 1.1045, time 44.95ms
2024-09-10 11:53:32 iter 71500: loss 1.1092, time 45.55ms
2024-09-10 11:53:36 iter 71600: loss 1.0770, time 44.79ms
2024-09-10 11:53:41 iter 71700: loss 1.0479, time 45.01ms
2024-09-10 11:53:45 iter 71800: loss 1.0578, time 45.75ms
2024-09-10 11:53:50 iter 71900: loss 1.0334, time 45.05ms
2024-09-10 11:54:00 step 72000: train loss 1.0177, val loss 1.0018
2024-09-10 11:54:00 iter 72000: loss 1.0243, time 5839.96ms
2024-09-10 11:54:04 iter 72100: loss 1.0810, time 44.77ms
2024-09-10 11:54:09 iter 72200: loss 1.0896, time 44.80ms
2024-09-10 11:54:14 iter 72300: loss 1.0548, time 45.12ms
2024-09-10 11:54:18 iter 72400: loss 1.0741, time 44.71ms
2024-09-10 11:54:23 iter 72500: loss 1.0889, time 44.94ms
2024-09-10 11:54:27 iter 72600: loss 1.0017, time 45.29ms
2024-09-10 11:54:32 iter 72700: loss 1.0861, time 46.47ms
2024-09-10 11:54:36 iter 72800: loss 1.0213, time 45.37ms
2024-09-10 11:54:41 iter 72900: loss 1.0172, time 45.21ms
2024-09-10 11:54:51 step 73000: train loss 1.0199, val loss 1.0011
2024-09-10 11:54:51 iter 73000: loss 1.1015, time 5821.18ms
2024-09-10 11:54:55 iter 73100: loss 1.1103, time 44.86ms
2024-09-10 11:55:00 iter 73200: loss 1.0870, time 44.87ms
2024-09-10 11:55:04 iter 73300: loss 1.0631, time 44.84ms
2024-09-10 11:55:09 iter 73400: loss 1.1012, time 45.37ms
2024-09-10 11:55:13 iter 73500: loss 1.0967, time 44.69ms
2024-09-10 11:55:18 iter 73600: loss 1.0676, time 45.34ms
2024-09-10 11:55:23 iter 73700: loss 1.0633, time 45.03ms
2024-09-10 11:55:27 iter 73800: loss 1.0635, time 44.87ms
2024-09-10 11:55:32 iter 73900: loss 1.0271, time 44.86ms
2024-09-10 11:55:42 step 74000: train loss 1.0162, val loss 1.0005
2024-09-10 11:55:42 iter 74000: loss 1.0046, time 5824.81ms
2024-09-10 11:55:46 iter 74100: loss 1.0442, time 45.35ms
2024-09-10 11:55:51 iter 74200: loss 1.0843, time 45.25ms
2024-09-10 11:55:55 iter 74300: loss 1.0513, time 45.18ms
2024-09-10 11:56:00 iter 74400: loss 1.0489, time 45.36ms
2024-09-10 11:56:04 iter 74500: loss 1.0429, time 45.28ms
2024-09-10 11:56:09 iter 74600: loss 1.0567, time 45.11ms
2024-09-10 11:56:13 iter 74700: loss 0.9845, time 45.16ms
2024-09-10 11:56:18 iter 74800: loss 1.0752, time 45.18ms
2024-09-10 11:56:23 iter 74900: loss 1.0267, time 47.25ms
2024-09-10 11:56:33 step 75000: train loss 1.0137, val loss 0.9999
2024-09-10 11:56:33 iter 75000: loss 1.0202, time 5844.40ms
2024-09-10 11:56:37 iter 75100: loss 1.0909, time 45.22ms
2024-09-10 11:56:42 iter 75200: loss 1.0318, time 45.33ms
2024-09-10 11:56:46 iter 75300: loss 1.0773, time 46.10ms
2024-09-10 11:56:51 iter 75400: loss 1.0412, time 44.98ms
2024-09-10 11:56:55 iter 75500: loss 1.1207, time 45.47ms
2024-09-10 11:57:00 iter 75600: loss 1.0723, time 44.72ms
2024-09-10 11:57:04 iter 75700: loss 1.0822, time 45.29ms
2024-09-10 11:57:09 iter 75800: loss 1.0847, time 45.34ms
2024-09-10 11:57:14 iter 75900: loss 1.0655, time 46.16ms
2024-09-10 11:57:24 step 76000: train loss 1.0124, val loss 0.9990
2024-09-10 11:57:24 iter 76000: loss 1.0221, time 5844.64ms
2024-09-10 11:57:28 iter 76100: loss 1.0079, time 45.28ms
2024-09-10 11:57:33 iter 76200: loss 1.0725, time 45.33ms
2024-09-10 11:57:37 iter 76300: loss 1.0724, time 45.27ms
2024-09-10 11:57:42 iter 76400: loss 1.0703, time 45.14ms
2024-09-10 11:57:46 iter 76500: loss 1.0630, time 45.17ms
2024-09-10 11:57:51 iter 76600: loss 1.0108, time 45.38ms
2024-09-10 11:57:55 iter 76700: loss 1.0347, time 45.40ms
2024-09-10 11:58:00 iter 76800: loss 1.0506, time 45.31ms
2024-09-10 11:58:05 iter 76900: loss 1.0582, time 47.72ms
2024-09-10 11:58:15 step 77000: train loss 1.0088, val loss 1.0031
2024-09-10 11:58:15 iter 77000: loss 1.0033, time 5815.69ms
2024-09-10 11:58:19 iter 77100: loss 1.0852, time 45.38ms
2024-09-10 11:58:24 iter 77200: loss 1.0970, time 45.41ms
2024-09-10 11:58:28 iter 77300: loss 1.0162, time 45.06ms
2024-09-10 11:58:33 iter 77400: loss 1.0174, time 45.08ms
2024-09-10 11:58:37 iter 77500: loss 1.0411, time 45.31ms
2024-09-10 11:58:42 iter 77600: loss 1.0506, time 45.24ms
2024-09-10 11:58:47 iter 77700: loss 1.0130, time 45.35ms
2024-09-10 11:58:51 iter 77800: loss 1.0939, time 45.38ms
2024-09-10 11:58:56 iter 77900: loss 1.0321, time 47.37ms
2024-09-10 11:59:06 step 78000: train loss 1.0122, val loss 0.9954
2024-09-10 11:59:06 iter 78000: loss 1.0294, time 5833.20ms
2024-09-10 11:59:10 iter 78100: loss 1.0640, time 45.33ms
2024-09-10 11:59:15 iter 78200: loss 1.0590, time 45.38ms
2024-09-10 11:59:19 iter 78300: loss 1.0007, time 45.66ms
2024-09-10 11:59:24 iter 78400: loss 1.0764, time 45.34ms
2024-09-10 11:59:29 iter 78500: loss 1.0889, time 45.24ms
2024-09-10 11:59:33 iter 78600: loss 1.0394, time 45.26ms
2024-09-10 11:59:38 iter 78700: loss 1.0291, time 45.37ms
2024-09-10 11:59:42 iter 78800: loss 1.0333, time 45.28ms
2024-09-10 11:59:47 iter 78900: loss 1.0391, time 47.60ms
2024-09-10 11:59:57 step 79000: train loss 1.0073, val loss 0.9976
2024-09-10 11:59:57 iter 79000: loss 1.0271, time 5833.14ms
2024-09-10 12:00:01 iter 79100: loss 0.9955, time 45.36ms
2024-09-10 12:00:06 iter 79200: loss 1.0076, time 45.32ms
2024-09-10 12:00:10 iter 79300: loss 1.0510, time 45.33ms
2024-09-10 12:00:15 iter 79400: loss 1.0236, time 45.40ms
2024-09-10 12:00:20 iter 79500: loss 1.0716, time 45.00ms
2024-09-10 12:00:24 iter 79600: loss 1.0769, time 45.28ms
2024-09-10 12:00:29 iter 79700: loss 1.0502, time 45.07ms
2024-09-10 12:00:33 iter 79800: loss 1.0203, time 45.50ms
2024-09-10 12:00:38 iter 79900: loss 1.0304, time 46.56ms
2024-09-10 12:00:48 step 80000: train loss 1.0091, val loss 0.9969
2024-09-10 12:00:48 iter 80000: loss 1.0903, time 5821.24ms
2024-09-10 12:00:52 iter 80100: loss 0.9985, time 45.31ms
2024-09-10 12:00:57 iter 80200: loss 1.0784, time 45.25ms
2024-09-10 12:01:02 iter 80300: loss 1.0413, time 45.34ms
2024-09-10 12:01:06 iter 80400: loss 1.0326, time 45.33ms
2024-09-10 12:01:11 iter 80500: loss 1.0406, time 45.54ms
2024-09-10 12:01:15 iter 80600: loss 1.1172, time 45.48ms
2024-09-10 12:01:20 iter 80700: loss 1.0757, time 45.42ms
2024-09-10 12:01:24 iter 80800: loss 1.0236, time 45.27ms
2024-09-10 12:01:29 iter 80900: loss 1.0241, time 45.54ms
2024-09-10 12:01:39 step 81000: train loss 1.0060, val loss 0.9942
2024-09-10 12:01:39 iter 81000: loss 1.0511, time 5827.68ms
2024-09-10 12:01:43 iter 81100: loss 1.0339, time 45.36ms
2024-09-10 12:01:48 iter 81200: loss 1.0485, time 45.33ms
2024-09-10 12:01:53 iter 81300: loss 1.0481, time 45.40ms
2024-09-10 12:01:57 iter 81400: loss 1.0355, time 45.39ms
2024-09-10 12:02:02 iter 81500: loss 1.0181, time 45.15ms
2024-09-10 12:02:06 iter 81600: loss 1.1012, time 45.34ms
2024-09-10 12:02:11 iter 81700: loss 1.0961, time 45.44ms
2024-09-10 12:02:15 iter 81800: loss 1.0770, time 45.44ms
2024-09-10 12:02:20 iter 81900: loss 1.0658, time 45.28ms
2024-09-10 12:02:30 step 82000: train loss 1.0067, val loss 0.9939
2024-09-10 12:02:30 iter 82000: loss 1.0524, time 5852.23ms
2024-09-10 12:02:35 iter 82100: loss 0.9947, time 45.06ms
2024-09-10 12:02:39 iter 82200: loss 1.0848, time 46.40ms
2024-09-10 12:02:44 iter 82300: loss 1.0061, time 45.29ms
2024-09-10 12:02:48 iter 82400: loss 1.0160, time 45.06ms
2024-09-10 12:02:53 iter 82500: loss 1.0757, time 44.77ms
2024-09-10 12:02:57 iter 82600: loss 1.0474, time 45.25ms
2024-09-10 12:03:02 iter 82700: loss 1.0871, time 47.36ms
2024-09-10 12:03:06 iter 82800: loss 1.0125, time 45.20ms
2024-09-10 12:03:11 iter 82900: loss 1.0458, time 45.02ms
2024-09-10 12:03:21 step 83000: train loss 1.0028, val loss 0.9911
2024-09-10 12:03:21 iter 83000: loss 1.1181, time 5840.60ms
2024-09-10 12:03:26 iter 83100: loss 1.0478, time 45.62ms
2024-09-10 12:03:30 iter 83200: loss 1.0188, time 45.39ms
2024-09-10 12:03:35 iter 83300: loss 1.0030, time 45.27ms
2024-09-10 12:03:39 iter 83400: loss 1.0048, time 44.30ms
2024-09-10 12:03:44 iter 83500: loss 1.0631, time 45.55ms
2024-09-10 12:03:48 iter 83600: loss 1.0166, time 45.09ms
2024-09-10 12:03:53 iter 83700: loss 1.0361, time 46.49ms
2024-09-10 12:03:57 iter 83800: loss 1.0064, time 45.29ms
2024-09-10 12:04:02 iter 83900: loss 1.0291, time 45.49ms
2024-09-10 12:04:12 step 84000: train loss 1.0035, val loss 0.9915
2024-09-10 12:04:12 iter 84000: loss 1.0152, time 5827.07ms
2024-09-10 12:04:17 iter 84100: loss 1.0674, time 45.32ms
2024-09-10 12:04:21 iter 84200: loss 1.0790, time 45.87ms
2024-09-10 12:04:26 iter 84300: loss 1.0419, time 46.42ms
2024-09-10 12:04:30 iter 84400: loss 1.0495, time 45.31ms
2024-09-10 12:04:35 iter 84500: loss 1.0183, time 46.04ms
2024-09-10 12:04:39 iter 84600: loss 1.1042, time 45.34ms
2024-09-10 12:04:44 iter 84700: loss 1.0646, time 44.97ms
2024-09-10 12:04:49 iter 84800: loss 1.0137, time 45.22ms
2024-09-10 12:04:53 iter 84900: loss 1.0447, time 46.22ms
2024-09-10 12:05:03 step 85000: train loss 1.0062, val loss 0.9918
2024-09-10 12:05:03 iter 85000: loss 1.0316, time 5839.07ms
2024-09-10 12:05:08 iter 85100: loss 1.0791, time 45.35ms
2024-09-10 12:05:12 iter 85200: loss 1.0383, time 45.51ms
2024-09-10 12:05:17 iter 85300: loss 1.0292, time 46.78ms
2024-09-10 12:05:21 iter 85400: loss 1.0387, time 45.24ms
2024-09-10 12:05:26 iter 85500: loss 1.0529, time 44.73ms
2024-09-10 12:05:31 iter 85600: loss 0.9821, time 45.32ms
2024-09-10 12:05:35 iter 85700: loss 1.0087, time 45.33ms
2024-09-10 12:05:40 iter 85800: loss 0.9832, time 45.39ms
2024-09-10 12:05:44 iter 85900: loss 1.1118, time 45.36ms
2024-09-10 12:05:54 step 86000: train loss 0.9989, val loss 0.9948
2024-09-10 12:05:54 iter 86000: loss 1.0116, time 5833.83ms
2024-09-10 12:05:59 iter 86100: loss 1.0285, time 45.80ms
2024-09-10 12:06:03 iter 86200: loss 1.0250, time 45.31ms
2024-09-10 12:06:08 iter 86300: loss 1.0498, time 44.90ms
2024-09-10 12:06:12 iter 86400: loss 1.0316, time 45.25ms
2024-09-10 12:06:17 iter 86500: loss 1.0188, time 46.58ms
2024-09-10 12:06:22 iter 86600: loss 1.0263, time 45.57ms
2024-09-10 12:06:26 iter 86700: loss 1.0296, time 45.39ms
2024-09-10 12:06:31 iter 86800: loss 1.0646, time 45.35ms
2024-09-10 12:06:35 iter 86900: loss 1.0729, time 45.01ms
2024-09-10 12:06:45 step 87000: train loss 0.9997, val loss 0.9885
2024-09-10 12:06:45 iter 87000: loss 1.0445, time 5823.54ms
2024-09-10 12:06:50 iter 87100: loss 1.0053, time 45.04ms
2024-09-10 12:06:54 iter 87200: loss 1.0286, time 45.29ms
2024-09-10 12:06:59 iter 87300: loss 1.0430, time 45.32ms
2024-09-10 12:07:03 iter 87400: loss 1.0312, time 45.37ms
2024-09-10 12:07:08 iter 87500: loss 1.0573, time 46.97ms
2024-09-10 12:07:13 iter 87600: loss 1.0910, time 44.70ms
2024-09-10 12:07:17 iter 87700: loss 1.0657, time 44.96ms
2024-09-10 12:07:22 iter 87800: loss 1.0641, time 45.45ms
2024-09-10 12:07:26 iter 87900: loss 1.0057, time 44.89ms
2024-09-10 12:07:36 step 88000: train loss 1.0029, val loss 0.9910
2024-09-10 12:07:36 iter 88000: loss 1.0700, time 5830.58ms
2024-09-10 12:07:41 iter 88100: loss 1.0105, time 45.38ms
2024-09-10 12:07:45 iter 88200: loss 1.0585, time 45.32ms
2024-09-10 12:07:50 iter 88300: loss 1.0345, time 45.68ms
2024-09-10 12:07:54 iter 88400: loss 1.0317, time 45.48ms
2024-09-10 12:07:59 iter 88500: loss 1.0473, time 46.48ms
2024-09-10 12:08:04 iter 88600: loss 1.0620, time 44.88ms
2024-09-10 12:08:08 iter 88700: loss 1.0225, time 44.26ms
2024-09-10 12:08:13 iter 88800: loss 0.9376, time 45.26ms
2024-09-10 12:08:17 iter 88900: loss 1.0409, time 44.88ms
2024-09-10 12:08:27 step 89000: train loss 0.9962, val loss 0.9874
2024-09-10 12:08:27 iter 89000: loss 1.0468, time 5833.58ms
2024-09-10 12:08:32 iter 89100: loss 1.0204, time 45.26ms
2024-09-10 12:08:36 iter 89200: loss 1.0597, time 45.65ms
2024-09-10 12:08:41 iter 89300: loss 1.1006, time 45.53ms
2024-09-10 12:08:46 iter 89400: loss 1.0514, time 45.36ms
2024-09-10 12:08:50 iter 89500: loss 1.0559, time 46.47ms
2024-09-10 12:08:55 iter 89600: loss 0.9931, time 45.34ms
2024-09-10 12:08:59 iter 89700: loss 1.0589, time 45.03ms
2024-09-10 12:09:04 iter 89800: loss 1.0311, time 45.39ms
2024-09-10 12:09:08 iter 89900: loss 1.0001, time 45.25ms
2024-09-10 12:09:18 step 90000: train loss 0.9989, val loss 0.9881
2024-09-10 12:09:18 iter 90000: loss 1.0291, time 5818.32ms
2024-09-10 12:09:23 iter 90100: loss 1.0327, time 44.68ms
2024-09-10 12:09:28 iter 90200: loss 1.0485, time 45.55ms
2024-09-10 12:09:32 iter 90300: loss 0.9985, time 47.33ms
2024-09-10 12:09:37 iter 90400: loss 1.0623, time 45.10ms
2024-09-10 12:09:41 iter 90500: loss 1.0141, time 44.93ms
2024-09-10 12:09:46 iter 90600: loss 1.0689, time 45.28ms
2024-09-10 12:09:50 iter 90700: loss 1.0111, time 45.36ms
2024-09-10 12:09:55 iter 90800: loss 1.0268, time 45.43ms
2024-09-10 12:09:59 iter 90900: loss 1.0755, time 45.48ms
2024-09-10 12:10:09 step 91000: train loss 0.9965, val loss 0.9844
2024-09-10 12:10:09 iter 91000: loss 1.0137, time 5816.96ms
2024-09-10 12:10:14 iter 91100: loss 1.0588, time 45.31ms
2024-09-10 12:10:19 iter 91200: loss 1.0264, time 45.28ms
2024-09-10 12:10:23 iter 91300: loss 1.1009, time 45.52ms
2024-09-10 12:10:28 iter 91400: loss 1.0946, time 46.31ms
2024-09-10 12:10:32 iter 91500: loss 1.0338, time 45.87ms
2024-09-10 12:10:37 iter 91600: loss 1.0130, time 45.45ms
2024-09-10 12:10:41 iter 91700: loss 1.0546, time 45.00ms
2024-09-10 12:10:46 iter 91800: loss 1.0665, time 45.89ms
2024-09-10 12:10:50 iter 91900: loss 0.9610, time 46.29ms
2024-09-10 12:11:00 step 92000: train loss 0.9946, val loss 0.9871
2024-09-10 12:11:00 iter 92000: loss 1.0111, time 5843.96ms
2024-09-10 12:11:05 iter 92100: loss 1.0844, time 45.66ms
2024-09-10 12:11:10 iter 92200: loss 1.0233, time 45.40ms
2024-09-10 12:11:14 iter 92300: loss 1.0567, time 45.43ms
2024-09-10 12:11:19 iter 92400: loss 1.0456, time 43.96ms
2024-09-10 12:11:23 iter 92500: loss 1.0134, time 43.88ms
2024-09-10 12:11:28 iter 92600: loss 1.0765, time 45.30ms
2024-09-10 12:11:32 iter 92700: loss 1.0391, time 44.94ms
2024-09-10 12:11:37 iter 92800: loss 1.0233, time 45.22ms
2024-09-10 12:11:41 iter 92900: loss 1.0753, time 45.50ms
2024-09-10 12:11:51 step 93000: train loss 0.9937, val loss 0.9857
2024-09-10 12:11:51 iter 93000: loss 1.0504, time 5819.05ms
2024-09-10 12:11:56 iter 93100: loss 1.0667, time 45.27ms
2024-09-10 12:12:00 iter 93200: loss 0.9983, time 44.87ms
2024-09-10 12:12:05 iter 93300: loss 1.0523, time 45.10ms
2024-09-10 12:12:10 iter 93400: loss 1.0044, time 45.01ms
2024-09-10 12:12:14 iter 93500: loss 1.0492, time 45.26ms
2024-09-10 12:12:19 iter 93600: loss 1.0635, time 45.68ms
2024-09-10 12:12:23 iter 93700: loss 1.0327, time 44.75ms
2024-09-10 12:12:28 iter 93800: loss 0.9971, time 45.20ms
2024-09-10 12:12:32 iter 93900: loss 1.0568, time 44.89ms
2024-09-10 12:12:42 step 94000: train loss 0.9952, val loss 0.9821
2024-09-10 12:12:42 iter 94000: loss 1.0324, time 5824.88ms
2024-09-10 12:12:47 iter 94100: loss 1.0434, time 45.26ms
2024-09-10 12:12:51 iter 94200: loss 1.0296, time 45.35ms
2024-09-10 12:12:56 iter 94300: loss 1.0389, time 45.34ms
2024-09-10 12:13:01 iter 94400: loss 1.0355, time 45.51ms
2024-09-10 12:13:05 iter 94500: loss 0.9888, time 45.73ms
2024-09-10 12:13:10 iter 94600: loss 1.0000, time 45.31ms
2024-09-10 12:13:14 iter 94700: loss 1.0227, time 45.14ms
2024-09-10 12:13:19 iter 94800: loss 1.0093, time 45.36ms
2024-09-10 12:13:23 iter 94900: loss 1.0246, time 45.04ms
2024-09-10 12:13:33 step 95000: train loss 0.9923, val loss 0.9846
2024-09-10 12:13:33 iter 95000: loss 1.0111, time 5839.52ms
2024-09-10 12:13:38 iter 95100: loss 1.0666, time 45.50ms
2024-09-10 12:13:42 iter 95200: loss 1.0440, time 44.87ms
2024-09-10 12:13:47 iter 95300: loss 1.0271, time 44.82ms
2024-09-10 12:13:52 iter 95400: loss 1.0235, time 45.00ms
2024-09-10 12:13:56 iter 95500: loss 1.0110, time 44.91ms
2024-09-10 12:14:01 iter 95600: loss 1.0061, time 45.42ms
2024-09-10 12:14:05 iter 95700: loss 1.0766, time 45.66ms
2024-09-10 12:14:10 iter 95800: loss 1.0459, time 44.93ms
2024-09-10 12:14:14 iter 95900: loss 1.0167, time 45.66ms
2024-09-10 12:14:24 step 96000: train loss 0.9930, val loss 0.9866
2024-09-10 12:14:24 iter 96000: loss 1.0694, time 5831.66ms
2024-09-10 12:14:29 iter 96100: loss 1.0299, time 44.85ms
2024-09-10 12:14:33 iter 96200: loss 1.0401, time 45.92ms
2024-09-10 12:14:38 iter 96300: loss 1.0335, time 44.94ms
2024-09-10 12:14:43 iter 96400: loss 1.0260, time 45.37ms
2024-09-10 12:14:47 iter 96500: loss 1.0401, time 44.82ms
2024-09-10 12:14:52 iter 96600: loss 0.9898, time 44.24ms
2024-09-10 12:14:56 iter 96700: loss 1.0292, time 44.23ms
2024-09-10 12:15:01 iter 96800: loss 1.0422, time 46.36ms
2024-09-10 12:15:05 iter 96900: loss 1.0664, time 45.72ms
2024-09-10 12:15:15 step 97000: train loss 0.9923, val loss 0.9799
2024-09-10 12:15:15 iter 97000: loss 0.9919, time 5817.67ms
2024-09-10 12:15:20 iter 97100: loss 1.0173, time 44.90ms
2024-09-10 12:15:24 iter 97200: loss 1.0614, time 46.03ms
2024-09-10 12:15:29 iter 97300: loss 1.0278, time 45.20ms
2024-09-10 12:15:33 iter 97400: loss 1.0450, time 45.22ms
2024-09-10 12:15:38 iter 97500: loss 1.0000, time 45.56ms
2024-09-10 12:15:43 iter 97600: loss 0.9644, time 46.26ms
2024-09-10 12:15:47 iter 97700: loss 1.0572, time 45.32ms
2024-09-10 12:15:52 iter 97800: loss 1.0411, time 44.43ms
2024-09-10 12:15:56 iter 97900: loss 1.0523, time 44.87ms
2024-09-10 12:16:06 step 98000: train loss 0.9927, val loss 0.9821
2024-09-10 12:16:06 iter 98000: loss 1.0414, time 5824.29ms
2024-09-10 12:16:11 iter 98100: loss 1.0218, time 45.05ms
2024-09-10 12:16:15 iter 98200: loss 1.0333, time 43.84ms
2024-09-10 12:16:20 iter 98300: loss 0.9814, time 45.81ms
2024-09-10 12:16:24 iter 98400: loss 1.0441, time 44.87ms
2024-09-10 12:16:29 iter 98500: loss 1.0264, time 45.33ms
2024-09-10 12:16:33 iter 98600: loss 0.9953, time 45.34ms
2024-09-10 12:16:38 iter 98700: loss 1.0380, time 45.40ms
2024-09-10 12:16:43 iter 98800: loss 1.0186, time 44.98ms
2024-09-10 12:16:47 iter 98900: loss 1.0343, time 45.25ms
2024-09-10 12:16:57 step 99000: train loss 0.9922, val loss 0.9822
2024-09-10 12:16:57 iter 99000: loss 1.0142, time 5823.33ms
2024-09-10 12:17:02 iter 99100: loss 1.0697, time 45.34ms
2024-09-10 12:17:06 iter 99200: loss 1.0413, time 46.58ms
2024-09-10 12:17:11 iter 99300: loss 1.0241, time 44.97ms
2024-09-10 12:17:15 iter 99400: loss 1.0674, time 45.34ms
2024-09-10 12:17:20 iter 99500: loss 1.0616, time 45.53ms
2024-09-10 12:17:24 iter 99600: loss 1.0606, time 45.15ms
2024-09-10 12:17:29 iter 99700: loss 1.0076, time 44.70ms
2024-09-10 12:17:33 iter 99800: loss 1.0586, time 45.15ms
2024-09-10 12:17:38 iter 99900: loss 1.0197, time 45.08ms
2024-09-10 12:17:48 step 100000: train loss 0.9896, val loss 0.9863
2024-09-10 12:17:48 iter 100000: loss 1.0027, time 5820.58ms
2024-09-10 12:17:48 training done
2024-09-10 12:17:48 Best validation loss: 0.9798951745033264
2024-09-10 12:17:48 Total train time: 87.15 mins
2024-09-10 12:17:48 Loading meta from ../../data/text8/meta.pkl...
2024-09-10 12:17:50 Sample 1:
2024-09-10 12:17:50  the mongol industry in the south the mongol subject to the island s crossing ground being aware of the island for the principal municipalities in the north of a similar state from the east to the territory it is possible to speak country it is considered to be at two six zero zero pounds as the largest country the country residing in the west and west along the east is one of the irish inscriptions on the side of the september two one two zero zero one by the bay of jerusalem in the same year th
2024-09-10 12:17:50 Inference time: 2.03 seconds
2024-09-10 12:17:50 Tokens per second: 246.70
2024-09-10 12:17:50 ---------------
2024-09-10 12:17:52 Sample 2:
2024-09-10 12:17:52  the rings were in fact recently the individual form the common neighbouring states of germany and united states are both worshipped in the united states members of the european union of germany during the united states and the soviet union many attempted to provide the resolution of the european union and the united nations act of one nine nine one have been adopted in the consumer and one nine nine two the european union program for a british community established in nations but in fact some ar
2024-09-10 12:17:52 Inference time: 2.14 seconds
2024-09-10 12:17:52 Tokens per second: 234.08
2024-09-10 12:17:52 ---------------
2024-09-10 12:17:54 Sample 3:
2024-09-10 12:17:54  legal debate over the house of commons on the department of state opposition military vote opposition prime minister of iraq and iraq the iraqi forces of the iraqi republic of iraq had not been either dissatisfied or necessitated by the process of state to protect the opposition from the iraqi revolution to the iraqi party and military human rights in iraq according to annotations of iraq was a governor of the country s political party in the one nine two two act of one nine three five the iraqi
2024-09-10 12:17:54 Inference time: 1.86 seconds
2024-09-10 12:17:54 Tokens per second: 268.28
2024-09-10 12:17:54 ---------------
2024-09-10 12:17:56 Sample 4:
2024-09-10 12:17:56  onto its successor as one nine five three and summer olympics and league in one nine five three and is defined as the note that the first company to recommend that a three the program was defined during the nation had the suffix basic wager the dominion of the program of the international model after shown as the first company to increase an international difference between nations and austrian history german and nations include an international policy of the united states security program of an
2024-09-10 12:17:56 Inference time: 1.85 seconds
2024-09-10 12:17:56 Tokens per second: 270.33
2024-09-10 12:17:56 ---------------
2024-09-10 12:17:58 Sample 5:
2024-09-10 12:17:58  one nine nine six market slowly separated from the red winds of mining with one four hours during the victims of the proposed group of governments instead of the general federal federal government and constitution of replacing india one six five four and seven three four with a true constitutional monarchy in one six four three the secretary of state of state became a major member of the federal government in one six five three in one six eight one the electoral elections were federal government
2024-09-10 12:17:58 Inference time: 1.95 seconds
2024-09-10 12:17:58 Tokens per second: 255.98
2024-09-10 12:17:58 ---------------
2024-09-10 12:18:00 Sample 6:
2024-09-10 12:18:00  he is very merely great under the transmission of the cell brass and trial art as of two zero zero five he sits in recent years and uses his work for a unique act the three facts that involve activity and other performance activity therefore activity reduces the performance of mercury in this context the health of education has not yet taken peace to activity at the same time it is the three medical record of a group education has developed as a conductor or as a symbol of the movement to the he
2024-09-10 12:18:00 Inference time: 1.93 seconds
2024-09-10 12:18:00 Tokens per second: 258.41
2024-09-10 12:18:00 ---------------
2024-09-10 12:18:02 Sample 7:
2024-09-10 12:18:02  eight nine english architect b one seven zero seven one eight zero four ariosto vagueli italian physicist b one seven seven four one eight one eight two nine ernest barton tappa german physicist b one seven two five one eight one seven seven eight george barton corner austrian poet b one seven two six one eight two zero joseph richard smith german physicist b one seven four six one eight four one john barton italian chemist b one seven four two one eight six three charles hemitide german author 
2024-09-10 12:18:02 Inference time: 1.95 seconds
2024-09-10 12:18:02 Tokens per second: 256.86
2024-09-10 12:18:02 ---------------
2024-09-10 12:18:04 Sample 8:
2024-09-10 12:18:04  and many other more comprehensive sources including politics occur in the so called programme based on the language of programme the programme is not a successful description of the public s television series which produced uncomprehensive records of the personal family company chairman of contemporary programmes that have been comprehensive and supported operating system was often inspired by the advancement of maximum programme and so as the internet development for the contemporary social pro
2024-09-10 12:18:04 Inference time: 1.93 seconds
2024-09-10 12:18:04 Tokens per second: 259.30
2024-09-10 12:18:04 ---------------
2024-09-10 12:18:06 Sample 9:
2024-09-10 12:18:06  last history of the two zero th century the people involved the direction of the artificial education the name of the great depression was held to be in the direction of jewish christianity and the ideal there is an extensive stature of evolution in the two zero th century educational biographical example the direction of the great depression with the german middle east to east of the eastern part of the two zero th century educational biography and the educational direction of educational studi
2024-09-10 12:18:06 Inference time: 1.89 seconds
2024-09-10 12:18:06 Tokens per second: 265.09
2024-09-10 12:18:06 ---------------
2024-09-10 12:18:08 Sample 10:
2024-09-10 12:18:08  reference to nearest natural gas and services in some instability industries and societies some conservative agencies are made available on a combination of such responsibilities that were formed as well as specializing nations and hardware for example the south american responsibilities that have been able to serve on a new large term for the case that are the south african responsibilities of gdp remains a third large number of the so called american responsibilities of new large numbers of ca
2024-09-10 12:18:08 Inference time: 1.98 seconds
2024-09-10 12:18:08 Tokens per second: 252.59
2024-09-10 12:18:08 ---------------
2024-09-10 12:18:08 Average tokens per second: 256.76
2024-09-10 12:51:15 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python plot.py
2024-09-10 12:51:16 
/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
2024-09-10 12:51:16   return _methods._mean(a, axis=axis, dtype=dtype,
2024-09-10 12:51:16 /usr/local/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
2024-09-10 12:51:16   ret = ret.dtype.type(ret / rcount)
2024-09-10 12:53:16 root@1d58f5ac47cd:/app/AI-Scientist/templates/nanoGPT# python plot.py
2024-09-10 12:53:16 
/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
2024-09-10 12:53:16   return _methods._mean(a, axis=axis, dtype=dtype,
2024-09-10 12:53:16 /usr/local/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
2024-09-10 12:53:16   ret = ret.dtype.type(ret / rcount)python launch_scientist.py --model "gpt-4o-2024-05-13" --experiment nanoGPT --
2024-09-10 12:54:49 cd ..
2024-09-10 12:54:50 
root@1d58f5ac47cd:/app/AI-Scientist/templates# cd ..
2024-09-10 12:54:52 
root@1d58f5ac47cd:/app/AI-Scientist# python launch_scientist.py --model "gpt-4o-2024-05-13" --experiment nanoGPT --num-ideas 2
python launch_scientist.py --model "gpt-4o-2024-05-13" --experiment nanoGPT --num-ideas 2
2024-09-10 12:54:55 
Using GPUs: [0]
2024-09-10 12:54:55 Using OpenAI API with model gpt-4o-2024-05-13.
2024-09-10 12:54:55 
2024-09-10 12:54:55 Generating idea 1/2
2024-09-10 12:54:55 Iteration 1/3
2024-09-10 12:55:00 {'Name': 'curriculum_learning', 'Title': 'Curriculum Learning: Progressive Exposure to Complex Data for Improved Language Model Training', 'Experiment': 'Implement a curriculum learning strategy by dividing the dataset into multiple subsets based on a complexity measure (e.g., sentence length, vocabulary usage). Modify the data loading and training loop to support phased training, where the model is first trained on simpler data and then on progressively complex subsets. Compare the training efficiency, validation loss, and final performance with the baseline model.', 'Interestingness': 7, 'Feasibility': 5, 'Novelty': 6}
2024-09-10 12:55:00 Iteration 2/3
2024-09-10 12:55:06 {'Name': 'curriculum_learning', 'Title': 'Curriculum Learning: Progressive Exposure to Complex Data for Improved Language Model Training', 'Experiment': 'Implement a curriculum learning strategy by dividing the dataset into multiple subsets based on a simple complexity measure (e.g., sentence length or word frequency). Modify the data loading and training loop to support phased training, where the model is first trained on simpler data and then on progressively complex subsets. Define explicit phases and criteria for transitioning between phases. Compare the training efficiency, validation loss, and generalization performance on a separate test set with the baseline model.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 6}
2024-09-10 12:55:06 Iteration 3/3
2024-09-10 12:55:10 {'Name': 'curriculum_learning', 'Title': 'Curriculum Learning: Progressive Exposure to Complex Data for Improved Language Model Training', 'Experiment': 'Implement a curriculum learning strategy by dividing the dataset into three subsets based on a simple complexity measure (e.g., sentence length or word frequency). Modify the data loading and training loop to support phased training, where the model is first trained on simpler data, then on intermediate, and finally on complex subsets. Define explicit phases and criteria for transitioning between phases, such as a fixed number of iterations or performance threshold. Compare the training efficiency, validation loss, and generalization performance on a separate test set with the baseline model.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 6}
2024-09-10 12:55:10 Idea generation converged after 3 iterations.
2024-09-10 12:55:10 
2024-09-10 12:55:10 Generating idea 2/2
2024-09-10 12:55:10 Iteration 1/3
2024-09-10 12:55:15 {'Name': 'complexity_aware_attention', 'Title': 'Complexity-Aware Attention Scaling: Enhancing Transformer Models with Dynamic Attention Mechanisms', 'Experiment': 'Modify the CausalSelfAttention class to include a complexity-aware scaling factor for the attention weights. This scaling factor will be computed based on a complexity measure of the input tokens, which could be derived using token frequency or token entropy. Implement this by adding a new function to calculate token complexity and integrate it into the forward method of the CausalSelfAttention class. Compare the training dynamics, convergence speed, and final performance with the baseline model.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}
2024-09-10 12:55:15 Iteration 2/3
2024-09-10 12:55:20 {'Name': 'complexity_aware_attention', 'Title': 'Complexity-Aware Attention Scaling: Enhancing Transformer Models with Dynamic Attention Mechanisms', 'Experiment': 'Modify the CausalSelfAttention class to include a complexity-aware scaling factor for the attention weights. This scaling factor will be computed based on a running average of token importance derived from the attention weights during training. Implement this by adding a new function to calculate token complexity and integrate it into the forward method of the CausalSelfAttention class. Compare the training dynamics, convergence speed, and final performance with the baseline model.', 'Interestingness': 8, 'Feasibility': 6, 'Novelty': 7}
2024-09-10 12:55:20 Idea generation converged after 2 iterations.
2024-09-10 12:55:20 
2024-09-10 12:55:20 Checking novelty of idea 0: adaptive_block_size
2024-09-10 12:55:23 Response Status Code: 200
2024-09-10 12:55:23 Response Content: {"total": 476, "offset": 0, "next": 10, "data": [{"paperId": "d4b99821ab8c1ee3271a72dc4163feb8d310c8a0", "title": "DBPS: Dynamic Block Size and Precision Scaling for Efficient DNN Training Supported by RISC-V ISA Extensions", "abstract": "Over the past decade, it has been found that deep neural networks (DNNs) perform better on visual perception and language understanding tasks as their size increases. However, this comes at the cost of high energy consumption and large memory requirement to tra
2024-09-10 12:55:27 Response Status Code: 200
2024-09-10 12:55:27 Response Content: {"total": 30, "offset": 0, "next": 10, "data": [{"paperId": "ba609e5c83ad9b32723e547b9d3c89d97373f755", "title": "VSA: Learning Varied-Size Window Attention in Vision Transformers", "abstract": "Attention within windows has been widely explored in vision transformers to balance the performance, computation complexity, and memory footprint. However, current models adopt a hand-crafted fixed-size window design, which restricts their capacity of modeling long-term dependencies and adapting to objec
2024-09-10 12:55:34 Decision made: novel after round 2
2024-09-10 12:55:34 
2024-09-10 12:55:34 Checking novelty of idea 1: layerwise_learning_rates
2024-09-10 12:55:36 Response Status Code: 200
2024-09-10 12:55:36 Response Content: {"total": 16626, "offset": 0, "next": 10, "data": [{"paperId": "e09bfc955fbf66e0a042ca3f921108b823093b2e", "title": "Layer-wise Learning Rate Optimization for Task-Dependent Fine-Tuning of Pre-trained Models: An Evolutionary Approach", "abstract": "The superior performance of large-scale pre-trained models, such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), has received increasing attention in both academic and industrial research
2024-09-10 12:55:39 Decision made: not novel after round 1
2024-09-10 12:55:39 
2024-09-10 12:55:39 Checking novelty of idea 2: curriculum_learning
2024-09-10 12:55:42 Response Status Code: 200
2024-09-10 12:55:42 Response Content: {"total": 42014, "offset": 0, "next": 10, "data": [{"paperId": "308db6f428a20fbef0a72f9ee73798d60fe9ff50", "title": "BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition", "abstract": "Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NE
2024-09-10 12:55:46 Response Status Code: 200
2024-09-10 12:55:46 Response Content: {"total": 87, "offset": 0, "next": 10, "data": [{"paperId": "02fa3db10284e71675d384199683a8e3e2ef7584", "title": "Sentence Level Curriculum Learning for Improved Neural Conversational Models", "abstract": "Designing machine intelligence to converse with a human user necessarily requires an understanding of how humans participate in conversation, and thus conversation modeling is an important task in natural language processing. New breakthroughs in architecture and data gathering continue to pus
2024-09-10 12:55:48 Decision made: not novel after round 2
2024-09-10 12:55:48 
2024-09-10 12:55:48 Checking novelty of idea 3: complexity_aware_attention
2024-09-10 12:55:51 Response Status Code: 200
2024-09-10 12:55:51 Response Content: {"total": 1480, "offset": 0, "next": 10, "data": [{"paperId": "be7900706c51f70f8be62caaa0288bf9894d608a", "title": "SparseCoder: Identifier-Aware Sparse Transformer for File- Level Code Summarization", "abstract": "Code summarization aims to generate natural language descriptions of source code, facilitating programmers to understand and maintain it rapidly. While previous code summarization efforts have predominantly focused on method-level, this paper studies file-level code summarization, whi
2024-09-10 12:55:55 Response Status Code: 200
2024-09-10 12:55:55 Response Content: {"total": 18286, "offset": 0, "next": 10, "data": [{"paperId": "5b94dd68c7484d828bb3ed4b065fc58ddbd3b202", "title": "Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention", "abstract": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. Whil
2024-09-10 12:55:59 Response Status Code: 200
2024-09-10 12:55:59 Response Content: {"total": 17891, "offset": 0, "next": 10, "data": [{"paperId": "97202823f1619da9f4d9fee5b356e34dffb1413e", "title": "SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention", "abstract": "We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It conve
2024-09-10 12:56:01 Decision made: novel after round 3
2024-09-10 12:56:01 Processing idea: adaptive_block_size
2024-09-10 12:56:01 2024-09-10 03:56:01
2024-09-10 12:56:01 *Starting idea: 20240910_035601_adaptive_block_size*
2024-09-10 12:56:01 2024-09-10 03:56:01
2024-09-10 12:56:01 *Starting Experiments*
2024-09-10 12:56:15                                                         Plan for Experiments                                                        
2024-09-10 12:56:15 
2024-09-10 12:56:15 To implement the idea of adaptive block size, we will modify the training loop to dynamically adjust the block size during training.
2024-09-10 12:56:15 We will start with a smaller block size and gradually increase it. Here is the plan for the experiments:                            
2024-09-10 12:56:15 
2024-09-10 12:56:15  1 Run 1: Start with a block size of 64 and increase it to 256 over the course of training.                                         
2024-09-10 12:56:15  2 Run 2: Start with a block size of 128 and increase it to 256 over the course of training.                                        
2024-09-10 12:56:15  3 Run 3: Start with a block size of 64 and increase it to 512 over the course of training.                                         
2024-09-10 12:56:15  4 Run 4: Start with a block size of 128 and increase it to 512 over the course of training.                                        
2024-09-10 12:56:15  5 Run 5: Start with a block size of 256 and increase it to 512 over the course of training.                                        
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                         Implementation Steps                                                        
2024-09-10 12:56:15 
2024-09-10 12:56:15  1 Modify the train function in experiment.py to include logic for dynamic block size adjustment.                                   
2024-09-10 12:56:15  2 Add a parameter to control the initial and final block sizes.                                                                    
2024-09-10 12:56:15  3 Adjust the data loading and model configuration to handle the changing block size.                                               
2024-09-10 12:56:15 
2024-09-10 12:56:15 Let's start with Run 1.                                                                                                             
2024-09-10 12:56:15 
2024-09-10 12:56:15                                     Run 1: Start with a block size of 64 and increase it to 256                                     
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                       Changes to experiment.py                                                      
2024-09-10 12:56:15 
2024-09-10 12:56:15  1 Add parameters for initial and final block sizes.                                                                                
2024-09-10 12:56:15  2 Modify the training loop to adjust the block size dynamically.                                                                   
2024-09-10 12:56:15 
2024-09-10 12:56:15 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py                                                                                                                       
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:15  def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):                                                             
2024-09-10 12:56:15  =======                                                                                                                            
2024-09-10 12:56:15  def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0, initial_block_size=64, final_block_size=256):                
2024-09-10 12:56:15  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py                                                                                                                       
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:15      block_size = 256  # context of up to 256 previous characters                                                                   
2024-09-10 12:56:15  =======                                                                                                                            
2024-09-10 12:56:15      block_size = initial_block_size  # start with the initial block size                                                           
2024-09-10 12:56:15  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py                                                                                                                       
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:15      while True:                                                                                                                    
2024-09-10 12:56:15  =======                                                                                                                            
2024-09-10 12:56:15      while True:                                                                                                                    
2024-09-10 12:56:15          # Adjust block size dynamically                                                                                            
2024-09-10 12:56:15          if iter_num < max_iters:                                                                                                   
2024-09-10 12:56:15              block_size = initial_block_size + (final_block_size - initial_block_size) * iter_num // max_iters                      
2024-09-10 12:56:15              model.crop_block_size(block_size)                                                                                      
2024-09-10 12:56:15  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15 
2024-09-10 12:56:15 Now, you can run the first experiment with the command:                                                                             
2024-09-10 12:56:15 
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15  python experiment.py --out_dir=run_1                                                                                               
2024-09-10 12:56:15                                                                                                                                     
2024-09-10 12:56:15 
2024-09-10 12:56:15 Please run the command and let me know the results. Once we have the results, we can proceed with the next experiment.              
2024-09-10 12:56:15 
2024-09-10 12:56:15 Tokens: 10,177 sent, 559 received. Cost: $0.06 message, $0.06 session.
2024-09-10 12:56:15 Applied edit to experiment.py
2024-09-10 12:56:15 ### Plan for Experiments
2024-09-10 12:56:15 
2024-09-10 12:56:15 To implement the idea of adaptive block size, we will modify the training loop to dynamically adjust the block size during training. We will start with a smaller block size and gradually increase it. Here is the plan for the experiments:
2024-09-10 12:56:15 
2024-09-10 12:56:15 1. **Run 1**: Start with a block size of 64 and increase it to 256 over the course of training.
2024-09-10 12:56:15 2. **Run 2**: Start with a block size of 128 and increase it to 256 over the course of training.
2024-09-10 12:56:15 3. **Run 3**: Start with a block size of 64 and increase it to 512 over the course of training.
2024-09-10 12:56:15 4. **Run 4**: Start with a block size of 128 and increase it to 512 over the course of training.
2024-09-10 12:56:15 5. **Run 5**: Start with a block size of 256 and increase it to 512 over the course of training.
2024-09-10 12:56:15 
2024-09-10 12:56:15 ### Implementation Steps
2024-09-10 12:56:15 
2024-09-10 12:56:15 1. Modify the `train` function in `experiment.py` to include logic for dynamic block size adjustment.
2024-09-10 12:56:15 2. Add a parameter to control the initial and final block sizes.
2024-09-10 12:56:15 3. Adjust the data loading and model configuration to handle the changing block size.
2024-09-10 12:56:15 
2024-09-10 12:56:15 Let's start with **Run 1**.
2024-09-10 12:56:15 
2024-09-10 12:56:15 ### Run 1: Start with a block size of 64 and increase it to 256
2024-09-10 12:56:15 
2024-09-10 12:56:15 #### Changes to `experiment.py`
2024-09-10 12:56:15 
2024-09-10 12:56:15 1. Add parameters for initial and final block sizes.
2024-09-10 12:56:15 2. Modify the training loop to adjust the block size dynamically.
2024-09-10 12:56:15 
2024-09-10 12:56:15 Here are the *SEARCH/REPLACE* blocks:
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py
2024-09-10 12:56:15 ```python
2024-09-10 12:56:15 <<<<<<< SEARCH
2024-09-10 12:56:15 def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
2024-09-10 12:56:15 =======
2024-09-10 12:56:15 def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0, initial_block_size=64, final_block_size=256):
2024-09-10 12:56:15 >>>>>>> REPLACE
2024-09-10 12:56:15 ```
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py
2024-09-10 12:56:15 ```python
2024-09-10 12:56:15 <<<<<<< SEARCH
2024-09-10 12:56:15     block_size = 256  # context of up to 256 previous characters
2024-09-10 12:56:15 =======
2024-09-10 12:56:15     block_size = initial_block_size  # start with the initial block size
2024-09-10 12:56:15 >>>>>>> REPLACE
2024-09-10 12:56:15 ```
2024-09-10 12:56:15 
2024-09-10 12:56:15 experiment.py
2024-09-10 12:56:15 ```python
2024-09-10 12:56:15 <<<<<<< SEARCH
2024-09-10 12:56:15     while True:
2024-09-10 12:56:15 =======
2024-09-10 12:56:15     while True:
2024-09-10 12:56:15         # Adjust block size dynamically
2024-09-10 12:56:15         if iter_num < max_iters:
2024-09-10 12:56:15             block_size = initial_block_size + (final_block_size - initial_block_size) * iter_num // max_iters
2024-09-10 12:56:15             model.crop_block_size(block_size)
2024-09-10 12:56:15 >>>>>>> REPLACE
2024-09-10 12:56:15 ```
2024-09-10 12:56:15 
2024-09-10 12:56:15 Now, you can run the first experiment with the command:
2024-09-10 12:56:15 ```sh
2024-09-10 12:56:15 python experiment.py --out_dir=run_1
2024-09-10 12:56:15 ```
2024-09-10 12:56:15 
2024-09-10 12:56:15 Please run the command and let me know the results. Once we have the results, we can proceed with the next experiment.
2024-09-10 12:56:16 tokens per iteration will be: 4,096
2024-09-10 12:56:16 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:56:16 Initializing a new model from scratch
2024-09-10 12:56:16 number of parameters: 10.65M
2024-09-10 12:56:17 num decayed parameter tensors: 26, with 10,666,368 parameters
2024-09-10 12:56:17 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:56:18 using fused AdamW: True
2024-09-10 12:56:18 compiling the model... (takes a ~minute)
2024-09-10 12:56:32 step 0: train loss 4.3225, val loss 4.3175
2024-09-10 12:56:46 iter 0: loss 4.3020, time 27748.87ms
2024-09-10 12:56:47 iter 10: loss 3.3099, time 22.90ms
2024-09-10 12:56:47 iter 20: loss 2.8787, time 24.85ms
2024-09-10 12:56:48 Traceback (most recent call last):
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:56:48     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:56:48                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 520, in train
2024-09-10 12:56:48     model.crop_block_size(block_size)
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 242, in crop_block_size
2024-09-10 12:56:48     assert block_size <= self.config.block_size
2024-09-10 12:56:48            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:56:48 AssertionError
2024-09-10 12:56:48 
2024-09-10 12:56:48 Run 1 failed with return code 1
2024-09-10 12:56:48 Run failed with the following error Traceback (most recent call last):
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:56:48     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:56:48                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 520, in train
2024-09-10 12:56:48     model.crop_block_size(block_size)
2024-09-10 12:56:48   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 242, in crop_block_size
2024-09-10 12:56:48     assert block_size <= self.config.block_size
2024-09-10 12:56:48            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:56:48 AssertionError
2024-09-10 12:56:48 
2024-09-10 12:56:52 The error indicates that the block_size being set dynamically during training is exceeding the initial block_size set in the model  
2024-09-10 12:56:52 configuration. To fix this, we need to ensure that the initial block_size in the model configuration is set to the maximum block    
2024-09-10 12:56:52 size we intend to use during training.                                                                                              
2024-09-10 12:56:52 
2024-09-10 12:56:52 Let's update the train function to set the initial block_size in the model configuration to the final_block_size.                   
2024-09-10 12:56:52 
2024-09-10 12:56:52 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:56:52 
2024-09-10 12:56:52 experiment.py                                                                                                                       
2024-09-10 12:56:52 
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:52      block_size = initial_block_size  # start with the initial block size                                                           
2024-09-10 12:56:52  =======                                                                                                                            
2024-09-10 12:56:52      block_size = final_block_size  # set the initial block size to the final block size                                            
2024-09-10 12:56:52  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52 
2024-09-10 12:56:52 experiment.py                                                                                                                       
2024-09-10 12:56:52 
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:52      model_args["block_size"] = block_size  # so that the checkpoint will have the right value                                      
2024-09-10 12:56:52  =======                                                                                                                            
2024-09-10 12:56:52      model_args["block_size"] = final_block_size  # so that the checkpoint will have the right value                                
2024-09-10 12:56:52  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52 
2024-09-10 12:56:52 Please apply these changes and run the experiment again with the command:                                                           
2024-09-10 12:56:52 
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52  python experiment.py --out_dir=run_1                                                                                               
2024-09-10 12:56:52                                                                                                                                     
2024-09-10 12:56:52 
2024-09-10 12:56:52 Let me know the results once the run is complete.                                                                                   
2024-09-10 12:56:52 
2024-09-10 12:56:52 Tokens: 10,997 sent, 251 received. Cost: $0.06 message, $0.12 session.
2024-09-10 12:56:52 The LLM did not conform to the edit format.
2024-09-10 12:56:52 https://aider.chat/docs/troubleshooting/edit-errors.html
2024-09-10 12:56:52 
2024-09-10 12:56:52 # 1 SEARCH/REPLACE block failed to match!
2024-09-10 12:56:52 
2024-09-10 12:56:52 ## SearchReplaceNoExactMatch: This SEARCH block failed to exactly match lines in experiment.py
2024-09-10 12:56:52 <<<<<<< SEARCH
2024-09-10 12:56:52     model_args["block_size"] = block_size  # so that the checkpoint will have the right value
2024-09-10 12:56:52 =======
2024-09-10 12:56:52     model_args["block_size"] = final_block_size  # so that the checkpoint will have the right value
2024-09-10 12:56:52 >>>>>>> REPLACE
2024-09-10 12:56:52 
2024-09-10 12:56:52 The SEARCH section must exactly match an existing block of lines including all white space, comments, indentation, docstrings, etc
2024-09-10 12:56:52 
2024-09-10 12:56:52 # The other 1 SEARCH/REPLACE block were applied successfully.
2024-09-10 12:56:52 Don't re-send them.
2024-09-10 12:56:52 Just reply with fixed versions of the block above that failed to match.
2024-09-10 12:56:52 
2024-09-10 12:56:55 Let's correct the failed SEARCH/REPLACE block by ensuring the search section matches the existing code exactly.                     
2024-09-10 12:56:55 
2024-09-10 12:56:55 experiment.py                                                                                                                       
2024-09-10 12:56:55 
2024-09-10 12:56:55                                                                                                                                     
2024-09-10 12:56:55  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:56:55      if block_size < model.config.block_size:                                                                                       
2024-09-10 12:56:55          model.crop_block_size(block_size)                                                                                          
2024-09-10 12:56:55          model_args["block_size"] = (                                                                                               
2024-09-10 12:56:55              block_size  # so that the checkpoint will have the right value                                                         
2024-09-10 12:56:55          )                                                                                                                          
2024-09-10 12:56:55  =======                                                                                                                            
2024-09-10 12:56:55      if final_block_size < model.config.block_size:                                                                                 
2024-09-10 12:56:55          model.crop_block_size(final_block_size)                                                                                    
2024-09-10 12:56:55          model_args["block_size"] = (                                                                                               
2024-09-10 12:56:55              final_block_size  # so that the checkpoint will have the right value                                                   
2024-09-10 12:56:55          )                                                                                                                          
2024-09-10 12:56:55  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:56:55                                                                                                                                     
2024-09-10 12:56:55 
2024-09-10 12:56:55 Tokens: 11,402 sent, 128 received. Cost: $0.06 message, $0.18 session.
2024-09-10 12:56:55 Applied edit to experiment.py
2024-09-10 12:56:55 Let's correct the failed *SEARCH/REPLACE* block by ensuring the search section matches the existing code exactly.
2024-09-10 12:56:55 
2024-09-10 12:56:55 experiment.py
2024-09-10 12:56:55 ```python
2024-09-10 12:56:55 <<<<<<< SEARCH
2024-09-10 12:56:55     if block_size < model.config.block_size:
2024-09-10 12:56:55         model.crop_block_size(block_size)
2024-09-10 12:56:55         model_args["block_size"] = (
2024-09-10 12:56:55             block_size  # so that the checkpoint will have the right value
2024-09-10 12:56:55         )
2024-09-10 12:56:55 =======
2024-09-10 12:56:55     if final_block_size < model.config.block_size:
2024-09-10 12:56:55         model.crop_block_size(final_block_size)
2024-09-10 12:56:55         model_args["block_size"] = (
2024-09-10 12:56:55             final_block_size  # so that the checkpoint will have the right value
2024-09-10 12:56:55         )
2024-09-10 12:56:55 >>>>>>> REPLACE
2024-09-10 12:56:55 ```
2024-09-10 12:56:56 tokens per iteration will be: 16,384
2024-09-10 12:56:56 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:56:56 Initializing a new model from scratch
2024-09-10 12:56:56 number of parameters: 10.65M
2024-09-10 12:56:56 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:56:56 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:56:57 using fused AdamW: True
2024-09-10 12:56:57 compiling the model... (takes a ~minute)
2024-09-10 12:57:07 step 0: train loss 4.2644, val loss 4.2561
2024-09-10 12:57:08 Traceback (most recent call last):
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:08     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:08                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:08     logits, loss = model(X, Y)
2024-09-10 12:57:08                    ^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:08     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:08     return forward_call(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:08     return fn(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:08     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:08     return forward_call(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:08     t <= self.config.block_size
2024-09-10 12:57:08 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:08 
2024-09-10 12:57:08 Run 1 failed with return code 1
2024-09-10 12:57:08 Run failed with the following error Traceback (most recent call last):
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:08     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:08                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:08     logits, loss = model(X, Y)
2024-09-10 12:57:08                    ^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:08     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:08     return forward_call(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:08     return fn(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:08     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:08     return forward_call(*args, **kwargs)
2024-09-10 12:57:08            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:08   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:08     t <= self.config.block_size
2024-09-10 12:57:08 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:08 
2024-09-10 12:57:14 The error indicates that the sequence length t is exceeding the current block_size during the forward pass. This is because the     
2024-09-10 12:57:14 block_size is being dynamically adjusted, but the data loading function get_batch is still using the initial block_size.            
2024-09-10 12:57:14 
2024-09-10 12:57:14 To fix this, we need to adjust the get_batch function to use the current block_size dynamically.                                    
2024-09-10 12:57:14 
2024-09-10 12:57:14 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py                                                                                                                       
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:14      def get_batch(split):                                                                                                          
2024-09-10 12:57:14  =======                                                                                                                            
2024-09-10 12:57:14      def get_batch(split, block_size):                                                                                              
2024-09-10 12:57:14  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py                                                                                                                       
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:14          ix = torch.randint(len(data) - block_size, (batch_size,))                                                                  
2024-09-10 12:57:14  =======                                                                                                                            
2024-09-10 12:57:14          ix = torch.randint(len(data) - block_size, (batch_size,))                                                                  
2024-09-10 12:57:14  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py                                                                                                                       
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:14      X, Y = get_batch("train")  # fetch the very first batch                                                                        
2024-09-10 12:57:14  =======                                                                                                                            
2024-09-10 12:57:14      X, Y = get_batch("train", block_size)  # fetch the very first batch                                                            
2024-09-10 12:57:14  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py                                                                                                                       
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:14                  X, Y = get_batch(split)                                                                                            
2024-09-10 12:57:14  =======                                                                                                                            
2024-09-10 12:57:14                  X, Y = get_batch(split, block_size)                                                                                
2024-09-10 12:57:14  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py                                                                                                                       
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:14              X, Y = get_batch("train")                                                                                              
2024-09-10 12:57:14  =======                                                                                                                            
2024-09-10 12:57:14              X, Y = get_batch("train", block_size)                                                                                  
2024-09-10 12:57:14  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 Please apply these changes and run the experiment again with the command:                                                           
2024-09-10 12:57:14 
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14  python experiment.py --out_dir=run_1                                                                                               
2024-09-10 12:57:14                                                                                                                                     
2024-09-10 12:57:14 
2024-09-10 12:57:14 Let me know the results once the run is complete.                                                                                   
2024-09-10 12:57:14 
2024-09-10 12:57:14 Tokens: 11,960 sent, 339 received. Cost: $0.06 message, $0.24 session.
2024-09-10 12:57:14 Applied edit to experiment.py
2024-09-10 12:57:14 The error indicates that the sequence length `t` is exceeding the current `block_size` during the forward pass. This is because the `block_size` is being dynamically adjusted, but the data loading function `get_batch` is still using the initial `block_size`.
2024-09-10 12:57:14 
2024-09-10 12:57:14 To fix this, we need to adjust the `get_batch` function to use the current `block_size` dynamically.
2024-09-10 12:57:14 
2024-09-10 12:57:14 Here are the *SEARCH/REPLACE* blocks:
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py
2024-09-10 12:57:14 ```python
2024-09-10 12:57:14 <<<<<<< SEARCH
2024-09-10 12:57:14     def get_batch(split):
2024-09-10 12:57:14 =======
2024-09-10 12:57:14     def get_batch(split, block_size):
2024-09-10 12:57:14 >>>>>>> REPLACE
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py
2024-09-10 12:57:14 ```python
2024-09-10 12:57:14 <<<<<<< SEARCH
2024-09-10 12:57:14         ix = torch.randint(len(data) - block_size, (batch_size,))
2024-09-10 12:57:14 =======
2024-09-10 12:57:14         ix = torch.randint(len(data) - block_size, (batch_size,))
2024-09-10 12:57:14 >>>>>>> REPLACE
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py
2024-09-10 12:57:14 ```python
2024-09-10 12:57:14 <<<<<<< SEARCH
2024-09-10 12:57:14     X, Y = get_batch("train")  # fetch the very first batch
2024-09-10 12:57:14 =======
2024-09-10 12:57:14     X, Y = get_batch("train", block_size)  # fetch the very first batch
2024-09-10 12:57:14 >>>>>>> REPLACE
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py
2024-09-10 12:57:14 ```python
2024-09-10 12:57:14 <<<<<<< SEARCH
2024-09-10 12:57:14                 X, Y = get_batch(split)
2024-09-10 12:57:14 =======
2024-09-10 12:57:14                 X, Y = get_batch(split, block_size)
2024-09-10 12:57:14 >>>>>>> REPLACE
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 experiment.py
2024-09-10 12:57:14 ```python
2024-09-10 12:57:14 <<<<<<< SEARCH
2024-09-10 12:57:14             X, Y = get_batch("train")
2024-09-10 12:57:14 =======
2024-09-10 12:57:14             X, Y = get_batch("train", block_size)
2024-09-10 12:57:14 >>>>>>> REPLACE
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 Please apply these changes and run the experiment again with the command:
2024-09-10 12:57:14 ```sh
2024-09-10 12:57:14 python experiment.py --out_dir=run_1
2024-09-10 12:57:14 ```
2024-09-10 12:57:14 
2024-09-10 12:57:14 Let me know the results once the run is complete.
2024-09-10 12:57:15 tokens per iteration will be: 16,384
2024-09-10 12:57:15 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:57:15 Initializing a new model from scratch
2024-09-10 12:57:15 number of parameters: 10.65M
2024-09-10 12:57:16 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:57:16 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:57:16 using fused AdamW: True
2024-09-10 12:57:16 compiling the model... (takes a ~minute)
2024-09-10 12:57:26 step 0: train loss 4.2644, val loss 4.2561
2024-09-10 12:57:28 Traceback (most recent call last):
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:28     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:28                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:28     logits, loss = model(X, Y)
2024-09-10 12:57:28                    ^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:28     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:28     return forward_call(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:28     return fn(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:28     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:28     return forward_call(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:28     t <= self.config.block_size
2024-09-10 12:57:28 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:28 
2024-09-10 12:57:28 Run 1 failed with return code 1
2024-09-10 12:57:28 Run failed with the following error Traceback (most recent call last):
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:28     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:28                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:28     logits, loss = model(X, Y)
2024-09-10 12:57:28                    ^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:28     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:28     return forward_call(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:28     return fn(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:28     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:28     return forward_call(*args, **kwargs)
2024-09-10 12:57:28            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:28   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:28     t <= self.config.block_size
2024-09-10 12:57:28 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:28 
2024-09-10 12:57:33 The error indicates that the sequence length t is exceeding the current block_size during the forward pass. This is because the     
2024-09-10 12:57:33 block_size is being dynamically adjusted, but the data loading function get_batch is still using the initial block_size.            
2024-09-10 12:57:33 
2024-09-10 12:57:33 To fix this, we need to adjust the get_batch function to use the current block_size dynamically.                                    
2024-09-10 12:57:33 
2024-09-10 12:57:33 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py                                                                                                                       
2024-09-10 12:57:33 
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:33      def get_batch(split, block_size):                                                                                              
2024-09-10 12:57:33  =======                                                                                                                            
2024-09-10 12:57:33      def get_batch(split, block_size):                                                                                              
2024-09-10 12:57:33  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py                                                                                                                       
2024-09-10 12:57:33 
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:33      X, Y = get_batch("train", block_size)  # fetch the very first batch                                                            
2024-09-10 12:57:33  =======                                                                                                                            
2024-09-10 12:57:33      X, Y = get_batch("train", block_size)  # fetch the very first batch                                                            
2024-09-10 12:57:33  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py                                                                                                                       
2024-09-10 12:57:33 
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:33                  X, Y = get_batch(split, block_size)                                                                                
2024-09-10 12:57:33  =======                                                                                                                            
2024-09-10 12:57:33                  X, Y = get_batch(split, block_size)                                                                                
2024-09-10 12:57:33  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py                                                                                                                       
2024-09-10 12:57:33 
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:33              X, Y = get_batch("train", block_size)                                                                                  
2024-09-10 12:57:33  =======                                                                                                                            
2024-09-10 12:57:33              X, Y = get_batch("train", block_size)                                                                                  
2024-09-10 12:57:33  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33 
2024-09-10 12:57:33 Please apply these changes and run the experiment again with the command:                                                           
2024-09-10 12:57:33 
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33  python experiment.py --out_dir=run_1                                                                                               
2024-09-10 12:57:33                                                                                                                                     
2024-09-10 12:57:33 
2024-09-10 12:57:33 Let me know the results once the run is complete.                                                                                   
2024-09-10 12:57:33 
2024-09-10 12:57:33 Tokens: 11,277 sent, 303 received. Cost: $0.06 message, $0.30 session.
2024-09-10 12:57:33 Applied edit to experiment.py
2024-09-10 12:57:33 The error indicates that the sequence length `t` is exceeding the current `block_size` during the forward pass. This is because the `block_size` is being dynamically adjusted, but the data loading function `get_batch` is still using the initial `block_size`.
2024-09-10 12:57:33 
2024-09-10 12:57:33 To fix this, we need to adjust the `get_batch` function to use the current `block_size` dynamically.
2024-09-10 12:57:33 
2024-09-10 12:57:33 Here are the *SEARCH/REPLACE* blocks:
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py
2024-09-10 12:57:33 ```python
2024-09-10 12:57:33 <<<<<<< SEARCH
2024-09-10 12:57:33     def get_batch(split, block_size):
2024-09-10 12:57:33 =======
2024-09-10 12:57:33     def get_batch(split, block_size):
2024-09-10 12:57:33 >>>>>>> REPLACE
2024-09-10 12:57:33 ```
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py
2024-09-10 12:57:33 ```python
2024-09-10 12:57:33 <<<<<<< SEARCH
2024-09-10 12:57:33     X, Y = get_batch("train", block_size)  # fetch the very first batch
2024-09-10 12:57:33 =======
2024-09-10 12:57:33     X, Y = get_batch("train", block_size)  # fetch the very first batch
2024-09-10 12:57:33 >>>>>>> REPLACE
2024-09-10 12:57:33 ```
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py
2024-09-10 12:57:33 ```python
2024-09-10 12:57:33 <<<<<<< SEARCH
2024-09-10 12:57:33                 X, Y = get_batch(split, block_size)
2024-09-10 12:57:33 =======
2024-09-10 12:57:33                 X, Y = get_batch(split, block_size)
2024-09-10 12:57:33 >>>>>>> REPLACE
2024-09-10 12:57:33 ```
2024-09-10 12:57:33 
2024-09-10 12:57:33 experiment.py
2024-09-10 12:57:33 ```python
2024-09-10 12:57:33 <<<<<<< SEARCH
2024-09-10 12:57:33             X, Y = get_batch("train", block_size)
2024-09-10 12:57:33 =======
2024-09-10 12:57:33             X, Y = get_batch("train", block_size)
2024-09-10 12:57:33 >>>>>>> REPLACE
2024-09-10 12:57:33 ```
2024-09-10 12:57:33 
2024-09-10 12:57:33 Please apply these changes and run the experiment again with the command:
2024-09-10 12:57:33 ```sh
2024-09-10 12:57:33 python experiment.py --out_dir=run_1
2024-09-10 12:57:33 ```
2024-09-10 12:57:33 
2024-09-10 12:57:33 Let me know the results once the run is complete.
2024-09-10 12:57:34 tokens per iteration will be: 16,384
2024-09-10 12:57:34 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:57:34 Initializing a new model from scratch
2024-09-10 12:57:35 number of parameters: 10.65M
2024-09-10 12:57:35 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:57:35 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:57:35 using fused AdamW: True
2024-09-10 12:57:35 compiling the model... (takes a ~minute)
2024-09-10 12:57:46 step 0: train loss 4.2644, val loss 4.2561
2024-09-10 12:57:47 Traceback (most recent call last):
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:47     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:47                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:47     logits, loss = model(X, Y)
2024-09-10 12:57:47                    ^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:47     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:47     return forward_call(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:47     return fn(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:47     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:47     return forward_call(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:47     t <= self.config.block_size
2024-09-10 12:57:47 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:47 
2024-09-10 12:57:47 Run 1 failed with return code 1
2024-09-10 12:57:47 Run failed with the following error Traceback (most recent call last):
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 701, in <module>
2024-09-10 12:57:47     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:57:47                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 560, in train
2024-09-10 12:57:47     logits, loss = model(X, Y)
2024-09-10 12:57:47                    ^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:47     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:47     return forward_call(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:57:47     return fn(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:57:47     return self._call_impl(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:57:47     return forward_call(*args, **kwargs)
2024-09-10 12:57:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:57:47   File "/app/AI-Scientist/results/nanoGPT/20240910_035601_adaptive_block_size/experiment.py", line 211, in forward
2024-09-10 12:57:47     t <= self.config.block_size
2024-09-10 12:57:47 AssertionError: Cannot forward sequence of length 256, block size is only 64
2024-09-10 12:57:47 
2024-09-10 12:57:47 Max iterations reached
2024-09-10 12:57:47 Not all experiments completed.
2024-09-10 12:57:47 Experiments failed for idea 20240910_035601_adaptive_block_size
2024-09-10 12:57:47 FINISHED IDEA
2024-09-10 12:57:47 Completed idea: adaptive_block_size, Success: False
2024-09-10 12:57:47 Processing idea: complexity_aware_attention
2024-09-10 12:57:47 2024-09-10 03:57:47
2024-09-10 12:57:47 *Starting idea: 20240910_035747_complexity_aware_attention*
2024-09-10 12:57:47 2024-09-10 03:57:47
2024-09-10 12:57:47 *Starting Experiments*
2024-09-10 12:57:58                                                         Plan for Experiments                                                        
2024-09-10 12:57:58 
2024-09-10 12:57:58 To implement the Complexity-Aware Attention Scaling, we will modify the CausalSelfAttention class to include a complexity-aware     
2024-09-10 12:57:58 scaling factor for the attention weights. This scaling factor will be computed based on a running average of token importance       
2024-09-10 12:57:58 derived from the attention weights during training.                                                                                 
2024-09-10 12:57:58 
2024-09-10 12:57:58 We will conduct the following experiments:                                                                                          
2024-09-10 12:57:58 
2024-09-10 12:57:58  1 Run 1: Implement the complexity-aware scaling factor with a basic running average mechanism.                                     
2024-09-10 12:57:58  2 Run 2: Experiment with a different decay rate for the running average.                                                           
2024-09-10 12:57:58  3 Run 3: Experiment with a different initialization for the complexity scores.                                                     
2024-09-10 12:57:58  4 Run 4: Combine the best settings from Runs 2 and 3.                                                                              
2024-09-10 12:57:58  5 Run 5: Fine-tune the best model from Run 4 with a different learning rate.                                                       
2024-09-10 12:57:58 
2024-09-10 12:57:58                                                         Implementation Steps                                                        
2024-09-10 12:57:58 
2024-09-10 12:57:58  1 Add a function to calculate token complexity.                                                                                    
2024-09-10 12:57:58  2 Modify the forward method of the CausalSelfAttention class to include the complexity-aware scaling factor.                       
2024-09-10 12:57:58 
2024-09-10 12:57:58 Let's start with Run 1.                                                                                                             
2024-09-10 12:57:58 
2024-09-10 12:57:58                                                     Run 1: Basic Implementation                                                     
2024-09-10 12:57:58 
2024-09-10 12:57:58                                         Step 1: Add a function to calculate token complexity                                        
2024-09-10 12:57:58 
2024-09-10 12:57:58 We'll add a new method calculate_token_complexity to the CausalSelfAttention class.                                                 
2024-09-10 12:57:58 
2024-09-10 12:57:58                           Step 2: Modify the forward method to include the complexity-aware scaling factor                          
2024-09-10 12:57:58 
2024-09-10 12:57:58 We'll modify the forward method to use the complexity-aware scaling factor.                                                         
2024-09-10 12:57:58 
2024-09-10 12:57:58 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:57:58 
2024-09-10 12:57:58 experiment.py                                                                                                                       
2024-09-10 12:57:58 
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:57:58      def forward(self, x):                                                                                                          
2024-09-10 12:57:58  =======                                                                                                                            
2024-09-10 12:57:58      def calculate_token_complexity(self, attn_weights):                                                                            
2024-09-10 12:57:58          # Calculate token complexity based on attention weights                                                                    
2024-09-10 12:57:58          token_importance = attn_weights.mean(dim=-2)  # Average over the sequence length                                           
2024-09-10 12:57:58          return token_importance                                                                                                    
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58      def forward(self, x):                                                                                                          
2024-09-10 12:57:58          B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)                                       
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58          # calculate query, key, values for all heads in batch and move head forward to be the batch dim                            
2024-09-10 12:57:58          q, k, v = self.c_attn(x).split(self.n_embd, dim=2)                                                                         
2024-09-10 12:57:58          k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)                                          
2024-09-10 12:57:58          q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)                                          
2024-09-10 12:57:58          v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)                                          
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58          # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)                                     
2024-09-10 12:57:58          if self.flash:                                                                                                             
2024-09-10 12:57:58              # efficient attention using Flash Attention CUDA kernels                                                               
2024-09-10 12:57:58              y = torch.nn.functional.scaled_dot_product_attention(                                                                  
2024-09-10 12:57:58                  q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True                            
2024-09-10 12:57:58              )                                                                                                                      
2024-09-10 12:57:58          else:                                                                                                                      
2024-09-10 12:57:58              # manual implementation of attention                                                                                   
2024-09-10 12:57:58              att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))                                                        
2024-09-10 12:57:58              att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))                                                     
2024-09-10 12:57:58              att = F.softmax(att, dim=-1)                                                                                           
2024-09-10 12:57:58              att = self.attn_dropout(att)                                                                                           
2024-09-10 12:57:58              y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)                                                        
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58          # Calculate token complexity and apply scaling                                                                             
2024-09-10 12:57:58          token_complexity = self.calculate_token_complexity(att)                                                                    
2024-09-10 12:57:58          scaling_factor = token_complexity.unsqueeze(-1)  # Add dimension for broadcasting                                          
2024-09-10 12:57:58          y = y * scaling_factor                                                                                                     
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58          y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side                              
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58          # output projection                                                                                                        
2024-09-10 12:57:58          y = self.resid_dropout(self.c_proj(y))                                                                                     
2024-09-10 12:57:58          return y                                                                                                                   
2024-09-10 12:57:58  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:57:58                                                                                                                                     
2024-09-10 12:57:58 
2024-09-10 12:57:58 This completes the changes for Run 1. You can now run the command python experiment.py --out_dir=run_1 to evaluate the results. Let 
2024-09-10 12:57:58 me know when you're ready for the next experiment!                                                                                  
2024-09-10 12:57:58 
2024-09-10 12:57:58 Tokens: 10,275 sent, 932 received. Cost: $0.07 message, $0.07 session.
2024-09-10 12:57:58 Applied edit to experiment.py
2024-09-10 12:57:58 ### Plan for Experiments
2024-09-10 12:57:58 
2024-09-10 12:57:58 To implement the Complexity-Aware Attention Scaling, we will modify the `CausalSelfAttention` class to include a complexity-aware scaling factor for the attention weights. This scaling factor will be computed based on a running average of token importance derived from the attention weights during training.
2024-09-10 12:57:58 
2024-09-10 12:57:58 We will conduct the following experiments:
2024-09-10 12:57:58 
2024-09-10 12:57:58 1. **Run 1**: Implement the complexity-aware scaling factor with a basic running average mechanism.
2024-09-10 12:57:58 2. **Run 2**: Experiment with a different decay rate for the running average.
2024-09-10 12:57:58 3. **Run 3**: Experiment with a different initialization for the complexity scores.
2024-09-10 12:57:58 4. **Run 4**: Combine the best settings from Runs 2 and 3.
2024-09-10 12:57:58 5. **Run 5**: Fine-tune the best model from Run 4 with a different learning rate.
2024-09-10 12:57:58 
2024-09-10 12:57:58 ### Implementation Steps
2024-09-10 12:57:58 
2024-09-10 12:57:58 1. **Add a function to calculate token complexity.**
2024-09-10 12:57:58 2. **Modify the `forward` method of the `CausalSelfAttention` class to include the complexity-aware scaling factor.**
2024-09-10 12:57:58 
2024-09-10 12:57:58 Let's start with **Run 1**.
2024-09-10 12:57:58 
2024-09-10 12:57:58 ### Run 1: Basic Implementation
2024-09-10 12:57:58 
2024-09-10 12:57:58 #### Step 1: Add a function to calculate token complexity
2024-09-10 12:57:58 
2024-09-10 12:57:58 We'll add a new method `calculate_token_complexity` to the `CausalSelfAttention` class.
2024-09-10 12:57:58 
2024-09-10 12:57:58 #### Step 2: Modify the `forward` method to include the complexity-aware scaling factor
2024-09-10 12:57:58 
2024-09-10 12:57:58 We'll modify the `forward` method to use the complexity-aware scaling factor.
2024-09-10 12:57:58 
2024-09-10 12:57:58 Here are the *SEARCH/REPLACE* blocks:
2024-09-10 12:57:58 
2024-09-10 12:57:58 experiment.py
2024-09-10 12:57:58 ```python
2024-09-10 12:57:58 <<<<<<< SEARCH
2024-09-10 12:57:58     def forward(self, x):
2024-09-10 12:57:58 =======
2024-09-10 12:57:58     def calculate_token_complexity(self, attn_weights):
2024-09-10 12:57:58         # Calculate token complexity based on attention weights
2024-09-10 12:57:58         token_importance = attn_weights.mean(dim=-2)  # Average over the sequence length
2024-09-10 12:57:58         return token_importance
2024-09-10 12:57:58 
2024-09-10 12:57:58     def forward(self, x):
2024-09-10 12:57:58         B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)
2024-09-10 12:57:58 
2024-09-10 12:57:58         # calculate query, key, values for all heads in batch and move head forward to be the batch dim
2024-09-10 12:57:58         q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
2024-09-10 12:57:58         k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
2024-09-10 12:57:58         q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
2024-09-10 12:57:58         v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
2024-09-10 12:57:58 
2024-09-10 12:57:58         # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
2024-09-10 12:57:58         if self.flash:
2024-09-10 12:57:58             # efficient attention using Flash Attention CUDA kernels
2024-09-10 12:57:58             y = torch.nn.functional.scaled_dot_product_attention(
2024-09-10 12:57:58                 q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True
2024-09-10 12:57:58             )
2024-09-10 12:57:58         else:
2024-09-10 12:57:58             # manual implementation of attention
2024-09-10 12:57:58             att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
2024-09-10 12:57:58             att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))
2024-09-10 12:57:58             att = F.softmax(att, dim=-1)
2024-09-10 12:57:58             att = self.attn_dropout(att)
2024-09-10 12:57:58             y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
2024-09-10 12:57:58 
2024-09-10 12:57:58         # Calculate token complexity and apply scaling
2024-09-10 12:57:58         token_complexity = self.calculate_token_complexity(att)
2024-09-10 12:57:58         scaling_factor = token_complexity.unsqueeze(-1)  # Add dimension for broadcasting
2024-09-10 12:57:58         y = y * scaling_factor
2024-09-10 12:57:58 
2024-09-10 12:57:58         y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side
2024-09-10 12:57:58 
2024-09-10 12:57:58         # output projection
2024-09-10 12:57:58         y = self.resid_dropout(self.c_proj(y))
2024-09-10 12:57:58         return y
2024-09-10 12:57:58 >>>>>>> REPLACE
2024-09-10 12:57:58 ```
2024-09-10 12:57:58 
2024-09-10 12:57:58 This completes the changes for **Run 1**. You can now run the command `python experiment.py --out_dir=run_1` to evaluate the results. Let me know when you're ready for the next experiment!
2024-09-10 12:57:59 tokens per iteration will be: 16,384
2024-09-10 12:57:59 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:57:59 Initializing a new model from scratch
2024-09-10 12:57:59 number of parameters: 10.65M
2024-09-10 12:57:59 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:57:59 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:58:00 using fused AdamW: True
2024-09-10 12:58:00 compiling the model... (takes a ~minute)
2024-09-10 12:58:04 Traceback (most recent call last):
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 734, in <module>
2024-09-10 12:58:04     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:04                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 562, in train
2024-09-10 12:58:04     losses = estimate_loss()
2024-09-10 12:58:04              ^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:04     return func(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 523, in estimate_loss
2024-09-10 12:58:04     logits, loss = model(X, Y)
2024-09-10 12:58:04                    ^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:04     return fn(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 257, in forward
2024-09-10 12:58:04     x = block(x)
2024-09-10 12:58:04         ^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 165, in forward
2024-09-10 12:58:04     def forward(self, x):
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 87, in forward
2024-09-10 12:58:04     token_complexity = self.calculate_token_complexity(att)
2024-09-10 12:58:04                                                        ^^^
2024-09-10 12:58:04 UnboundLocalError: cannot access local variable 'att' where it is not associated with a value
2024-09-10 12:58:04 
2024-09-10 12:58:04 Run 1 failed with return code 1
2024-09-10 12:58:04 Run failed with the following error Traceback (most recent call last):
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 734, in <module>
2024-09-10 12:58:04     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:04                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 562, in train
2024-09-10 12:58:04     losses = estimate_loss()
2024-09-10 12:58:04              ^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:04     return func(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 523, in estimate_loss
2024-09-10 12:58:04     logits, loss = model(X, Y)
2024-09-10 12:58:04                    ^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:04     return fn(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 257, in forward
2024-09-10 12:58:04     x = block(x)
2024-09-10 12:58:04         ^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 165, in forward
2024-09-10 12:58:04     def forward(self, x):
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:04     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:04     return forward_call(*args, **kwargs)
2024-09-10 12:58:04            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:04   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 87, in forward
2024-09-10 12:58:04     token_complexity = self.calculate_token_complexity(att)
2024-09-10 12:58:04                                                        ^^^
2024-09-10 12:58:04 UnboundLocalError: cannot access local variable 'att' where it is not associated with a value
2024-09-10 12:58:04 
2024-09-10 12:58:08 The error indicates that the variable att is not defined in the case where self.flash is True. We need to ensure that att is defined
2024-09-10 12:58:08 in both branches of the conditional statement.                                                                                      
2024-09-10 12:58:08 
2024-09-10 12:58:08 Let's fix this by defining att in both branches of the if self.flash condition.                                                     
2024-09-10 12:58:08 
2024-09-10 12:58:08 Here are the SEARCH/REPLACE blocks:                                                                                                 
2024-09-10 12:58:08 
2024-09-10 12:58:08 experiment.py                                                                                                                       
2024-09-10 12:58:08 
2024-09-10 12:58:08                                                                                                                                     
2024-09-10 12:58:08  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:08              y = torch.nn.functional.scaled_dot_product_attention(                                                                  
2024-09-10 12:58:08  =======                                                                                                                            
2024-09-10 12:58:08              att = torch.nn.functional.scaled_dot_product_attention(                                                                
2024-09-10 12:58:08  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:08                                                                                                                                     
2024-09-10 12:58:08 
2024-09-10 12:58:08 experiment.py                                                                                                                       
2024-09-10 12:58:08 
2024-09-10 12:58:08                                                                                                                                     
2024-09-10 12:58:08  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:08          else:                                                                                                                      
2024-09-10 12:58:08  =======                                                                                                                            
2024-09-10 12:58:08              y = att                                                                                                                
2024-09-10 12:58:08          else:                                                                                                                      
2024-09-10 12:58:08  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:08                                                                                                                                     
2024-09-10 12:58:08 
2024-09-10 12:58:08 This ensures that att is defined regardless of whether self.flash is True or False.                                                 
2024-09-10 12:58:08 
2024-09-10 12:58:08 You can now run the command python experiment.py --out_dir=run_1 to evaluate the results. Let me know when you're ready for the next
2024-09-10 12:58:08 experiment!                                                                                                                         
2024-09-10 12:58:08 
2024-09-10 12:58:08 Tokens: 12,174 sent, 197 received. Cost: $0.06 message, $0.13 session.
2024-09-10 12:58:08 Applied edit to experiment.py
2024-09-10 12:58:08 The error indicates that the variable `att` is not defined in the case where `self.flash` is `True`. We need to ensure that `att` is defined in both branches of the conditional statement.
2024-09-10 12:58:08 
2024-09-10 12:58:08 Let's fix this by defining `att` in both branches of the `if self.flash` condition.
2024-09-10 12:58:08 
2024-09-10 12:58:08 Here are the *SEARCH/REPLACE* blocks:
2024-09-10 12:58:08 
2024-09-10 12:58:08 experiment.py
2024-09-10 12:58:08 ```python
2024-09-10 12:58:08 <<<<<<< SEARCH
2024-09-10 12:58:08             y = torch.nn.functional.scaled_dot_product_attention(
2024-09-10 12:58:08 =======
2024-09-10 12:58:08             att = torch.nn.functional.scaled_dot_product_attention(
2024-09-10 12:58:08 >>>>>>> REPLACE
2024-09-10 12:58:08 ```
2024-09-10 12:58:08 
2024-09-10 12:58:08 experiment.py
2024-09-10 12:58:08 ```python
2024-09-10 12:58:08 <<<<<<< SEARCH
2024-09-10 12:58:08         else:
2024-09-10 12:58:08 =======
2024-09-10 12:58:08             y = att
2024-09-10 12:58:08         else:
2024-09-10 12:58:08 >>>>>>> REPLACE
2024-09-10 12:58:08 ```
2024-09-10 12:58:08 
2024-09-10 12:58:08 This ensures that `att` is defined regardless of whether `self.flash` is `True` or `False`.
2024-09-10 12:58:08 
2024-09-10 12:58:08 You can now run the command `python experiment.py --out_dir=run_1` to evaluate the results. Let me know when you're ready for the next experiment!
2024-09-10 12:58:09 tokens per iteration will be: 16,384
2024-09-10 12:58:09 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:58:09 Initializing a new model from scratch
2024-09-10 12:58:09 number of parameters: 10.65M
2024-09-10 12:58:09 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:58:09 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:58:10 using fused AdamW: True
2024-09-10 12:58:10 compiling the model... (takes a ~minute)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 1]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:12 Traceback (most recent call last):
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 735, in <module>
2024-09-10 12:58:12     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:12                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 563, in train
2024-09-10 12:58:12     losses = estimate_loss()
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:12     return func(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 524, in estimate_loss
2024-09-10 12:58:12     logits, loss = model(X, Y)
2024-09-10 12:58:12                    ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:12     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:12     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:12     return self._torchdynamo_orig_callable(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:12     result = self._inner_convert(
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:12     return _compile(
2024-09-10 12:58:12            ^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:12     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:12     return func(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:12     return func(*args, **kwds)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:12     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:12                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:12     r = func(*args, **kwargs)
2024-09-10 12:58:12         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:12     out_code = transform_code_object(code, transform)
2024-09-10 12:58:12                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:12     transformations(instructions, code_options)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:12     super().run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:12     self.call_function(fn, args, kwargs)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:12     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:12     self.call_function(fn, args, kwargs)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:12     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:12     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:12     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:12     return handler(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:12     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:12     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:12     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:12                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:12     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:12     ret_val = wrap_fake_exception(
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:12     return fn()
2024-09-10 12:58:12            ^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:12     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:12             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:12     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:12     return node.target(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:12     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:12     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:12     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:12     r = func(*args, **kwargs)
2024-09-10 12:58:12         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:12     return self_._op(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:12     result = fn(*args, **kwargs)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:12     result = fn(**bound.arguments)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:12     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:12     common_shape = _broadcast_shapes(
2024-09-10 12:58:12                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:12     raise RuntimeError(
2024-09-10 12:58:12 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 1), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:12 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 1]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:12 
2024-09-10 12:58:12 from user code:
2024-09-10 12:58:12    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 258, in forward
2024-09-10 12:58:12     x = block(x)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 167, in forward
2024-09-10 12:58:12     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 90, in forward
2024-09-10 12:58:12     y = y * scaling_factor
2024-09-10 12:58:12 
2024-09-10 12:58:12 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:12 
2024-09-10 12:58:12 
2024-09-10 12:58:12 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:12     import torch._dynamo
2024-09-10 12:58:12     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:12 
2024-09-10 12:58:12 
2024-09-10 12:58:12 Run 1 failed with return code 1
2024-09-10 12:58:12 Run failed with the following error E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:12 E0910 03:58:11.368000 139807511328576 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 1]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:12 Traceback (most recent call last):
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 735, in <module>
2024-09-10 12:58:12     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:12                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 563, in train
2024-09-10 12:58:12     losses = estimate_loss()
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:12     return func(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 524, in estimate_loss
2024-09-10 12:58:12     logits, loss = model(X, Y)
2024-09-10 12:58:12                    ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:12     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:12     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:12     return self._torchdynamo_orig_callable(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:12     result = self._inner_convert(
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:12     return _compile(
2024-09-10 12:58:12            ^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:12     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:12     return func(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:12     return func(*args, **kwds)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:12     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:12                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:12     r = func(*args, **kwargs)
2024-09-10 12:58:12         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:12     out_code = transform_code_object(code, transform)
2024-09-10 12:58:12                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:12     transformations(instructions, code_options)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:12     super().run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:12     self.call_function(fn, args, kwargs)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:12     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:12     self.call_function(fn, args, kwargs)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:12     return inner_fn(self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:12     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:12     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:12     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:12     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:12     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:12     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:12     tracer.run()
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:12     while self.step():
2024-09-10 12:58:12           ^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:12     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:12     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:12     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:12     return handler(tx, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:12     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:12     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:12     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:12                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:12     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:12     ret_val = wrap_fake_exception(
2024-09-10 12:58:12               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:12     return fn()
2024-09-10 12:58:12            ^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:12     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:12             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:12     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:12     return node.target(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:12     return fn(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:12     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:12     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:12     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:12     r = func(*args, **kwargs)
2024-09-10 12:58:12         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:12     return self_._op(*args, **kwargs)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:12     result = fn(*args, **kwargs)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:12     result = fn(**bound.arguments)
2024-09-10 12:58:12              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:12     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:12            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:12     common_shape = _broadcast_shapes(
2024-09-10 12:58:12                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:12     raise RuntimeError(
2024-09-10 12:58:12 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 1), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:12 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 1]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:12 
2024-09-10 12:58:12 from user code:
2024-09-10 12:58:12    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 258, in forward
2024-09-10 12:58:12     x = block(x)
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 167, in forward
2024-09-10 12:58:12     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:12   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:12     return forward_call(*args, **kwargs)
2024-09-10 12:58:12   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 90, in forward
2024-09-10 12:58:12     y = y * scaling_factor
2024-09-10 12:58:12 
2024-09-10 12:58:12 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:12 
2024-09-10 12:58:12 
2024-09-10 12:58:12 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:12     import torch._dynamo
2024-09-10 12:58:12     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:12 
2024-09-10 12:58:12 
2024-09-10 12:58:17 The error indicates a broadcasting issue when multiplying y by scaling_factor. The shapes of y and scaling_factor are not compatible
2024-09-10 12:58:17 for broadcasting. Specifically, y has a shape of [64, 6, 256, 64] and scaling_factor has a shape of [64, 6, 64, 1].                 
2024-09-10 12:58:17 
2024-09-10 12:58:17 To fix this, we need to ensure that scaling_factor has the correct shape to be broadcasted with y. We should modify the             
2024-09-10 12:58:17 calculate_token_complexity function to return a scaling factor that matches the shape of y.                                         
2024-09-10 12:58:17 
2024-09-10 12:58:17 Here are the necessary changes:                                                                                                     
2024-09-10 12:58:17 
2024-09-10 12:58:17  1 Modify the calculate_token_complexity function to return a scaling factor with the correct shape.                                
2024-09-10 12:58:17  2 Ensure that the scaling_factor is broadcastable with y.                                                                          
2024-09-10 12:58:17 
2024-09-10 12:58:17 Let's update the code accordingly.                                                                                                  
2024-09-10 12:58:17 
2024-09-10 12:58:17 experiment.py                                                                                                                       
2024-09-10 12:58:17 
2024-09-10 12:58:17                                                                                                                                     
2024-09-10 12:58:17  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:17          return token_importance                                                                                                    
2024-09-10 12:58:17  =======                                                                                                                            
2024-09-10 12:58:17          # Reshape token_importance to match the shape required for broadcasting                                                    
2024-09-10 12:58:17          return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head)                                       
2024-09-10 12:58:17  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:17                                                                                                                                     
2024-09-10 12:58:17 
2024-09-10 12:58:17 experiment.py                                                                                                                       
2024-09-10 12:58:17 
2024-09-10 12:58:17                                                                                                                                     
2024-09-10 12:58:17  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:17          scaling_factor = token_complexity.unsqueeze(-1)  # Add dimension for broadcasting                                          
2024-09-10 12:58:17  =======                                                                                                                            
2024-09-10 12:58:17          scaling_factor = token_complexity  # Already reshaped for broadcasting                                                     
2024-09-10 12:58:17  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:17                                                                                                                                     
2024-09-10 12:58:17 
2024-09-10 12:58:17 These changes should ensure that the scaling_factor has the correct shape to be broadcasted with y.                                 
2024-09-10 12:58:17 
2024-09-10 12:58:17 Tokens: 11,020 sent, 317 received. Cost: $0.06 message, $0.19 session.
2024-09-10 12:58:17 Applied edit to experiment.py
2024-09-10 12:58:17 The error indicates a broadcasting issue when multiplying `y` by `scaling_factor`. The shapes of `y` and `scaling_factor` are not compatible for broadcasting. Specifically, `y` has a shape of `[64, 6, 256, 64]` and `scaling_factor` has a shape of `[64, 6, 64, 1]`.
2024-09-10 12:58:17 
2024-09-10 12:58:17 To fix this, we need to ensure that `scaling_factor` has the correct shape to be broadcasted with `y`. We should modify the `calculate_token_complexity` function to return a scaling factor that matches the shape of `y`.
2024-09-10 12:58:17 
2024-09-10 12:58:17 Here are the necessary changes:
2024-09-10 12:58:17 
2024-09-10 12:58:17 1. Modify the `calculate_token_complexity` function to return a scaling factor with the correct shape.
2024-09-10 12:58:17 2. Ensure that the `scaling_factor` is broadcastable with `y`.
2024-09-10 12:58:17 
2024-09-10 12:58:17 Let's update the code accordingly.
2024-09-10 12:58:17 
2024-09-10 12:58:17 experiment.py
2024-09-10 12:58:17 ```python
2024-09-10 12:58:17 <<<<<<< SEARCH
2024-09-10 12:58:17         return token_importance
2024-09-10 12:58:17 =======
2024-09-10 12:58:17         # Reshape token_importance to match the shape required for broadcasting
2024-09-10 12:58:17         return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head)
2024-09-10 12:58:17 >>>>>>> REPLACE
2024-09-10 12:58:17 ```
2024-09-10 12:58:17 
2024-09-10 12:58:17 experiment.py
2024-09-10 12:58:17 ```python
2024-09-10 12:58:17 <<<<<<< SEARCH
2024-09-10 12:58:17         scaling_factor = token_complexity.unsqueeze(-1)  # Add dimension for broadcasting
2024-09-10 12:58:17 =======
2024-09-10 12:58:17         scaling_factor = token_complexity  # Already reshaped for broadcasting
2024-09-10 12:58:17 >>>>>>> REPLACE
2024-09-10 12:58:17 ```
2024-09-10 12:58:17 
2024-09-10 12:58:17 These changes should ensure that the `scaling_factor` has the correct shape to be broadcasted with `y`.
2024-09-10 12:58:18 tokens per iteration will be: 16,384
2024-09-10 12:58:18 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:58:18 Initializing a new model from scratch
2024-09-10 12:58:18 number of parameters: 10.65M
2024-09-10 12:58:19 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:58:19 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:58:19 using fused AdamW: True
2024-09-10 12:58:19 compiling the model... (takes a ~minute)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:21 Traceback (most recent call last):
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 736, in <module>
2024-09-10 12:58:21     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:21                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 564, in train
2024-09-10 12:58:21     losses = estimate_loss()
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:21     return func(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 525, in estimate_loss
2024-09-10 12:58:21     logits, loss = model(X, Y)
2024-09-10 12:58:21                    ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:21     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:21     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:21     return self._torchdynamo_orig_callable(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:21     result = self._inner_convert(
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:21     return _compile(
2024-09-10 12:58:21            ^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:21     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:21     return func(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:21     return func(*args, **kwds)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:21     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:21                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:21     r = func(*args, **kwargs)
2024-09-10 12:58:21         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:21     out_code = transform_code_object(code, transform)
2024-09-10 12:58:21                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:21     transformations(instructions, code_options)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:21     super().run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:21     self.call_function(fn, args, kwargs)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:21     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:21     self.call_function(fn, args, kwargs)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:21     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:21     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:21     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:21     return handler(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:21     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:21     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:21     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:21                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:21     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:21     ret_val = wrap_fake_exception(
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:21     return fn()
2024-09-10 12:58:21            ^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:21     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:21             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:21     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:21     return node.target(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:21     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:21     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:21     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:21     r = func(*args, **kwargs)
2024-09-10 12:58:21         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:21     return self_._op(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:21     result = fn(*args, **kwargs)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:21     result = fn(**bound.arguments)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:21     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:21     common_shape = _broadcast_shapes(
2024-09-10 12:58:21                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:21     raise RuntimeError(
2024-09-10 12:58:21 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 64), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:21 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:21 
2024-09-10 12:58:21 from user code:
2024-09-10 12:58:21    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 259, in forward
2024-09-10 12:58:21     x = block(x)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 168, in forward
2024-09-10 12:58:21     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 91, in forward
2024-09-10 12:58:21     y = y * scaling_factor
2024-09-10 12:58:21 
2024-09-10 12:58:21 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:21 
2024-09-10 12:58:21 
2024-09-10 12:58:21 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:21     import torch._dynamo
2024-09-10 12:58:21     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:21 
2024-09-10 12:58:21 
2024-09-10 12:58:21 Run 1 failed with return code 1
2024-09-10 12:58:21 Run failed with the following error E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:21 E0910 03:58:20.517000 140128831567680 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:21 Traceback (most recent call last):
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 736, in <module>
2024-09-10 12:58:21     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:21                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 564, in train
2024-09-10 12:58:21     losses = estimate_loss()
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:21     return func(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 525, in estimate_loss
2024-09-10 12:58:21     logits, loss = model(X, Y)
2024-09-10 12:58:21                    ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:21     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:21     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:21     return self._torchdynamo_orig_callable(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:21     result = self._inner_convert(
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:21     return _compile(
2024-09-10 12:58:21            ^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:21     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:21     return func(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:21     return func(*args, **kwds)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:21     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:21                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:21     r = func(*args, **kwargs)
2024-09-10 12:58:21         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:21     out_code = transform_code_object(code, transform)
2024-09-10 12:58:21                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:21     transformations(instructions, code_options)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:21     super().run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:21     self.call_function(fn, args, kwargs)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:21     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:21     self.call_function(fn, args, kwargs)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:21     return inner_fn(self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:21     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:21     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:21     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:21     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:21     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:21     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:21     tracer.run()
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:21     while self.step():
2024-09-10 12:58:21           ^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:21     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:21     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:21     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:21     return handler(tx, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:21     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:21     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:21     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:21                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:21     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:21     ret_val = wrap_fake_exception(
2024-09-10 12:58:21               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:21     return fn()
2024-09-10 12:58:21            ^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:21     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:21             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:21     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:21     return node.target(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:21     return fn(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:21     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:21     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:21     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:21     r = func(*args, **kwargs)
2024-09-10 12:58:21         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:21     return self_._op(*args, **kwargs)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:21     result = fn(*args, **kwargs)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:21     result = fn(**bound.arguments)
2024-09-10 12:58:21              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:21     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:21            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:21     common_shape = _broadcast_shapes(
2024-09-10 12:58:21                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:21     raise RuntimeError(
2024-09-10 12:58:21 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 64), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:21 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:21 
2024-09-10 12:58:21 from user code:
2024-09-10 12:58:21    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 259, in forward
2024-09-10 12:58:21     x = block(x)
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 168, in forward
2024-09-10 12:58:21     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:21   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:21     return forward_call(*args, **kwargs)
2024-09-10 12:58:21   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 91, in forward
2024-09-10 12:58:21     y = y * scaling_factor
2024-09-10 12:58:21 
2024-09-10 12:58:21 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:21 
2024-09-10 12:58:21 
2024-09-10 12:58:21 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:21     import torch._dynamo
2024-09-10 12:58:21     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:21 
2024-09-10 12:58:21 
2024-09-10 12:58:26 The error indicates a broadcasting issue when multiplying y by scaling_factor. The shapes of y and scaling_factor are not compatible
2024-09-10 12:58:26 for broadcasting. Specifically, y has a shape of [64, 6, 256, 64] and scaling_factor has a shape of [64, 6, 64, 64].                
2024-09-10 12:58:26 
2024-09-10 12:58:26 To fix this, we need to ensure that scaling_factor has the correct shape to be broadcasted with y. We should modify the             
2024-09-10 12:58:26 calculate_token_complexity function to return a scaling factor that matches the shape of y.                                         
2024-09-10 12:58:26 
2024-09-10 12:58:26 Here are the necessary changes:                                                                                                     
2024-09-10 12:58:26 
2024-09-10 12:58:26  1 Modify the calculate_token_complexity function to return a scaling factor with the correct shape.                                
2024-09-10 12:58:26  2 Ensure that the scaling_factor is broadcastable with y.                                                                          
2024-09-10 12:58:26 
2024-09-10 12:58:26 Let's update the code accordingly.                                                                                                  
2024-09-10 12:58:26 
2024-09-10 12:58:26 experiment.py                                                                                                                       
2024-09-10 12:58:26 
2024-09-10 12:58:26                                                                                                                                     
2024-09-10 12:58:26  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:26          return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head)                                       
2024-09-10 12:58:26  =======                                                                                                                            
2024-09-10 12:58:26          # Reshape token_importance to match the shape required for broadcasting                                                    
2024-09-10 12:58:26          return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head).transpose(-2, -1)                     
2024-09-10 12:58:26  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:26                                                                                                                                     
2024-09-10 12:58:26 
2024-09-10 12:58:26 experiment.py                                                                                                                       
2024-09-10 12:58:26 
2024-09-10 12:58:26                                                                                                                                     
2024-09-10 12:58:26  <<<<<<< SEARCH                                                                                                                     
2024-09-10 12:58:26          scaling_factor = token_complexity  # Already reshaped for broadcasting                                                     
2024-09-10 12:58:26  =======                                                                                                                            
2024-09-10 12:58:26          scaling_factor = token_complexity  # Already reshaped for broadcasting                                                     
2024-09-10 12:58:26  >>>>>>> REPLACE                                                                                                                    
2024-09-10 12:58:26                                                                                                                                     
2024-09-10 12:58:26 
2024-09-10 12:58:26 These changes should ensure that the scaling_factor has the correct shape to be broadcasted with y.                                 
2024-09-10 12:58:26 
2024-09-10 12:58:26 Tokens: 11,843 sent, 343 received. Cost: $0.06 message, $0.25 session.
2024-09-10 12:58:26 Applied edit to experiment.py
2024-09-10 12:58:27 The error indicates a broadcasting issue when multiplying `y` by `scaling_factor`. The shapes of `y` and `scaling_factor` are not compatible for broadcasting. Specifically, `y` has a shape of `[64, 6, 256, 64]` and `scaling_factor` has a shape of `[64, 6, 64, 64]`.
2024-09-10 12:58:27 
2024-09-10 12:58:27 To fix this, we need to ensure that `scaling_factor` has the correct shape to be broadcasted with `y`. We should modify the `calculate_token_complexity` function to return a scaling factor that matches the shape of `y`.
2024-09-10 12:58:27 
2024-09-10 12:58:27 Here are the necessary changes:
2024-09-10 12:58:27 
2024-09-10 12:58:27 1. Modify the `calculate_token_complexity` function to return a scaling factor with the correct shape.
2024-09-10 12:58:27 2. Ensure that the `scaling_factor` is broadcastable with `y`.
2024-09-10 12:58:27 
2024-09-10 12:58:27 Let's update the code accordingly.
2024-09-10 12:58:27 
2024-09-10 12:58:27 experiment.py
2024-09-10 12:58:27 ```python
2024-09-10 12:58:27 <<<<<<< SEARCH
2024-09-10 12:58:27         return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head)
2024-09-10 12:58:27 =======
2024-09-10 12:58:27         # Reshape token_importance to match the shape required for broadcasting
2024-09-10 12:58:27         return token_importance.unsqueeze(-1).expand(-1, -1, -1, self.n_embd // self.n_head).transpose(-2, -1)
2024-09-10 12:58:27 >>>>>>> REPLACE
2024-09-10 12:58:27 ```
2024-09-10 12:58:27 
2024-09-10 12:58:27 experiment.py
2024-09-10 12:58:27 ```python
2024-09-10 12:58:27 <<<<<<< SEARCH
2024-09-10 12:58:27         scaling_factor = token_complexity  # Already reshaped for broadcasting
2024-09-10 12:58:27 =======
2024-09-10 12:58:27         scaling_factor = token_complexity  # Already reshaped for broadcasting
2024-09-10 12:58:27 >>>>>>> REPLACE
2024-09-10 12:58:27 ```
2024-09-10 12:58:27 
2024-09-10 12:58:27 These changes should ensure that the `scaling_factor` has the correct shape to be broadcasted with `y`.
2024-09-10 12:58:28 tokens per iteration will be: 16,384
2024-09-10 12:58:28 found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
2024-09-10 12:58:28 Initializing a new model from scratch
2024-09-10 12:58:28 number of parameters: 10.65M
2024-09-10 12:58:28 num decayed parameter tensors: 26, with 10,740,096 parameters
2024-09-10 12:58:28 num non-decayed parameter tensors: 13, with 4,992 parameters
2024-09-10 12:58:28 using fused AdamW: True
2024-09-10 12:58:28 compiling the model... (takes a ~minute)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:31 Traceback (most recent call last):
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 737, in <module>
2024-09-10 12:58:31     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:31                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 565, in train
2024-09-10 12:58:31     losses = estimate_loss()
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:31     return func(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 526, in estimate_loss
2024-09-10 12:58:31     logits, loss = model(X, Y)
2024-09-10 12:58:31                    ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:31     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:31     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:31     return self._torchdynamo_orig_callable(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:31     result = self._inner_convert(
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:31     return _compile(
2024-09-10 12:58:31            ^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:31     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:31     return func(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:31     return func(*args, **kwds)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:31     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:31                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:31     r = func(*args, **kwargs)
2024-09-10 12:58:31         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:31     out_code = transform_code_object(code, transform)
2024-09-10 12:58:31                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:31     transformations(instructions, code_options)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:31     super().run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:31     self.call_function(fn, args, kwargs)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:31     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:31     self.call_function(fn, args, kwargs)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:31     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:31     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:31     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:31     return handler(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:31     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:31     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:31     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:31                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:31     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:31     ret_val = wrap_fake_exception(
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:31     return fn()
2024-09-10 12:58:31            ^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:31     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:31             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:31     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:31     return node.target(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:31     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:31     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:31     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:31     r = func(*args, **kwargs)
2024-09-10 12:58:31         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:31     return self_._op(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:31     result = fn(*args, **kwargs)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:31     result = fn(**bound.arguments)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:31     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:31     common_shape = _broadcast_shapes(
2024-09-10 12:58:31                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:31     raise RuntimeError(
2024-09-10 12:58:31 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 64), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:31 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:31 
2024-09-10 12:58:31 from user code:
2024-09-10 12:58:31    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 260, in forward
2024-09-10 12:58:31     x = block(x)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 169, in forward
2024-09-10 12:58:31     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 92, in forward
2024-09-10 12:58:31     y = y * scaling_factor
2024-09-10 12:58:31 
2024-09-10 12:58:31 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:31 
2024-09-10 12:58:31 
2024-09-10 12:58:31 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:31     import torch._dynamo
2024-09-10 12:58:31     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:31 
2024-09-10 12:58:31 
2024-09-10 12:58:31 Run 1 failed with return code 1
2024-09-10 12:58:31 Run failed with the following error E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] failed while attempting to run meta for aten.mul.Tensor
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] Traceback (most recent call last):
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     r = func(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     return self_._op(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(*args, **kwargs)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     result = fn(**bound.arguments)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     common_shape = _broadcast_shapes(
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0]     raise RuntimeError(
2024-09-10 12:58:31 E0910 03:58:29.866000 139902934427456 torch/_subclasses/fake_tensor.py:1761] [0/0] RuntimeError: Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:31 Traceback (most recent call last):
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 737, in <module>
2024-09-10 12:58:31     final_info, train_info, val_info = train(dataset, out_dir, seed_offset)
2024-09-10 12:58:31                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 565, in train
2024-09-10 12:58:31     losses = estimate_loss()
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
2024-09-10 12:58:31     return func(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 526, in estimate_loss
2024-09-10 12:58:31     logits, loss = model(X, Y)
2024-09-10 12:58:31                    ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:31     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
2024-09-10 12:58:31     return self._call_impl(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1116, in __call__
2024-09-10 12:58:31     return self._torchdynamo_orig_callable(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 948, in __call__
2024-09-10 12:58:31     result = self._inner_convert(
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 472, in __call__
2024-09-10 12:58:31     return _compile(
2024-09-10 12:58:31            ^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_utils_internal.py", line 84, in wrapper_function
2024-09-10 12:58:31     return StrobelightCompileTimeProfiler.profile_compile_time(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_strobelight/compile_time_profiler.py", line 129, in profile_compile_time
2024-09-10 12:58:31     return func(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/contextlib.py", line 81, in inner
2024-09-10 12:58:31     return func(*args, **kwds)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 817, in _compile
2024-09-10 12:58:31     guarded_code = compile_inner(code, one_graph, hooks, transform)
2024-09-10 12:58:31                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 231, in time_wrapper
2024-09-10 12:58:31     r = func(*args, **kwargs)
2024-09-10 12:58:31         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 636, in compile_inner
2024-09-10 12:58:31     out_code = transform_code_object(code, transform)
2024-09-10 12:58:31                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1185, in transform_code_object
2024-09-10 12:58:31     transformations(instructions, code_options)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 178, in _fn
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 582, in transform
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2451, in run
2024-09-10 12:58:31     super().run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:31     self.call_function(fn, args, kwargs)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:31     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2059, in CALL
2024-09-10 12:58:31     self.call_function(fn, args, kwargs)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py", line 437, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 499, in wrapper
2024-09-10 12:58:31     return inner_fn(self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1500, in CALL_FUNCTION_EX
2024-09-10 12:58:31     self.call_function(fn, argsvars.items, kwargsvars)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 743, in call_function
2024-09-10 12:58:31     self.push(fn.call_function(self, args, kwargs))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 344, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
2024-09-10 12:58:31     return super().call_function(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 90, in call_function
2024-09-10 12:58:31     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 749, in inline_user_function_return
2024-09-10 12:58:31     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2666, in inline_call
2024-09-10 12:58:31     return cls.inline_call_(parent, func, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2782, in inline_call_
2024-09-10 12:58:31     tracer.run()
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 893, in run
2024-09-10 12:58:31     while self.step():
2024-09-10 12:58:31           ^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 805, in step
2024-09-10 12:58:31     self.dispatch_table[inst.opcode](self, inst)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2023, in BINARY_OP
2024-09-10 12:58:31     return _binary_op_lookup[inst.arg](self, inst)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 234, in impl
2024-09-10 12:58:31     self.push(fn_var.call_function(self, self.popn(nargs), {}))
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 962, in call_function
2024-09-10 12:58:31     return handler(tx, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py", line 941, in _handle_insert_op_in_graph
2024-09-10 12:58:31     return wrap_fx_proxy(tx, proxy)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1713, in wrap_fx_proxy
2024-09-10 12:58:31     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 1798, in wrap_fx_proxy_cls
2024-09-10 12:58:31     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
2024-09-10 12:58:31                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1853, in get_fake_value
2024-09-10 12:58:31     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1785, in get_fake_value
2024-09-10 12:58:31     ret_val = wrap_fake_exception(
2024-09-10 12:58:31               ^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1300, in wrap_fake_exception
2024-09-10 12:58:31     return fn()
2024-09-10 12:58:31            ^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1786, in <lambda>
2024-09-10 12:58:31     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
2024-09-10 12:58:31             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1921, in run_node
2024-09-10 12:58:31     raise RuntimeError(make_error_message(e)).with_traceback(
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 1903, in run_node
2024-09-10 12:58:31     return node.target(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/utils/_stats.py", line 21, in wrapper
2024-09-10 12:58:31     return fn(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1061, in __torch_dispatch__
2024-09-10 12:58:31     return self.dispatch(func, types, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1450, in dispatch
2024-09-10 12:58:31     return self._cached_dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1145, in _cached_dispatch_impl
2024-09-10 12:58:31     output = self._dispatch_impl(func, types, args, kwargs)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py", line 1757, in _dispatch_impl
2024-09-10 12:58:31     r = func(*args, **kwargs)
2024-09-10 12:58:31         ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_ops.py", line 667, in __call__
2024-09-10 12:58:31     return self_._op(*args, **kwargs)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
2024-09-10 12:58:31     result = fn(*args, **kwargs)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_prims_common/wrappers.py", line 138, in _fn
2024-09-10 12:58:31     result = fn(**bound.arguments)
2024-09-10 12:58:31              ^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 1043, in _ref
2024-09-10 12:58:31     a, b = _maybe_broadcast(a, b)
2024-09-10 12:58:31            ^^^^^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 417, in _maybe_broadcast
2024-09-10 12:58:31     common_shape = _broadcast_shapes(
2024-09-10 12:58:31                    ^^^^^^^^^^^^^^^^^^
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/_refs/__init__.py", line 406, in _broadcast_shapes
2024-09-10 12:58:31     raise RuntimeError(
2024-09-10 12:58:31 torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function mul>(*(FakeTensor(..., device='cuda:0', size=(64, 6, 256, 64), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(64, 6, 64, 64), dtype=torch.bfloat16)), **{}):
2024-09-10 12:58:31 Attempting to broadcast a dimension of length 64 at -2! Mismatching argument at index 1 had torch.Size([64, 6, 64, 64]); but expected shape should be broadcastable to [64, 6, 256, 64]
2024-09-10 12:58:31 
2024-09-10 12:58:31 from user code:
2024-09-10 12:58:31    File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 260, in forward
2024-09-10 12:58:31     x = block(x)
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 169, in forward
2024-09-10 12:58:31     x = x + self.attn(self.ln_1(x))
2024-09-10 12:58:31   File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
2024-09-10 12:58:31     return forward_call(*args, **kwargs)
2024-09-10 12:58:31   File "/app/AI-Scientist/results/nanoGPT/20240910_035747_complexity_aware_attention/experiment.py", line 92, in forward
2024-09-10 12:58:31     y = y * scaling_factor
2024-09-10 12:58:31 
2024-09-10 12:58:31 Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
2024-09-10 12:58:31 
2024-09-10 12:58:31 
2024-09-10 12:58:31 You can suppress this exception and fall back to eager by setting:
2024-09-10 12:58:31     import torch._dynamo
2024-09-10 12:58:31     torch._dynamo.config.suppress_errors = True
2024-09-10 12:58:31 
2024-09-10 12:58:31 
2024-09-10 12:58:31 Max iterations reached
2024-09-10 12:58:31 Not all experiments completed.
2024-09-10 12:58:31 Experiments failed for idea 20240910_035747_complexity_aware_attention
2024-09-10 12:58:31 FINISHED IDEA
2024-09-10 12:58:31 Completed idea: complexity_aware_attention, Success: False
2024-09-10 12:58:31 All ideas evaluated.
